{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Team88_Drake_rap_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donganzh/csc413/blob/main/FinalProject/Team88_Drake_rap_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7LzlmPOLz7"
      },
      "source": [
        "#A Comparison of Transformer and LSTM Towards Rap Lyrics Generation Inspired by Drake’s Music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UwgFvDHOpPR"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "First we import the drake lyrics csv file and extract the lyrics column to construct a new text file containing only the lyrics. The data set is available at [kaggle](https://www.kaggle.com/juicobowley/drake-lyrics)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "B1DdK-qlYpjX",
        "outputId": "11862eda-5de3-480a-faf2-afe9b537b9f5"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./drake_data.csv',skiprows=1, names=['album', 'title', 'url', 'lyrics', 'review'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>album</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Certified Lover Boy</td>\n",
              "      <td>Certified Lover Boy* Lyrics</td>\n",
              "      <td>https://genius.com/Drake-certified-lover-boy-l...</td>\n",
              "      <td>[Verse]\\nPut my feelings on ice\\nAlways been a...</td>\n",
              "      <td>8.7K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Certified Lover Boy</td>\n",
              "      <td>Like I’m Supposed To/Do Things Lyrics</td>\n",
              "      <td>https://genius.com/Drake-like-im-supposed-to-d...</td>\n",
              "      <td>[Verse]\\nHands are tied\\nSomeone's in my ear f...</td>\n",
              "      <td>38.8K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Certified Lover Boy</td>\n",
              "      <td>Not Around Lyrics</td>\n",
              "      <td>https://genius.com/Drake-not-around-lyrics</td>\n",
              "      <td>[Intro]\\nYeah, we back\\nWassup ladies?\\nSwisha...</td>\n",
              "      <td>129.8K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Certified Lover Boy</td>\n",
              "      <td>In the Cut (Ft. Roddy Ricch) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-in-the-cut-lyrics</td>\n",
              "      <td>[Intro: Drake]\\nAyy, yeah\\nPipe this shit up a...</td>\n",
              "      <td>72.1K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Certified Lover Boy</td>\n",
              "      <td>Zodiac Sign (Ft. Jessie Reyez) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-zodiac-sign-lyrics</td>\n",
              "      <td>[Verse 1: Drake]\\nYou ask how many girls I bee...</td>\n",
              "      <td>54.8K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>Unreleased Songs</td>\n",
              "      <td>Pop Style (Demo) (Ft. Kanye West) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-pop-style-demo-lyrics</td>\n",
              "      <td>Lyrics from cut studio recording\\n\\n[Verse 1: ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>Unreleased Songs</td>\n",
              "      <td>Time Flies (Demo) (Ft. Future) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-time-flies-demo-lyrics</td>\n",
              "      <td>Lyrics from Snippet\\n\\n[Chorus: Drake &amp; Future...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Unreleased Songs</td>\n",
              "      <td>Walk It Talk It (Demo) by Quavo (Ft. Drake) Ly...</td>\n",
              "      <td>https://genius.com/Quavo-walk-it-talk-it-demo-...</td>\n",
              "      <td>[Intro: Quavo]\\nYeah, yeah\\n(Deko)\\nWoah, hold...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>Unreleased Songs</td>\n",
              "      <td>God’s Plan (Demo) (Ft. Trippie Redd) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-gods-plan-demo-lyrics</td>\n",
              "      <td>[Intro: Drake]\\nThey wishing, they wishing, th...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>Unreleased Songs</td>\n",
              "      <td>Real Bad Karma (Ft. Bryson Tiller) Lyrics</td>\n",
              "      <td>https://genius.com/Drake-real-bad-karma-lyrics</td>\n",
              "      <td>[Intro: Drake]\\n(Sonorous on the beat)\\nAnd I ...</td>\n",
              "      <td>50.6K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>290 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   album  ...  review\n",
              "0    Certified Lover Boy  ...    8.7K\n",
              "1    Certified Lover Boy  ...   38.8K\n",
              "2    Certified Lover Boy  ...  129.8K\n",
              "3    Certified Lover Boy  ...   72.1K\n",
              "4    Certified Lover Boy  ...   54.8K\n",
              "..                   ...  ...     ...\n",
              "285     Unreleased Songs  ...     NaN\n",
              "286     Unreleased Songs  ...     NaN\n",
              "287     Unreleased Songs  ...     NaN\n",
              "288     Unreleased Songs  ...     NaN\n",
              "289     Unreleased Songs  ...   50.6K\n",
              "\n",
              "[290 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238zdQlMYTHi"
      },
      "source": [
        "def build_dataset(df, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    lyrics = df['lyrics'].tolist()\n",
        "    for lyric in lyrics:\n",
        "        lyric = str(lyric).strip()\n",
        "        data+=lyric\n",
        "    f.write(data)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyvcVJEsahEt"
      },
      "source": [
        "import re\n",
        "build_dataset(df, './drake_data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G1zXJNaYQdq"
      },
      "source": [
        "#part1 - GPT2\n",
        "\n",
        "We adapt the transformer models from the paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). This model was developed by Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya in 2019 and is available at [this repo](https://github.com/openai/gpt-2).\n",
        "\n",
        "We fine tone the model by providing different step sizes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbq_M7-6PTpr"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPjNf1-VTfLg",
        "outputId": "02a9eb02-4257-48aa-c0c7-3955a0e41778"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 456Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 1.28Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 232Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:43, 11.5Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 688Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.68Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.40Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ2lSo0VVUw4",
        "outputId": "51845c61-3268-4ec1-9468-75321d26530b"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-d6KxxmVYHQ"
      },
      "source": [
        "filename = \"./drake_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8-SraBtPP94"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey1VANC0Vl6L",
        "outputId": "4a364d9b-24ee-4c0d-f1df-42d694bbda51"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, \n",
        "             dataset = filename,\n",
        "             model_name = \"124M\",\n",
        "             steps = 1000,\n",
        "             restore_from = \"fresh\",\n",
        "             run_name = 'run1',\n",
        "             print_every = 10,\n",
        "             sample_every =200,\n",
        "             save_every =500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 226068 tokens\n",
            "Training...\n",
            "[10 | 15.30] loss=3.05 avg=3.05\n",
            "[20 | 23.27] loss=3.16 avg=3.11\n",
            "[30 | 31.22] loss=2.66 avg=2.96\n",
            "[40 | 39.20] loss=2.65 avg=2.88\n",
            "[50 | 47.23] loss=2.51 avg=2.80\n",
            "[60 | 55.22] loss=2.56 avg=2.76\n",
            "[70 | 63.22] loss=2.41 avg=2.71\n",
            "[80 | 71.18] loss=2.55 avg=2.69\n",
            "[90 | 79.15] loss=2.78 avg=2.70\n",
            "[100 | 87.12] loss=2.44 avg=2.67\n",
            "[110 | 95.15] loss=2.54 avg=2.66\n",
            "[120 | 103.12] loss=2.36 avg=2.63\n",
            "[130 | 111.11] loss=2.66 avg=2.64\n",
            "[140 | 119.05] loss=2.37 avg=2.62\n",
            "[150 | 127.09] loss=2.49 avg=2.61\n",
            "[160 | 135.04] loss=2.10 avg=2.57\n",
            "[170 | 143.03] loss=2.48 avg=2.57\n",
            "[180 | 151.03] loss=1.67 avg=2.51\n",
            "[190 | 159.01] loss=1.67 avg=2.46\n",
            "[200 | 167.02] loss=1.89 avg=2.43\n",
            "======== SAMPLE 1 ========\n",
            " than a season pass\n",
            "I should probably think about it, you know what it is\n",
            "\n",
            "[Chorus]\n",
            "So- and-so, that's my baby girl\n",
            "She cryin' and I cryin' and I'ma get her\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "\n",
            "[Verse 3]\n",
            "Your mother is in the bathroom, I'm comin' through, so yeah\n",
            "I'm so stressed out, she goin' through the same things\n",
            "And she wishin' that she could have a daddy\n",
            "She wishin' that she could have a real daddy, but I don't have one\n",
            "And she wishin' that she could go and get a pro\n",
            "I mean, she can't have no dad, she can't have no pro\n",
            "But she can have a pro and I can't have no pro\n",
            "I know that my city's livin' like a pro, I could use that\n",
            "I know that my city's livin' like a pro\n",
            "I know that my city's livin' like a pro, I could use that\n",
            "\n",
            "[Bridge]\n",
            "How are we talkin' 'bout what's left of us?\n",
            "'Cause we talkin' 'bout what, 'bout what we went through, you know?\n",
            "\n",
            "[Chorus]\n",
            "So- and-so, that's my baby girl\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "\n",
            "[Bridge]\n",
            "So- and-so, that's my baby girl\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "You know what it is, all right, now we out here doin' what we doin'\n",
            "\n",
            "[Verse 4]\n",
            "And so, I know how to get her mother's number\n",
            "Say hi to me, and get to texting me\n",
            "And get to texting me, and I text her and tell her that I'm here\n",
            "She texts me back, and I text her again\n",
            "And back, back, and I text her and tell her that she miss you\n",
            "And told her that I'm here, and I'm on my way\n",
            "\n",
            "[Chorus]\n",
            "So- and-so, that's my baby girl\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "She cryin', I know how to get her, but I can't get her to you\n",
            "You know what it is, all right, now we out here doin' what we doin'\n",
            "\n",
            "[Outro]\n",
            "You say you're done with me [Intro: Quavo]\n",
            "What are we talkin' 'bout?\n",
            "Niggas wanna party\n",
            "Niggas wanna party\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga?\n",
            "\n",
            "[Verse 1: Drake]\n",
            "Ayy, nothin' like party in my city this summer\n",
            "Niggas gettin' hittin' out when we party\n",
            "Niggas gettin' drunk when we party\n",
            "Niggas wanna party when we party\n",
            "Niggas wanna party when we party\n",
            "Niggas wanna party, Quavo\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga?\n",
            "\n",
            "[Chorus: Quavo]\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are all these people talkin' 'bout?\n",
            "Nigga, what are all these people talking 'bout?\n",
            "Nigga, what are all these people talkin' 'bout?\n",
            "Nigga, what are all these people talkin' 'bout?\n",
            "Quavo, I'm tryna party 'bout, don't party 'bout\n",
            "Nigga, what are we talkin' 'bout?\n",
            "Nigga, what are all these people talkin' 'bout?\n",
            "Nigga, what are all these people talkin' 'bout?\n",
            "Nigga\n",
            "\n",
            "[210 | 184.49] loss=1.93 avg=2.41\n",
            "[220 | 192.49] loss=1.65 avg=2.37\n",
            "[230 | 200.48] loss=1.85 avg=2.34\n",
            "[240 | 208.45] loss=1.99 avg=2.33\n",
            "[250 | 216.49] loss=1.72 avg=2.30\n",
            "[260 | 224.48] loss=1.33 avg=2.26\n",
            "[270 | 232.49] loss=1.40 avg=2.22\n",
            "[280 | 240.49] loss=1.36 avg=2.19\n",
            "[290 | 248.47] loss=1.55 avg=2.16\n",
            "[300 | 256.45] loss=1.33 avg=2.13\n",
            "[310 | 264.41] loss=1.50 avg=2.11\n",
            "[320 | 272.36] loss=1.08 avg=2.07\n",
            "[330 | 280.40] loss=1.19 avg=2.04\n",
            "[340 | 288.45] loss=0.87 avg=2.00\n",
            "[350 | 296.49] loss=1.32 avg=1.97\n",
            "[360 | 304.46] loss=1.22 avg=1.95\n",
            "[370 | 312.48] loss=1.34 avg=1.93\n",
            "[380 | 320.45] loss=0.91 avg=1.90\n",
            "[390 | 328.42] loss=0.57 avg=1.86\n",
            "[400 | 336.39] loss=1.08 avg=1.83\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "I've never been scared, never been shy\n",
            "Anybody that's gonna make you a wish\n",
            "You're gonna make me nervous\n",
            "If you don't say it\n",
            "I bet we gon' do it\n",
            "\n",
            "[Chorus: Drake]\n",
            "It's okay, it's okay, it's okay\n",
            "It's okay, it's okay\n",
            "It's okay, it's okay\n",
            "Let's be real, it's okay, it's okay\n",
            "It's okay, it's okay, it's okay\n",
            "If you a motherfucking man\n",
            "I'm on the other side, I'm on the other side\n",
            "\n",
            "[Outro: Drake & Trey Songz]\n",
            "And I'm still on the other side, yeah\n",
            "I'm still on the other side, yeah\n",
            "'Til shit go..\n",
            "I don't wanna fight with you anymore, won't\n",
            "Fool me once and I would make you my friend\n",
            "I wanna play with you, baby\n",
            "But first I wanna dance, nigga..\n",
            "I wanna dance, nigga..\n",
            "\n",
            "[Verse 1: Trey Songz]\n",
            "I love that I've been here all along\n",
            "These kids wanna go tricking..\n",
            "I don't wanna leave without some practice\n",
            "I've been goin' out on my own since this town got warm up\n",
            "This town's mine all along, mya do that\n",
            "But this town's yours all along, mya do that\n",
            "So I should take this thing or leave it alone\n",
            "And take this thing as a sign that I'm taken care of\n",
            "No new tricks up my sleeve\n",
            "Just a dirty toothbrush on my way\n",
            "No new friends in my shoes, I'll take 'em when I get 'em\n",
            "I'm too smart to ever question the worth\n",
            "Of mistakes that I made, this shit ain't even 11\n",
            "These old pictures of me doin' well, they left a mark\n",
            "These new ones up doin' well, and that's just me doing scratch 'em, scratch em\n",
            "These young girls doin' well, but they're just tryna age\n",
            "All I care about is the crib back then\n",
            "It don't matter to me, they named the crib after me\n",
            "Now they own the land and the water is over my head\n",
            "\n",
            "[Chorus: Trey Songz]\n",
            "You wild and bold in the streets tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "\n",
            "[Outro: Trey Songz]\n",
            "Yeah-yeah\n",
            "Uh-huh\n",
            "\n",
            "[Interlude: Birdman]\n",
            "Uh, this that Ashley, yeah, this that Rebecca\n",
            "And of course, my left foot's gonna go, too\n",
            "And it's gonna cost me a fortune at the bank, too\n",
            "And it's gonna cost me a fortune at the club too\n",
            "And it's gonna cost me a hundred grand at home too\n",
            "And it's gonna cost me a hundred thousand just to make it back\n",
            "Yeah, and I mean it\n",
            "\n",
            "[Verse 2: Drake]\n",
            "When I saw you yesterday, I couldn't have no choice\n",
            "But today you showed me that you can do it\n",
            "And tonight, you showed me that you can't\n",
            "And I hate being alone, but tonight, you too\n",
            "Wonder if you feel safe with me?\n",
            "If you alone, then don't worry\n",
            "I made sure that you all alone tonight\n",
            "Wonder if you're okay?\n",
            "And if you're okay, then don't worry\n",
            "I made sure that you all alone tonight\n",
            "Wonder if you're okay?\n",
            "And if you're okay, then don't worry\n",
            "I made sure that you all alone tonight\n",
            "Wonder if you're okay?\n",
            "And if you're okay, then don't worry\n",
            "I made sure that you all alone tonight\n",
            "\n",
            "[Chorus: Drake]\n",
            "You wild and bold in the streets tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "Wonder if you count it or not tonight\n",
            "\n",
            "[Outro: Songz & Drake]\n",
            "Uh, I count time\n",
            "Count me down\n",
            "Count me down\n",
            "You down, you down, you down\n",
            "Yeah, you down, you down, you down tonight\n",
            "You down, you down\n",
            "\n",
            "[Verse 3: Drake]\n",
            "You didn't even live for dinner tonight\n",
            "You just loved the way it plays in the background\n",
            "\n",
            "[Chorus: Drake]\n",
            "You heard that girl in the video games\n",
            "She said she love me when I go out\n",
            "But she just wants to watch the match up\n",
            "She just wants to sip and savor the moment\n",
            "\n",
            "'Cause I'm just sitting here drinking with you ladies and gents\n",
            "Interlude: Let me get straight to business\n",
            "Let me get straight to business\n",
            "I don't get paid for this\n",
            "\n",
            "[410 | 352.67] loss=0.82 avg=1.80\n",
            "[420 | 360.67] loss=0.85 avg=1.78\n",
            "[430 | 368.67] loss=0.71 avg=1.75\n",
            "[440 | 376.73] loss=0.94 avg=1.72\n",
            "[450 | 384.74] loss=0.68 avg=1.69\n",
            "[460 | 392.74] loss=0.55 avg=1.66\n",
            "[470 | 400.72] loss=0.38 avg=1.63\n",
            "[480 | 408.72] loss=0.35 avg=1.60\n",
            "[490 | 416.70] loss=0.49 avg=1.57\n",
            "[500 | 424.74] loss=0.40 avg=1.54\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 435.60] loss=0.40 avg=1.51\n",
            "[520 | 443.59] loss=0.54 avg=1.49\n",
            "[530 | 451.58] loss=0.39 avg=1.46\n",
            "[540 | 459.61] loss=0.39 avg=1.43\n",
            "[550 | 467.63] loss=0.37 avg=1.41\n",
            "[560 | 475.64] loss=0.45 avg=1.39\n",
            "[570 | 483.62] loss=0.44 avg=1.36\n",
            "[580 | 491.61] loss=0.29 avg=1.34\n",
            "[590 | 499.63] loss=0.24 avg=1.32\n",
            "[600 | 507.62] loss=0.41 avg=1.30\n",
            "======== SAMPLE 1 ========\n",
            " attention they got a little something for me\n",
            "They never give you the time it's given you\n",
            "It's always him or her\n",
            "But when they know you got it, they'll let you get it, girl\n",
            "You got that shit that they say they want you to have\n",
            "They don't get you when you get it, they'll let you get it, girl[Intro: PARTYNEXTDOOR]\n",
            "She want a baby, she live for this whole life\n",
            "Part of me want her to cry on\n",
            "Now the other side\n",
            "Vibe that I often use to escape\n",
            "Sometimes I'm so militant\n",
            "Take a shot and let's break out the champagne\n",
            "Let's break out the champagne\n",
            "Let's break out the champagne\n",
            "\n",
            "[Verse 1: PARTYNEXTDOOR]\n",
            "You wonder why I never on occasion...\n",
            "Other times, it's just us two, standing in a field\n",
            "One heavy, the other one just doing her\n",
            "You on one side, hunkered down, both knees bent\n",
            "We're both on one side, hunkered down, one knee\n",
            "One thing, we don't pretend\n",
            "We get along great, because I figure out what I do is up, and I can't help her\n",
            "Either way, the other guy is busy doing his, and I can't help him]\n",
            "\n",
            "[Interlude: PARTYNEXTDOOR]\n",
            "You wonder why I never on occasion\n",
            "Other times, it's just us two, standing on a field\n",
            "One heavy, the other one just doing her\n",
            "You on one side, hunkered down, both knees bent\n",
            "We're both on one side, hunkered down, one knee\n",
            "One thing, we don't pretend\n",
            "We get along great, because I figure out what I do is up, and I can't help him\n",
            "Either way, the other guy is busy doing his, and I can't help him]\n",
            "\n",
            "[Verse 2: PARTYNEXTDOOR]\n",
            "When she say you gon' be a good lady\n",
            "I say it's no small feat, eh?\n",
            "Overseas and back, I was goin' out on my own\n",
            "Was that back in the days, you know?\n",
            "I'm goin' in now, yeah, yeah, yeah\n",
            "Yeah, yeah, yeah\n",
            "Yeah\n",
            "\n",
            "[Bridge: PARTYNEXTDOOR]\n",
            "What's up?\n",
            "Don't think it's an issue, it's an addiction\n",
            "An' it's no small feat, eh?\n",
            "Overseas and back, I was goin' out on my own\n",
            "Was that back in the days, you know?\n",
            "I'm goin' in now, yeah, yeah, yeah\n",
            "Yeah, yeah, yeah\n",
            "Yeah\n",
            "\n",
            "[Chorus: PARTYNEXTDOOR]\n",
            "Why do I feel so alone?\n",
            "Why do I feel so alone?\n",
            "I'm lonely, I'm lonely\n",
            "I'm lonely, I'm lonely\n",
            "I'm lonely, I'm lonely\n",
            "I'm lonely, I'm lonely\n",
            "Yeah, I'm lonely\n",
            "\n",
            "[Verse 3: Drake]\n",
            "Ooh, shawty, you never hold me down\n",
            "Gotta do me some good things\n",
            "Tell me what's happenin'\n",
            "Niggas always wonder where they left me\n",
            "Back when I could've been\n",
            "Back when I could've done it all\n",
            "So if you hear that we done everything she gone through\n",
            "She got a kid, she done it all\n",
            "It's not me and all that she left behind\n",
            "It's not me and all that she left behind\n",
            "I've got people's hopes in heaven and I don't want none\n",
            "They can see I left out in the cold\n",
            "Heard a lot of niggas say that like it's all mine\n",
            "You might as well bring fame to my city\n",
            "I got everybody nervous\n",
            "'Cause I know I'm a bad omen\n",
            "Reintroduced to a lot of hearts\n",
            "Heartbreak Drake, catch me if you can stand me\n",
            "Had a lot of friends that I don't remember\n",
            "It's been a couple things since I've been here\n",
            "But I've got a city that I'm happy with\n",
            "People are gettin' concerned\n",
            "About me, 'cause I've been here for awhile\n",
            "And I ain't tryna come back, just keep being the same guy\n",
            "People gonna forget me though\n",
            "Cause I've been here, I been, you know?\n",
            "You know how serious I take myself?''\n",
            "And I achieved that despite whether alcohol has affected me\n",
            "Overly-controlled fear that someone will find and say\n",
            "That they can't live without me or see me and love me\n",
            "Lost you to the game, I hope that there are no other players\n",
            "In my mind's eye I'm more concerned with the trophies I've won\n",
            "And the pride I've brought to the table\n",
            "I've learned that none of this is easy enough\n",
            "I was busted for Christmas gifts and\n",
            "\n",
            "[610 | 523.88] loss=0.24 avg=1.27\n",
            "[620 | 531.92] loss=0.35 avg=1.25\n",
            "[630 | 539.98] loss=0.25 avg=1.23\n",
            "[640 | 548.02] loss=0.25 avg=1.21\n",
            "[650 | 556.07] loss=0.19 avg=1.19\n",
            "[660 | 564.07] loss=0.17 avg=1.17\n",
            "[670 | 572.08] loss=0.44 avg=1.15\n",
            "[680 | 580.13] loss=0.23 avg=1.13\n",
            "[690 | 588.12] loss=0.12 avg=1.11\n",
            "[700 | 596.12] loss=0.09 avg=1.09\n",
            "[710 | 604.11] loss=0.10 avg=1.07\n",
            "[720 | 612.10] loss=0.13 avg=1.06\n",
            "[730 | 620.09] loss=0.17 avg=1.04\n",
            "[740 | 628.10] loss=0.10 avg=1.02\n",
            "[750 | 636.08] loss=0.10 avg=1.00\n",
            "[760 | 644.10] loss=0.08 avg=0.99\n",
            "[770 | 652.12] loss=0.11 avg=0.97\n",
            "[780 | 660.14] loss=0.10 avg=0.95\n",
            "[790 | 668.14] loss=0.11 avg=0.94\n",
            "[800 | 676.12] loss=0.12 avg=0.92\n",
            "======== SAMPLE 1 ========\n",
            "ful, I need to hear it all\n",
            "Cause cause I'm stuck between\n",
            "Cause I can't answer your questions, sorry\n",
            "You can ask 'em any time, you know\n",
            "Why I crave attention\n",
            "And I'm stuck between\n",
            "This is your lucky day\n",
            "And it's a day that's being alone\n",
            "You'll ever see me, and all my fears gone\n",
            "Talk to me, I'm nowhere to be found\n",
            "Go out in the world and see people\n",
            "I'm kind of wishing that they knew how I get to where I am\n",
            "I know what I've been up to\n",
            "Most of all I'm wishing that they knew how I get to where I am\n",
            "I know what I've been up to\n",
            "So I'ma ask around\n",
            "So I know what I like and what I've been up to\n",
            "So I don't really know what's going on\n",
            "I guess I'll have to go and look for someone to find me\n",
            "\n",
            "[Chorus: Drake]\n",
            "Ugh, we know what you're thinking, love\n",
            "You feel me? Ugh, we know what you're thinking, love\n",
            "You feel me? Ugh\n",
            "\n",
            "[Verse 2: Lil Wayne]\n",
            "'Cause to her I'm a reflection of insecurities\n",
            "I've always been one for the crystal ball\n",
            "There's no such thing as perfect, there's only Essence\n",
            "And I, I'm so far gone, gone, gone for you\n",
            "I should have put you somewhere where no one could find you\n",
            "And you right there with my dead mama\n",
            "Essence is like an afterthought\n",
            "But when I'm not around, I still adore D'Angelo\n",
            "Who always made sure that my soul was filled with it\n",
            "Smart ones that always get the upper hand\n",
            "I think I might've made you anointed with cement\n",
            "Most of my sudden fame inspired me to drop some jewels\n",
            "And rather look providin' perfectionist\n",
            "This isn't to say that I'm an angel bent on bustin' numbers\n",
            "I still bust with fundamentals in me\n",
            "Just know that every time I break a lead, they ask me how\n",
            "I recover the same amount I came with the split\n",
            "I answer '05[Intro]\n",
            "Yo, full-time\n",
            "I'm with it, Oli, and Aali-na-na-na\n",
            "We just hooked, yo' showtime\n",
            "Who the real real one right now\n",
            "My job is to one-up the other, man\n",
            "I'm with it, Oli, and Aali-na-na-na\n",
            "We just hooked, but who's next?\n",
            "\n",
            "[Verse 1]\n",
            "This a aaaaand the lights go out for you all mmcin' on\n",
            "I saw some niggas stealin' and I was makin' a profit\n",
            "Crib was signin' non-negotiables, and breadcrumbs\n",
            "These niggas couldn't accept minimum wage\n",
            "You was stealin' goods and they wasn't approvein' allowance\n",
            "I will have you stuck in the past like candy\n",
            "When I go to alter di, I always tell you I'm fresh 'cause I been re-creation\n",
            "Why you tryna give me all my glory? When I'm new, yeah\n",
            "\n",
            "[Chorus]\n",
            "Is this shit real? Should I X]\n",
            "If we can't get along, will we ever get along?\n",
            "\n",
            "[Outro]\n",
            "If we can't fuck, can we ever get along?\n",
            "\n",
            "[Produced By Waha!)\n",
            "\n",
            "[Verse 2]\n",
            "Yeah, and he's everything that these rappers claim\n",
            "You don't know nothin' besides us\n",
            "My name is Notion, and I'm from\n",
            "And I don't care how we feel\n",
            "My money is always located at my safe\n",
            "I'm always on my feet, makin' the most out of nothing\n",
            "This never happen to me, will never happen to me\n",
            "\n",
            "[Chorus]\n",
            "Is this shit real? Should I X]\n",
            "If we can't get along, will we ever coexist?\n",
            "\n",
            "[Interlude]\n",
            "Yeah, alright, great. Let's get some things outta here\n",
            "Okay, silent one more time\n",
            "I'ma kill a mama tama one more time\n",
            "One more time, hol' up, hol' up—no\n",
            "One more time, hol' up, hol' up—Hol' up, hol' up\n",
            "\n",
            "[Verse 3]\n",
            "Yeah, and I'm everything a motherfucker is\n",
            "Except I'm not, I'm not even\n",
            "This motherfucking pillow there, this hell there\n",
            "You don't talk to a rat except me 'cause you're too famous\n",
            "And I'm not around, this hell there\n",
            "You talkin' crazy, I don't take it shit\n",
            "No, you don't, you don't listen\n",
            "To what I'm sayin'? All you do is speculate\n",
            "\n",
            "[810 | 692.34] loss=0.08 avg=0.91\n",
            "[820 | 700.32] loss=0.10 avg=0.89\n",
            "[830 | 708.32] loss=0.10 avg=0.88\n",
            "[840 | 716.34] loss=0.11 avg=0.87\n",
            "[850 | 724.33] loss=0.08 avg=0.85\n",
            "[860 | 732.33] loss=0.10 avg=0.84\n",
            "[870 | 740.34] loss=0.08 avg=0.83\n",
            "[880 | 748.33] loss=0.10 avg=0.81\n",
            "[890 | 756.33] loss=0.09 avg=0.80\n",
            "[900 | 764.33] loss=0.10 avg=0.79\n",
            "[910 | 772.32] loss=0.09 avg=0.78\n",
            "[920 | 780.34] loss=0.08 avg=0.77\n",
            "[930 | 788.43] loss=0.08 avg=0.76\n",
            "[940 | 796.45] loss=0.09 avg=0.74\n",
            "[950 | 804.46] loss=0.09 avg=0.73\n",
            "[960 | 812.45] loss=0.09 avg=0.72\n",
            "[970 | 820.45] loss=0.08 avg=0.71\n",
            "[980 | 828.47] loss=0.07 avg=0.70\n",
            "[990 | 836.45] loss=0.09 avg=0.69\n",
            "[1000 | 844.44] loss=0.08 avg=0.68\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY7N46bQ_iz4"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmTsEie-_jg_"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSs16_ozPF_6"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LugXPZmT_lbR"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kdA6k-S_mY3"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHMHDcHVWNql"
      },
      "source": [
        "#part 2 - LSTM\n",
        "\n",
        "We adapt the TensorFlow implementation of Andrej Karpathy's [Char-RNN](https://github.com/karpathy/char-rnn). This code implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. Please refer to his blog post [The Unreasonable Effectiveness of Recurrent Neural Network](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) to learn more about this model. This TensorFlow model is developed by Chen Liang from Google Brain and is available at [this repo](https://github.com/crazydonkey200/tensorflow-char-rnn)\n",
        "\n",
        "We fine tone the model using the following hyperparameters:\n",
        "\n",
        "\n",
        "1.   num of LSTM parameters\n",
        "2.   num of hidden layers\n",
        "3.   batch size\n",
        "4.   learning rate\n",
        "\n",
        "After 30 epochs, the training loss becomes stable and produces a reasonable model for text generation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwgKQqj_XMcI"
      },
      "source": [
        "## 1. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ihdbihWNNO"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import shutil\n",
        "import sys\n",
        "from six import iteritems\n",
        "\n",
        "class CharRNN(object):\n",
        "  \"\"\"Character RNN model.\"\"\"\n",
        "  \n",
        "  def __init__(self, is_training, batch_size, num_unrollings, vocab_size, \n",
        "               hidden_size, max_grad_norm, embedding_size, num_layers,\n",
        "               learning_rate, dropout=0.0, input_dropout=0.0, use_batch=True):\n",
        "    self.batch_size = batch_size\n",
        "    self.num_unrollings = num_unrollings\n",
        "    if not use_batch:\n",
        "      self.batch_size = 1\n",
        "      self.num_unrollings = 1\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding_size = embedding_size\n",
        "    self.model = 'lstm'\n",
        "    self.dropout = dropout\n",
        "    self.input_dropout = input_dropout\n",
        "    if embedding_size <= 0:\n",
        "      self.input_size = vocab_size\n",
        "      # Don't do dropout on one hot representation.\n",
        "      self.input_dropout = 0.0\n",
        "    else:\n",
        "      self.input_size = embedding_size\n",
        "    self.model_size = (embedding_size * vocab_size + # embedding parameters\n",
        "                       # lstm parameters\n",
        "                       4 * hidden_size * (hidden_size + self.input_size + 1) +\n",
        "                       # softmax parameters\n",
        "                       vocab_size * (hidden_size + 1) +\n",
        "                       # multilayer lstm parameters for extra layers.\n",
        "                       (num_layers - 1) * 4 * hidden_size *\n",
        "                       (hidden_size + hidden_size + 1))\n",
        "    # self.decay_rate = decay_rate\n",
        "\n",
        "    # Placeholder to feed in input and targets/labels data.\n",
        "    self.input_data = tf.compat.v1.placeholder(tf.int64,\n",
        "                                     [self.batch_size, self.num_unrollings],\n",
        "                                     name='inputs')\n",
        "    self.targets = tf.compat.v1.placeholder(tf.int64,\n",
        "                                  [self.batch_size, self.num_unrollings],\n",
        "                                  name='targets')\n",
        "\n",
        "\n",
        "    cell_fn = tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n",
        "\n",
        "    # params = {'input_size': self.input_size}\n",
        "    params = {}\n",
        "    \n",
        "    # add bias to forget gate in lstm.\n",
        "    params['forget_bias'] = 0.0\n",
        "    params['state_is_tuple'] = True\n",
        "    # Create multilayer cell.\n",
        "    cell = cell_fn(\n",
        "        self.hidden_size, reuse=tf.compat.v1.get_variable_scope().reuse,\n",
        "        **params)\n",
        "\n",
        "    cells = [cell]\n",
        "\n",
        "    for i in range(self.num_layers-1):\n",
        "      higher_layer_cell = cell_fn(\n",
        "          self.hidden_size, reuse=tf.compat.v1.get_variable_scope().reuse,\n",
        "          **params)\n",
        "      cells.append(higher_layer_cell)\n",
        "\n",
        "    if is_training and self.dropout > 0:\n",
        "      cells = [tf.contrib.rnn.DropoutWrapper(\n",
        "        cell,\n",
        "        output_keep_prob=1.0-self.dropout)\n",
        "               for cell in cells]\n",
        "\n",
        "    multi_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells)\n",
        "\n",
        "    with tf.compat.v1.name_scope('initial_state'):\n",
        "      # zero_state is used to compute the intial state for cell.\n",
        "      self.zero_state = multi_cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "      self.initial_state = create_tuple_placeholders_with_default(\n",
        "        multi_cell.zero_state(batch_size, tf.float32),\n",
        "        extra_dims=(None,),\n",
        "        shape=multi_cell.state_size)      \n",
        "      \n",
        "\n",
        "    # Embeddings layers.\n",
        "    with tf.compat.v1.name_scope('embedding_layer'):\n",
        "      if embedding_size > 0:\n",
        "        self.embedding = tf.compat.v1.get_variable(\n",
        "          'embedding', [self.vocab_size, self.embedding_size])\n",
        "      else:\n",
        "        self.embedding = tf.constant(np.eye(self.vocab_size), dtype=tf.float32)\n",
        "\n",
        "      inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n",
        "      if is_training and self.input_dropout > 0:\n",
        "        inputs = tf.nn.dropout(inputs, 1 - self.input_dropout)\n",
        "\n",
        "    with tf.compat.v1.name_scope('slice_inputs'):\n",
        "      # Slice inputs into a list of shape [batch_size, 1] data colums.\n",
        "      sliced_inputs = [tf.squeeze(input_, [1])\n",
        "                       for input_ in tf.split(axis=1, num_or_size_splits=self.num_unrollings, value=inputs)]\n",
        "      \n",
        "    # Copy cell to do unrolling and collect outputs.\n",
        "    outputs, final_state = tf.compat.v1.nn.static_rnn(\n",
        "      multi_cell, sliced_inputs,\n",
        "      initial_state=self.initial_state)\n",
        "\n",
        "    self.final_state = final_state\n",
        "\n",
        "    with tf.compat.v1.name_scope('flatten_ouputs'):\n",
        "      # Flatten the outputs into one dimension.\n",
        "      flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, hidden_size])\n",
        "\n",
        "    with tf.compat.v1.name_scope('flatten_targets'):\n",
        "      # Flatten the targets too.\n",
        "      flat_targets = tf.reshape(tf.concat(axis=1, values=self.targets), [-1])\n",
        "    \n",
        "    # Create softmax parameters, weights and bias.\n",
        "    with tf.compat.v1.name_scope('softmax') as sm_vs:\n",
        "      softmax_w = tf.compat.v1.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
        "      softmax_b = tf.compat.v1.get_variable(\"softmax_b\", [vocab_size])\n",
        "      self.logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
        "      self.probs = tf.nn.softmax(self.logits)\n",
        "\n",
        "    with tf.compat.v1.name_scope('loss'):\n",
        "      # Compute mean cross entropy loss for each output.\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=self.logits, labels=flat_targets)\n",
        "      self.mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "    with tf.compat.v1.name_scope('loss_monitor'):\n",
        "      # Count the number of elements and the sum of mean_loss\n",
        "      # from each batch to compute the average loss.\n",
        "      count = tf.Variable(1.0, name='count')\n",
        "      sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
        "      \n",
        "      self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n",
        "                                         count.assign(0.0),\n",
        "                                         name='reset_loss_monitor')\n",
        "      self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss +\n",
        "                                                               self.mean_loss),\n",
        "                                          count.assign(count + 1),\n",
        "                                          name='update_loss_monitor')\n",
        "      with tf.control_dependencies([self.update_loss_monitor]):\n",
        "        self.average_loss = sum_mean_loss / count\n",
        "        self.ppl = tf.exp(self.average_loss)\n",
        "\n",
        "      # Monitor the loss.\n",
        "      loss_summary_name = \"average_loss\"\n",
        "      ppl_summary_name = \"perplexity\"\n",
        "  \n",
        "      average_loss_summary = tf.compat.v1.summary.scalar(loss_summary_name, self.average_loss)\n",
        "      ppl_summary = tf.compat.v1.summary.scalar(ppl_summary_name, self.ppl)\n",
        "\n",
        "    # Monitor the loss.\n",
        "    self.summaries = tf.compat.v1.summary.merge([average_loss_summary, ppl_summary],\n",
        "                                      name='loss_monitor')\n",
        "    \n",
        "    self.global_step = tf.compat.v1.get_variable('global_step', [],\n",
        "                                       initializer=tf.compat.v1.constant_initializer(0.0))\n",
        "\n",
        "    self.learning_rate = tf.constant(learning_rate)\n",
        "    if is_training:\n",
        "      tvars = tf.compat.v1.trainable_variables()\n",
        "      grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, tvars),\n",
        "                                        self.max_grad_norm)\n",
        "\n",
        "      optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "\n",
        "      self.train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
        "                                                global_step=self.global_step)\n",
        "      \n",
        "  def run_epoch(self, session, data_size, batch_generator, is_training,\n",
        "                verbose=0, freq=10, summary_writer=None, debug=False, divide_by_n=1):\n",
        "    \"\"\"Runs the model on the given data for one full pass.\"\"\"\n",
        "    # epoch_size = ((data_size // self.batch_size) - 1) // self.num_unrollings\n",
        "    epoch_size = data_size // (self.batch_size * self.num_unrollings)\n",
        "    if data_size % (self.batch_size * self.num_unrollings) != 0:\n",
        "        epoch_size += 1\n",
        "\n",
        "    if verbose > 0:\n",
        "        logging.info('epoch_size: %d', epoch_size)\n",
        "        logging.info('data_size: %d', data_size)\n",
        "        logging.info('num_unrollings: %d', self.num_unrollings)\n",
        "        logging.info('batch_size: %d', self.batch_size)\n",
        "\n",
        "    if is_training:\n",
        "      extra_op = self.train_op\n",
        "    else:\n",
        "      extra_op = tf.no_op()\n",
        "\n",
        "    # Prepare initial state and reset the average loss\n",
        "    # computation.\n",
        "    state = session.run(self.zero_state)\n",
        "    self.reset_loss_monitor.run()\n",
        "    start_time = time.time()\n",
        "    for step in range(epoch_size // divide_by_n):\n",
        "      # Generate the batch and use [:-1] as inputs and [1:] as targets.\n",
        "      data = batch_generator.next()\n",
        "      inputs = np.array(data[:-1]).transpose()\n",
        "      targets = np.array(data[1:]).transpose()\n",
        "\n",
        "      ops = [self.average_loss, self.final_state, extra_op,\n",
        "             self.summaries, self.global_step, self.learning_rate]\n",
        "\n",
        "      feed_dict = {self.input_data: inputs, self.targets: targets,\n",
        "                   self.initial_state: state}\n",
        "\n",
        "      results = session.run(ops, feed_dict)\n",
        "      average_loss, state, _, summary_str, global_step, lr = results\n",
        "      \n",
        "      ppl = np.exp(average_loss)\n",
        "      if (verbose > 0) and ((step+1) % freq == 0):\n",
        "        logging.info(\"%.1f%%, step:%d, perplexity: %.3f, speed: %.0f words\",\n",
        "                     (step + 1) * 1.0 / epoch_size * 100, step, ppl,\n",
        "                     (step + 1) * self.batch_size * self.num_unrollings /\n",
        "                     (time.time() - start_time))\n",
        "\n",
        "    logging.info(\"Perplexity: %.3f, speed: %.0f words per sec\",\n",
        "                 ppl, (step + 1) * self.batch_size * self.num_unrollings /\n",
        "                 (time.time() - start_time))\n",
        "    return ppl, summary_str, global_step\n",
        "\n",
        "  def sample_seq(self, session, length, start_text, vocab_index_dict,\n",
        "                 index_vocab_dict, temperature=1.0, max_prob=True):\n",
        "\n",
        "    state = session.run(self.zero_state)\n",
        "\n",
        "    # use start_text to warm up the RNN.\n",
        "    if start_text is not None and len(start_text) > 0:\n",
        "      seq = list(start_text)\n",
        "      for char in start_text[:-1]:\n",
        "        x = np.array([[char2id(char, vocab_index_dict)]])\n",
        "        state = session.run(self.final_state,\n",
        "                            {self.input_data: x,\n",
        "                             self.initial_state: state})\n",
        "      x = np.array([[char2id(start_text[-1], vocab_index_dict)]])\n",
        "    else:\n",
        "      vocab_size = len(vocab_index_dict.keys())\n",
        "      x = np.array([[np.random.randint(0, vocab_size)]])\n",
        "      seq = []\n",
        "\n",
        "    for i in range(length):\n",
        "      state, logits = session.run([self.final_state,\n",
        "                                   self.logits],\n",
        "                                  {self.input_data: x,\n",
        "                                   self.initial_state: state})\n",
        "      unnormalized_probs = np.exp((logits - np.max(logits)) / temperature)\n",
        "      probs = unnormalized_probs / np.sum(unnormalized_probs)\n",
        "\n",
        "      if max_prob:\n",
        "        sample = np.argmax(probs[0])\n",
        "      else:\n",
        "        sample = np.random.choice(self.vocab_size, 1, p=probs[0])[0]\n",
        "\n",
        "      seq.append(id2char(sample, index_vocab_dict))\n",
        "      x = np.array([[sample]])\n",
        "    return ''.join(seq)\n",
        "      \n",
        "        \n",
        "class BatchGenerator(object):\n",
        "    \"\"\"Generate and hold batches.\"\"\"\n",
        "    def __init__(self, text, batch_size, n_unrollings, vocab_size,\n",
        "                 vocab_index_dict, index_vocab_dict):\n",
        "      self._text = text\n",
        "      self._text_size = len(text)\n",
        "      self._batch_size = batch_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self._n_unrollings = n_unrollings\n",
        "      self.vocab_index_dict = vocab_index_dict\n",
        "      self.index_vocab_dict = index_vocab_dict\n",
        "      \n",
        "      segment = self._text_size // batch_size\n",
        "\n",
        "      # number of elements in cursor list is the same as\n",
        "      # batch_size.  each batch is just the collection of\n",
        "      # elements in where the cursors are pointing to.\n",
        "      self._cursor = [ offset * segment for offset in range(batch_size)]\n",
        "      self._last_batch = self._next_batch()\n",
        "      \n",
        "    def _next_batch(self):\n",
        "      \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
        "      batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
        "      for b in range(self._batch_size):\n",
        "        batch[b] = char2id(self._text[self._cursor[b]], self.vocab_index_dict)\n",
        "        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
        "      return batch\n",
        "\n",
        "    def next(self):\n",
        "      \"\"\"Generate the next array of batches from the data. The array consists of\n",
        "      the last batch of the previous array, followed by num_unrollings new ones.\n",
        "      \"\"\"\n",
        "      batches = [self._last_batch]\n",
        "      for step in range(self._n_unrollings):\n",
        "        batches.append(self._next_batch())\n",
        "      self._last_batch = batches[-1]\n",
        "      return batches\n",
        "\n",
        "\n",
        "# Utility functions\n",
        "def batches2string(batches, index_vocab_dict):\n",
        "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
        "  representation.\"\"\"\n",
        "  s = [''] * batches[0].shape[0]\n",
        "  for b in batches:\n",
        "    s = [''.join(x) for x in zip(s, id2char_list(b, index_vocab_dict))]\n",
        "  return s\n",
        "\n",
        "\n",
        "def characters(probabilities):\n",
        "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
        "  characters back into its (most likely) character representation.\"\"\"\n",
        "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
        "\n",
        "\n",
        "def char2id(char, vocab_index_dict):\n",
        "  try:\n",
        "    return vocab_index_dict[char]\n",
        "  except KeyError:\n",
        "    logging.info('Unexpected char %s', char)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def id2char(index, index_vocab_dict):\n",
        "  return index_vocab_dict[index]\n",
        "\n",
        "    \n",
        "def id2char_list(lst, index_vocab_dict):\n",
        "  return [id2char(i, index_vocab_dict) for i in lst]\n",
        "\n",
        "\n",
        "def create_tuple_placeholders_with_default(inputs, extra_dims, shape):\n",
        "  if isinstance(shape, int):\n",
        "    result = tf.compat.v1.placeholder_with_default(\n",
        "      inputs, list(extra_dims) + [shape])\n",
        "  else:\n",
        "    subplaceholders = [create_tuple_placeholders_with_default(\n",
        "      subinputs, extra_dims, subshape)\n",
        "                       for subinputs, subshape in zip(inputs, shape)]\n",
        "    t = type(shape)\n",
        "    if t == tuple:\n",
        "      result = t(subplaceholders)\n",
        "    else:\n",
        "      result = t(*subplaceholders)    \n",
        "  return result\n",
        "\n",
        "        \n",
        "def create_tuple_placeholders(dtype, extra_dims, shape):\n",
        "  if isinstance(shape, int):\n",
        "    result = tf.compat.v1.placeholder(dtype, list(extra_dims) + [shape])\n",
        "  else:\n",
        "    subplaceholders = [create_tuple_placeholders(dtype, extra_dims, subshape)\n",
        "                       for subshape in shape]\n",
        "    t = type(shape)\n",
        "\n",
        "    # Handles both tuple and LSTMStateTuple.\n",
        "    if t == tuple:\n",
        "      result = t(subplaceholders)\n",
        "    else:\n",
        "      result = t(*subplaceholders)\n",
        "  return result\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHqPGscLXdIn"
      },
      "source": [
        "##2. training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIfOJeepXYLY"
      },
      "source": [
        "def train(data_file, num_epochs, hidden_size, num_layers, batch_size, output_dir):\n",
        "\n",
        "    encoding = 'utf-8'\n",
        "    n_save=1\n",
        "    max_to_keep=5\n",
        "    embedding_size=0\n",
        "    num_unrollings=10\n",
        "    train_frac=0.8\n",
        "    valid_frac=0.1\n",
        "    dropout=0.0\n",
        "\n",
        "    input_dropout=0.0\n",
        "\n",
        "    # Parameters for gradient descent.\n",
        "    max_grad_norm=5\n",
        "    learning_rate=1e-4\n",
        "    decay_rate=0.95\n",
        "\n",
        "    # Parameters for logging.    \n",
        "    progress_freq=100\n",
        "    verbose=0\n",
        "\n",
        "    # Parameters to feed in the initial model and current best model.\n",
        "    init_model=''\n",
        "    best_model=''\n",
        "    best_valid_ppl=np.Inf\n",
        "                       \n",
        "    # Parameters for using saved best models.\n",
        "    init_dir=''\n",
        "\n",
        "    # Specifying location to store model, best model and tensorboard log.\n",
        "    save_model = os.path.join(output_dir, 'save_model/model')\n",
        "    save_best_model = os.path.join(output_dir, 'best_model/model')\n",
        "    tb_log_dir = os.path.join(output_dir, 'tensorboard_log/')\n",
        "    vocab_file = ''\n",
        "\n",
        "    # Create necessary directories.\n",
        "    if init_dir:\n",
        "        output_dir = init_dir\n",
        "    else:\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        for paths in [save_model, save_best_model,\n",
        "                      tb_log_dir]:\n",
        "            os.makedirs(os.path.dirname(paths))\n",
        "\n",
        "\n",
        "    logging.basicConfig(stream=sys.stdout,\n",
        "                        format='%(asctime)s %(levelname)s:%(message)s', \n",
        "                        level=logging.INFO,\n",
        "                        datefmt='%I:%M:%S')\n",
        "\n",
        "    print('=' * 60)\n",
        "    print('All final and intermediate outputs will be stored in %s/' % output_dir)\n",
        "    print('=' * 60 + '\\n')\n",
        "\n",
        "    # Prepare parameters.\n",
        "    if init_dir:\n",
        "        with open(os.path.join(init_dir, 'result.json'), 'r') as f:\n",
        "            result = json.load(f)\n",
        "        params = result['params']\n",
        "        init_model = result['latest_model']\n",
        "        best_model = result['best_model']\n",
        "        best_valid_ppl = result['best_valid_ppl']\n",
        "        if 'encoding' in result:\n",
        "            encoding = result['encoding']\n",
        "        else:\n",
        "            encoding = 'utf-8'\n",
        "        vocab_file = os.path.join(init_dir, 'vocab.json')\n",
        "    else:\n",
        "        params = {'batch_size': batch_size,\n",
        "                  'num_unrollings': num_unrollings,\n",
        "                  'hidden_size': hidden_size,\n",
        "                  'max_grad_norm': max_grad_norm,\n",
        "                  'embedding_size': embedding_size,\n",
        "                  'num_layers': num_layers,\n",
        "                  'learning_rate': learning_rate,\n",
        "                  'dropout': dropout,\n",
        "                  'input_dropout': input_dropout}\n",
        "        best_model = ''\n",
        "    logging.info('Parameters are:\\n%s\\n', json.dumps(params, sort_keys=True, indent=4))\n",
        "\n",
        "    # Read and split data.\n",
        "    print('Reading data from: %s', data_file)\n",
        "    with codecs.open(data_file, 'r', encoding=encoding) as f:\n",
        "        text = f.read()\n",
        "\n",
        "    logging.info('Number of characters: %s', len(text))\n",
        "\n",
        "    logging.info('Creating train, valid, test split')\n",
        "    train_size = int(train_frac * len(text))\n",
        "    valid_size = int(valid_frac * len(text))\n",
        "    test_size = len(text) - train_size - valid_size\n",
        "    train_text = text[:train_size]\n",
        "    valid_text = text[train_size:train_size + valid_size]\n",
        "    test_text = text[train_size + valid_size:]\n",
        "\n",
        "    if vocab_file:\n",
        "        vocab_index_dict, index_vocab_dict, vocab_size = load_vocab(\n",
        "          vocab_file, encoding)\n",
        "    else:\n",
        "        logging.info('Creating vocabulary')\n",
        "        vocab_index_dict, index_vocab_dict, vocab_size = create_vocab(text)\n",
        "        vocab_file = os.path.join(output_dir, 'vocab.json')\n",
        "        save_vocab(vocab_index_dict, vocab_file, encoding)\n",
        "        logging.info('Vocabulary is saved in %s', vocab_file)\n",
        "        vocab_file = vocab_file\n",
        "\n",
        "    params['vocab_size'] = vocab_size\n",
        "    logging.info('Vocab size: %d', vocab_size)\n",
        "\n",
        "    # Create batch generators.\n",
        "    batch_size = params['batch_size']\n",
        "    num_unrollings = params['num_unrollings']\n",
        "    train_batches = BatchGenerator(train_text, batch_size, num_unrollings, vocab_size, \n",
        "                                   vocab_index_dict, index_vocab_dict)\n",
        "    valid_batches = BatchGenerator(valid_text, batch_size, num_unrollings, vocab_size,\n",
        "                                   vocab_index_dict, index_vocab_dict)\n",
        "    test_batches = BatchGenerator(test_text, 1, 1, vocab_size,\n",
        "                                  vocab_index_dict, index_vocab_dict)\n",
        "        \n",
        "    # Create graphs\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        with tf.compat.v1.name_scope('training'):\n",
        "            train_model = CharRNN(is_training=True, use_batch=True, **params)\n",
        "        tf.compat.v1.get_variable_scope().reuse_variables()\n",
        "        with tf.compat.v1.name_scope('validation'):\n",
        "            valid_model = CharRNN(is_training=False, use_batch=True, **params)\n",
        "        with tf.compat.v1.name_scope('evaluation'):\n",
        "            test_model = CharRNN(is_training=False, use_batch=False, **params)\n",
        "            saver = tf.compat.v1.train.Saver(name='checkpoint_saver', max_to_keep=max_to_keep)\n",
        "            best_model_saver = tf.compat.v1.train.Saver(name='best_model_saver')\n",
        "\n",
        "    logging.info('Model size (number of parameters): %s\\n', train_model.model_size)\n",
        "    logging.info('Start training\\n')\n",
        "\n",
        "    result = {}\n",
        "    result['params'] = params\n",
        "    result['vocab_file'] = vocab_file\n",
        "    result['encoding'] = encoding\n",
        "\n",
        "    try:\n",
        "        with tf.compat.v1.Session(graph=graph) as session:\n",
        "            graph_info = session.graph\n",
        "\n",
        "            train_writer = tf.compat.v1.summary.FileWriter(tb_log_dir + 'train/', graph_info)\n",
        "            valid_writer = tf.compat.v1.summary.FileWriter(tb_log_dir + 'valid/', graph_info)\n",
        "\n",
        "            # load a saved model or start from random initialization.\n",
        "            if init_model:\n",
        "                saver.restore(session, init_model)\n",
        "            else:\n",
        "                tf.compat.v1.global_variables_initializer().run()\n",
        "            for i in range(num_epochs):\n",
        "                for j in range(n_save):\n",
        "                    logging.info(\n",
        "                        '=' * 19 + ' Epoch %d: %d/%d' + '=' * 19 + '\\n', i+1, j+1, n_save)\n",
        "                    logging.info('Training on training set')\n",
        "                    # training step\n",
        "                    ppl, train_summary_str, global_step = train_model.run_epoch(\n",
        "                        session,\n",
        "                        train_size,\n",
        "                        train_batches,\n",
        "                        is_training=True,\n",
        "                        verbose=verbose,\n",
        "                        freq=progress_freq,\n",
        "                        divide_by_n=n_save)\n",
        "                    # record the summary\n",
        "                    train_writer.add_summary(train_summary_str, global_step)\n",
        "                    train_writer.flush()\n",
        "                    # save model\n",
        "                    saved_path = saver.save(session, save_model,\n",
        "                                            global_step=train_model.global_step)\n",
        "                    logging.info('Latest model saved in %s\\n', saved_path)\n",
        "                    logging.info('Evaluate on validation set')\n",
        "\n",
        "                    # valid_ppl, valid_summary_str, _ = valid_model.run_epoch(\n",
        "                    valid_ppl, valid_summary_str, _ = valid_model.run_epoch(\n",
        "                        session,\n",
        "                        valid_size,\n",
        "                        valid_batches, \n",
        "                        is_training=False,\n",
        "                        verbose=verbose,\n",
        "                        freq=progress_freq)\n",
        "\n",
        "                    # save and update best model\n",
        "                    if (not best_model) or (valid_ppl < best_valid_ppl):\n",
        "                        best_model = best_model_saver.save(\n",
        "                            session,\n",
        "                            save_best_model,\n",
        "                            global_step=train_model.global_step)\n",
        "                        best_valid_ppl = valid_ppl\n",
        "                    valid_writer.add_summary(valid_summary_str, global_step)\n",
        "                    valid_writer.flush()\n",
        "                    logging.info('Best model is saved in %s', best_model)\n",
        "                    logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n",
        "                    result['latest_model'] = saved_path\n",
        "                    result['best_model'] = best_model\n",
        "                    # Convert to float because numpy.float is not json serializable.\n",
        "                    result['best_valid_ppl'] = float(best_valid_ppl)\n",
        "                    result_path = os.path.join(output_dir, 'result.json')\n",
        "                    if os.path.exists(result_path):\n",
        "                        os.remove(result_path)\n",
        "                    with open(result_path, 'w') as f:\n",
        "                        json.dump(result, f, indent=2, sort_keys=True)\n",
        "\n",
        "            logging.info('Latest model is saved in %s', saved_path)\n",
        "            logging.info('Best model is saved in %s', best_model)\n",
        "            logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n",
        "            logging.info('Evaluate the best model on test set')\n",
        "            saver.restore(session, best_model)\n",
        "            test_ppl, _, _ = test_model.run_epoch(session, test_size, test_batches,\n",
        "                                                   is_training=False,\n",
        "                                                   verbose=verbose,\n",
        "                                                   freq=progress_freq)\n",
        "            result['test_ppl'] = float(test_ppl)\n",
        "    finally:\n",
        "        result_path = os.path.join(output_dir, 'result.json')\n",
        "        if os.path.exists(result_path):\n",
        "            os.remove(result_path)\n",
        "        with open(result_path, 'w') as f:\n",
        "            json.dump(result, f, indent=2, sort_keys=True)\n",
        "\n",
        "\n",
        "def create_vocab(text):\n",
        "    unique_chars = list(set(text))\n",
        "    vocab_size = len(unique_chars)\n",
        "    vocab_index_dict = {}\n",
        "    index_vocab_dict = {}\n",
        "    for i, char in enumerate(unique_chars):\n",
        "        vocab_index_dict[char] = i\n",
        "        index_vocab_dict[i] = char\n",
        "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file, encoding):\n",
        "    with codecs.open(vocab_file, 'r', encoding=encoding) as f:\n",
        "        vocab_index_dict = json.load(f)\n",
        "    index_vocab_dict = {}\n",
        "    vocab_size = 0\n",
        "    for char, index in iteritems(vocab_index_dict):\n",
        "        index_vocab_dict[index] = char\n",
        "        vocab_size += 1\n",
        "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
        "\n",
        "\n",
        "def save_vocab(vocab_index_dict, vocab_file, encoding):\n",
        "    with codecs.open(vocab_file, 'w', encoding=encoding) as f:\n",
        "        json.dump(vocab_index_dict, f, indent=2, sort_keys=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20mzIMKBXtxQ"
      },
      "source": [
        "##3. Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4qdu6PbXzq2"
      },
      "source": [
        "def sampling(init_dir, start_text, length):\n",
        "    \n",
        "    # Parameters for using saved best models.\n",
        "    # Parameters for picking which model to use. \n",
        "    model_path=''\n",
        "    # Parameters for sampling.\n",
        "    temperature=1.0\n",
        "    max_prob=False\n",
        "    seed=999\n",
        "\n",
        "    # Parameters for evaluation (computing perplexity of given text).\n",
        "    evaluate=False\n",
        "    example_text='The meaning of life is 42.'\n",
        "    \n",
        "    # Prepare parameters.\n",
        "    with open(os.path.join(init_dir, 'result.json'), 'r') as f:\n",
        "        result = json.load(f)\n",
        "    params = result['params']\n",
        "\n",
        "    if model_path:    \n",
        "        best_model = model_path\n",
        "    else:\n",
        "        best_model = result['best_model']\n",
        "\n",
        "    best_valid_ppl = result['best_valid_ppl']\n",
        "    if 'encoding' in result:\n",
        "        encoding = result['encoding']\n",
        "    else:\n",
        "        encoding = 'utf-8'\n",
        "    vocab_file = os.path.join(init_dir, 'vocab.json')\n",
        "    vocab_index_dict, index_vocab_dict, vocab_size = load_vocab(vocab_file, encoding)\n",
        "\n",
        "    # Create graphs\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        with tf.compat.v1.name_scope('evaluation'):\n",
        "            test_model = CharRNN(is_training=False, use_batch=False, **params)\n",
        "            saver = tf.compat.v1.train.Saver(name='checkpoint_saver')\n",
        "\n",
        "    if evaluate:\n",
        "        example_batches = BatchGenerator(example_text, 1, 1, vocab_size,\n",
        "                                         vocab_index_dict, index_vocab_dict)\n",
        "        with tf.compat.v1.Session(graph=graph) as session:\n",
        "            saver.restore(session, best_model)\n",
        "            ppl = test_model.run_epoch(session, len(example_text),\n",
        "                                        example_batches,\n",
        "                                        is_training=False)[0]\n",
        "            print('Example text is: %s' % example_text)\n",
        "            print('Perplexity is: %s' % ppl)\n",
        "    else:\n",
        "        if seed >= 0:\n",
        "            np.random.seed(seed)\n",
        "        # Sampling a sequence \n",
        "        with tf.compat.v1.Session(graph=graph) as session:\n",
        "            saver.restore(session, best_model)\n",
        "            sample = test_model.sample_seq(session, length, start_text,\n",
        "                                            vocab_index_dict, index_vocab_dict,\n",
        "                                            temperature=temperature,\n",
        "                                            max_prob=max_prob)\n",
        "            print('Sampled text is:\\n%s' % sample)\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WGho1-Q_p3w",
        "outputId": "2be324de-d017-40ac-ef79-4c57e5d43986"
      },
      "source": [
        "train(\"./drake_data.txt\", 30, 256, 4, 64, \"./drake_output_hidden256_layers4_batch64\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "All final and intermediate outputs will be stored in ./drake_output_hidden256_layers4_batch64/\n",
            "All information will be logged to stdout\n",
            "============================================================\n",
            "\n",
            "09:38:09 INFO:Parameters are:\n",
            "{\n",
            "    \"batch_size\": 64,\n",
            "    \"dropout\": 0.0,\n",
            "    \"embedding_size\": 0,\n",
            "    \"hidden_size\": 256,\n",
            "    \"input_dropout\": 0.0,\n",
            "    \"learning_rate\": 0.0001,\n",
            "    \"max_grad_norm\": 5,\n",
            "    \"num_layers\": 4,\n",
            "    \"num_unrollings\": 10\n",
            "}\n",
            "\n",
            "Reading data from: %s ./drake_data.txt\n",
            "09:38:09 INFO:Number of characters: 769592\n",
            "09:38:09 INFO:Creating train, valid, test split\n",
            "09:38:09 INFO:Creating vocabulary\n",
            "09:38:09 INFO:Vocabulary is saved in ./drake_output_hidden256_layers4_batch64/vocab.json\n",
            "09:38:09 INFO:Vocab size: 104\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "09:38:09 WARNING:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "09:38:11 WARNING:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "09:38:11 WARNING:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "09:38:11 INFO:Model size (number of parameters): 1972328\n",
            "\n",
            "09:38:11 INFO:Start training\n",
            "\n",
            "09:38:12 INFO:=================== Epoch 1: 1/1===================\n",
            "\n",
            "09:38:12 INFO:Training on training set\n",
            "09:38:23 INFO:Perplexity: 26.858, speed: 54570 words per sec\n",
            "09:38:24 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-962\n",
            "\n",
            "09:38:24 INFO:Evaluate on validation set\n",
            "09:38:25 INFO:Perplexity: 24.755, speed: 95755 words per sec\n",
            "09:38:25 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-962\n",
            "09:38:25 INFO:Best validation ppl is 24.754810\n",
            "\n",
            "09:38:25 INFO:=================== Epoch 2: 1/1===================\n",
            "\n",
            "09:38:25 INFO:Training on training set\n",
            "09:38:36 INFO:Perplexity: 21.660, speed: 56820 words per sec\n",
            "09:38:36 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-1924\n",
            "\n",
            "09:38:36 INFO:Evaluate on validation set\n",
            "09:38:37 INFO:Perplexity: 16.579, speed: 94539 words per sec\n",
            "09:38:37 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-1924\n",
            "09:38:37 INFO:Best validation ppl is 16.579472\n",
            "\n",
            "09:38:37 INFO:=================== Epoch 3: 1/1===================\n",
            "\n",
            "09:38:37 INFO:Training on training set\n",
            "09:38:48 INFO:Perplexity: 13.986, speed: 56812 words per sec\n",
            "09:38:48 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-2886\n",
            "\n",
            "09:38:48 INFO:Evaluate on validation set\n",
            "09:38:49 INFO:Perplexity: 12.256, speed: 95582 words per sec\n",
            "09:38:49 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-2886\n",
            "09:38:49 INFO:Best validation ppl is 12.256418\n",
            "\n",
            "09:38:49 INFO:=================== Epoch 4: 1/1===================\n",
            "\n",
            "09:38:49 INFO:Training on training set\n",
            "09:39:00 INFO:Perplexity: 10.424, speed: 56767 words per sec\n",
            "09:39:00 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-3848\n",
            "\n",
            "09:39:00 INFO:Evaluate on validation set\n",
            "09:39:01 INFO:Perplexity: 9.878, speed: 94633 words per sec\n",
            "09:39:02 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-3848\n",
            "09:39:02 INFO:Best validation ppl is 9.878149\n",
            "\n",
            "09:39:02 INFO:=================== Epoch 5: 1/1===================\n",
            "\n",
            "09:39:02 INFO:Training on training set\n",
            "09:39:12 INFO:Perplexity: 8.922, speed: 57024 words per sec\n",
            "09:39:13 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-4810\n",
            "\n",
            "09:39:13 INFO:Evaluate on validation set\n",
            "09:39:14 INFO:Perplexity: 8.970, speed: 93170 words per sec\n",
            "09:39:14 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-4810\n",
            "09:39:14 INFO:Best validation ppl is 8.969793\n",
            "\n",
            "09:39:14 INFO:=================== Epoch 6: 1/1===================\n",
            "\n",
            "09:39:14 INFO:Training on training set\n",
            "09:39:25 INFO:Perplexity: 8.081, speed: 56818 words per sec\n",
            "09:39:25 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-5772\n",
            "\n",
            "09:39:25 INFO:Evaluate on validation set\n",
            "09:39:26 INFO:Perplexity: 8.359, speed: 93980 words per sec\n",
            "09:39:26 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-5772\n",
            "09:39:26 INFO:Best validation ppl is 8.359343\n",
            "\n",
            "09:39:26 INFO:=================== Epoch 7: 1/1===================\n",
            "\n",
            "09:39:26 INFO:Training on training set\n",
            "09:39:37 INFO:Perplexity: 7.460, speed: 57147 words per sec\n",
            "09:39:37 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-6734\n",
            "\n",
            "09:39:37 INFO:Evaluate on validation set\n",
            "09:39:38 INFO:Perplexity: 7.881, speed: 94297 words per sec\n",
            "09:39:38 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-6734\n",
            "09:39:38 INFO:Best validation ppl is 7.881495\n",
            "\n",
            "09:39:38 INFO:=================== Epoch 8: 1/1===================\n",
            "\n",
            "09:39:38 INFO:Training on training set\n",
            "09:39:49 INFO:Perplexity: 6.960, speed: 56958 words per sec\n",
            "09:39:49 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-7696\n",
            "\n",
            "09:39:49 INFO:Evaluate on validation set\n",
            "09:39:50 INFO:Perplexity: 7.492, speed: 94152 words per sec\n",
            "09:39:50 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-7696\n",
            "09:39:50 INFO:Best validation ppl is 7.492360\n",
            "\n",
            "09:39:50 INFO:=================== Epoch 9: 1/1===================\n",
            "\n",
            "09:39:50 INFO:Training on training set\n",
            "09:40:01 INFO:Perplexity: 6.546, speed: 57147 words per sec\n",
            "09:40:01 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-8658\n",
            "\n",
            "09:40:01 INFO:Evaluate on validation set\n",
            "09:40:02 INFO:Perplexity: 7.116, speed: 94158 words per sec\n",
            "09:40:03 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-8658\n",
            "09:40:03 INFO:Best validation ppl is 7.115686\n",
            "\n",
            "09:40:03 INFO:=================== Epoch 10: 1/1===================\n",
            "\n",
            "09:40:03 INFO:Training on training set\n",
            "09:40:13 INFO:Perplexity: 6.205, speed: 57075 words per sec\n",
            "09:40:14 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-9620\n",
            "\n",
            "09:40:14 INFO:Evaluate on validation set\n",
            "09:40:15 INFO:Perplexity: 6.839, speed: 94937 words per sec\n",
            "09:40:15 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-9620\n",
            "09:40:15 INFO:Best validation ppl is 6.838656\n",
            "\n",
            "09:40:15 INFO:=================== Epoch 11: 1/1===================\n",
            "\n",
            "09:40:15 INFO:Training on training set\n",
            "09:40:26 INFO:Perplexity: 5.924, speed: 56974 words per sec\n",
            "09:40:26 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-10582\n",
            "\n",
            "09:40:26 INFO:Evaluate on validation set\n",
            "09:40:27 INFO:Perplexity: 6.601, speed: 92469 words per sec\n",
            "09:40:27 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-10582\n",
            "09:40:27 INFO:Best validation ppl is 6.600520\n",
            "\n",
            "09:40:27 INFO:=================== Epoch 12: 1/1===================\n",
            "\n",
            "09:40:27 INFO:Training on training set\n",
            "09:40:38 INFO:Perplexity: 5.684, speed: 57084 words per sec\n",
            "09:40:38 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-11544\n",
            "\n",
            "09:40:38 INFO:Evaluate on validation set\n",
            "09:40:39 INFO:Perplexity: 6.391, speed: 94228 words per sec\n",
            "09:40:39 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-11544\n",
            "09:40:39 INFO:Best validation ppl is 6.390961\n",
            "\n",
            "09:40:39 INFO:=================== Epoch 13: 1/1===================\n",
            "\n",
            "09:40:39 INFO:Training on training set\n",
            "09:40:50 INFO:Perplexity: 5.483, speed: 56963 words per sec\n",
            "09:40:50 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-12506\n",
            "\n",
            "09:40:50 INFO:Evaluate on validation set\n",
            "09:40:51 INFO:Perplexity: 6.212, speed: 96044 words per sec\n",
            "09:40:51 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-12506\n",
            "09:40:51 INFO:Best validation ppl is 6.211989\n",
            "\n",
            "09:40:51 INFO:=================== Epoch 14: 1/1===================\n",
            "\n",
            "09:40:51 INFO:Training on training set\n",
            "09:41:02 INFO:Perplexity: 5.310, speed: 57004 words per sec\n",
            "09:41:02 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-13468\n",
            "\n",
            "09:41:02 INFO:Evaluate on validation set\n",
            "09:41:03 INFO:Perplexity: 6.075, speed: 93132 words per sec\n",
            "09:41:04 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-13468\n",
            "09:41:04 INFO:Best validation ppl is 6.074881\n",
            "\n",
            "09:41:04 INFO:=================== Epoch 15: 1/1===================\n",
            "\n",
            "09:41:04 INFO:Training on training set\n",
            "09:41:14 INFO:Perplexity: 5.158, speed: 57373 words per sec\n",
            "09:41:15 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-14430\n",
            "\n",
            "09:41:15 INFO:Evaluate on validation set\n",
            "09:41:15 INFO:Perplexity: 5.953, speed: 94355 words per sec\n",
            "09:41:16 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-14430\n",
            "09:41:16 INFO:Best validation ppl is 5.953158\n",
            "\n",
            "09:41:16 INFO:=================== Epoch 16: 1/1===================\n",
            "\n",
            "09:41:16 INFO:Training on training set\n",
            "09:41:27 INFO:Perplexity: 5.022, speed: 57001 words per sec\n",
            "09:41:27 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-15392\n",
            "\n",
            "09:41:27 INFO:Evaluate on validation set\n",
            "09:41:28 INFO:Perplexity: 5.845, speed: 95881 words per sec\n",
            "09:41:28 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-15392\n",
            "09:41:28 INFO:Best validation ppl is 5.845378\n",
            "\n",
            "09:41:28 INFO:=================== Epoch 17: 1/1===================\n",
            "\n",
            "09:41:28 INFO:Training on training set\n",
            "09:41:39 INFO:Perplexity: 4.900, speed: 56889 words per sec\n",
            "09:41:39 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-16354\n",
            "\n",
            "09:41:39 INFO:Evaluate on validation set\n",
            "09:41:40 INFO:Perplexity: 5.749, speed: 92069 words per sec\n",
            "09:41:40 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-16354\n",
            "09:41:40 INFO:Best validation ppl is 5.749294\n",
            "\n",
            "09:41:40 INFO:=================== Epoch 18: 1/1===================\n",
            "\n",
            "09:41:40 INFO:Training on training set\n",
            "09:41:51 INFO:Perplexity: 4.790, speed: 56909 words per sec\n",
            "09:41:51 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-17316\n",
            "\n",
            "09:41:51 INFO:Evaluate on validation set\n",
            "09:41:52 INFO:Perplexity: 5.667, speed: 92561 words per sec\n",
            "09:41:52 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-17316\n",
            "09:41:52 INFO:Best validation ppl is 5.667487\n",
            "\n",
            "09:41:52 INFO:=================== Epoch 19: 1/1===================\n",
            "\n",
            "09:41:52 INFO:Training on training set\n",
            "09:42:03 INFO:Perplexity: 4.689, speed: 57183 words per sec\n",
            "09:42:03 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-18278\n",
            "\n",
            "09:42:03 INFO:Evaluate on validation set\n",
            "09:42:04 INFO:Perplexity: 5.584, speed: 94409 words per sec\n",
            "09:42:05 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-18278\n",
            "09:42:05 INFO:Best validation ppl is 5.584162\n",
            "\n",
            "09:42:05 INFO:=================== Epoch 20: 1/1===================\n",
            "\n",
            "09:42:05 INFO:Training on training set\n",
            "09:42:15 INFO:Perplexity: 4.597, speed: 57091 words per sec\n",
            "09:42:16 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-19240\n",
            "\n",
            "09:42:16 INFO:Evaluate on validation set\n",
            "09:42:17 INFO:Perplexity: 5.526, speed: 93591 words per sec\n",
            "09:42:17 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-19240\n",
            "09:42:17 INFO:Best validation ppl is 5.526398\n",
            "\n",
            "09:42:17 INFO:=================== Epoch 21: 1/1===================\n",
            "\n",
            "09:42:17 INFO:Training on training set\n",
            "09:42:28 INFO:Perplexity: 4.512, speed: 57136 words per sec\n",
            "09:42:28 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-20202\n",
            "\n",
            "09:42:28 INFO:Evaluate on validation set\n",
            "09:42:29 INFO:Perplexity: 5.443, speed: 95487 words per sec\n",
            "09:42:29 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-20202\n",
            "09:42:29 INFO:Best validation ppl is 5.442903\n",
            "\n",
            "09:42:29 INFO:=================== Epoch 22: 1/1===================\n",
            "\n",
            "09:42:29 INFO:Training on training set\n",
            "09:42:40 INFO:Perplexity: 4.433, speed: 56954 words per sec\n",
            "09:42:40 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-21164\n",
            "\n",
            "09:42:40 INFO:Evaluate on validation set\n",
            "09:42:41 INFO:Perplexity: 5.395, speed: 94490 words per sec\n",
            "09:42:41 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-21164\n",
            "09:42:41 INFO:Best validation ppl is 5.394661\n",
            "\n",
            "09:42:41 INFO:=================== Epoch 23: 1/1===================\n",
            "\n",
            "09:42:41 INFO:Training on training set\n",
            "09:42:52 INFO:Perplexity: 4.360, speed: 56817 words per sec\n",
            "09:42:52 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-22126\n",
            "\n",
            "09:42:52 INFO:Evaluate on validation set\n",
            "09:42:53 INFO:Perplexity: 5.336, speed: 92676 words per sec\n",
            "09:42:53 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-22126\n",
            "09:42:53 INFO:Best validation ppl is 5.336014\n",
            "\n",
            "09:42:53 INFO:=================== Epoch 24: 1/1===================\n",
            "\n",
            "09:42:53 INFO:Training on training set\n",
            "09:43:04 INFO:Perplexity: 4.291, speed: 56806 words per sec\n",
            "09:43:04 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-23088\n",
            "\n",
            "09:43:04 INFO:Evaluate on validation set\n",
            "09:43:05 INFO:Perplexity: 5.286, speed: 92146 words per sec\n",
            "09:43:06 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-23088\n",
            "09:43:06 INFO:Best validation ppl is 5.286347\n",
            "\n",
            "09:43:06 INFO:=================== Epoch 25: 1/1===================\n",
            "\n",
            "09:43:06 INFO:Training on training set\n",
            "09:43:16 INFO:Perplexity: 4.225, speed: 57022 words per sec\n",
            "09:43:17 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-24050\n",
            "\n",
            "09:43:17 INFO:Evaluate on validation set\n",
            "09:43:18 INFO:Perplexity: 5.240, speed: 92589 words per sec\n",
            "09:43:18 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-24050\n",
            "09:43:18 INFO:Best validation ppl is 5.239773\n",
            "\n",
            "09:43:18 INFO:=================== Epoch 26: 1/1===================\n",
            "\n",
            "09:43:18 INFO:Training on training set\n",
            "09:43:29 INFO:Perplexity: 4.164, speed: 56783 words per sec\n",
            "09:43:29 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-25012\n",
            "\n",
            "09:43:29 INFO:Evaluate on validation set\n",
            "09:43:30 INFO:Perplexity: 5.204, speed: 93525 words per sec\n",
            "09:43:30 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-25012\n",
            "09:43:30 INFO:Best validation ppl is 5.203571\n",
            "\n",
            "09:43:30 INFO:=================== Epoch 27: 1/1===================\n",
            "\n",
            "09:43:30 INFO:Training on training set\n",
            "09:43:41 INFO:Perplexity: 4.105, speed: 57054 words per sec\n",
            "09:43:41 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-25974\n",
            "\n",
            "09:43:41 INFO:Evaluate on validation set\n",
            "09:43:42 INFO:Perplexity: 5.169, speed: 92967 words per sec\n",
            "09:43:42 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-25974\n",
            "09:43:42 INFO:Best validation ppl is 5.169135\n",
            "\n",
            "09:43:42 INFO:=================== Epoch 28: 1/1===================\n",
            "\n",
            "09:43:42 INFO:Training on training set\n",
            "09:43:53 INFO:Perplexity: 4.051, speed: 56984 words per sec\n",
            "09:43:53 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-26936\n",
            "\n",
            "09:43:53 INFO:Evaluate on validation set\n",
            "09:43:54 INFO:Perplexity: 5.142, speed: 93303 words per sec\n",
            "09:43:55 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-26936\n",
            "09:43:55 INFO:Best validation ppl is 5.141673\n",
            "\n",
            "09:43:55 INFO:=================== Epoch 29: 1/1===================\n",
            "\n",
            "09:43:55 INFO:Training on training set\n",
            "09:44:05 INFO:Perplexity: 3.998, speed: 56956 words per sec\n",
            "09:44:06 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-27898\n",
            "\n",
            "09:44:06 INFO:Evaluate on validation set\n",
            "09:44:07 INFO:Perplexity: 5.111, speed: 93983 words per sec\n",
            "09:44:07 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-27898\n",
            "09:44:07 INFO:Best validation ppl is 5.110626\n",
            "\n",
            "09:44:07 INFO:=================== Epoch 30: 1/1===================\n",
            "\n",
            "09:44:07 INFO:Training on training set\n",
            "09:44:18 INFO:Perplexity: 3.949, speed: 57000 words per sec\n",
            "09:44:18 INFO:Latest model saved in ./drake_output_hidden256_layers4_batch64/save_model/model-28860\n",
            "\n",
            "09:44:18 INFO:Evaluate on validation set\n",
            "09:44:19 INFO:Perplexity: 5.097, speed: 95147 words per sec\n",
            "09:44:19 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-28860\n",
            "09:44:19 INFO:Best validation ppl is 5.096977\n",
            "\n",
            "09:44:19 INFO:Latest model is saved in ./drake_output_hidden256_layers4_batch64/save_model/model-28860\n",
            "09:44:19 INFO:Best model is saved in ./drake_output_hidden256_layers4_batch64/best_model/model-28860\n",
            "09:44:19 INFO:Best validation ppl is 5.096977\n",
            "\n",
            "09:44:19 INFO:Evaluate the best model on test set\n",
            "INFO:tensorflow:Restoring parameters from ./drake_output_hidden256_layers4_batch64/best_model/model-28860\n",
            "09:44:19 INFO:Restoring parameters from ./drake_output_hidden256_layers4_batch64/best_model/model-28860\n",
            "09:47:32 INFO:Perplexity: 4.714, speed: 398 words per sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnYsShvWK-Av",
        "outputId": "bd339089-ca09-4384-d112-c7d634b8f3ab"
      },
      "source": [
        "sample_lyrics = sampling(\"./drake_output_hidden256_layers2_batch64\", \"we go up\", 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08:52:08 INFO:Creating graph\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "08:52:08 WARNING:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n",
            "08:52:08 INFO:Summary name average loss is illegal; using average_loss instead.\n",
            "INFO:tensorflow:Restoring parameters from ./drake_output_hidden256_layers2_batch64/best_model/model-3848\n",
            "08:52:08 INFO:Restoring parameters from ./drake_output_hidden256_layers2_batch64/best_model/model-3848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sampled text is:\n",
            "we go up?\n",
            "Hold on, then\n",
            "Ayy a ressice, \"The next ineway\" than I had up nellin'? \n",
            "[Chorus: Drake]\n",
            "You won't abagh, a wings with my mole on the last night\n",
            "Look, when it's going?\n",
            "\n",
            "[Chorus]\n",
            "Women, so bromistaat, Talls, Henn these-nigga\n",
            "Lhink shots the ticket off with you, I'm all nextin' you\n",
            "Estated-doxet you coupted on the grave like *08 cateron\n",
            "My nehes tell it a, these niggas\n",
            "I'm what at rega-game you faston\n",
            "\n",
            "[Chorus]\n",
            "Ufffom long\n",
            "One conversation\n",
            "She don't think that hass a talkin' I'm thinking that?\n",
            "It's back\n",
            "There's a man? (Trust)\n",
            "You couldn't had to love you fills\n",
            "Are still be ciniy's drink like a snve my best for alst to blunclessove it in when she bolming like I'm driven on when me like the y_ar kite for you\n",
            "That's not now\n",
            "My name on you (Jeffer all on but you baisted a conec\n",
            "I was way and tyou, 'til you're doin' nigga\n",
            "\n",
            "[Chorus: LiV Wayne]\n",
            "Don't think to me, righ (brand ressected (Orill ard thingin' if you're back business\n",
            "One more, nigga Only nigga she love\n",
            "\n",
            "[Chorus: Drake]\n",
            "It's your motherfucker, don't know how it did for me\n",
            "It's doung to connuc, I spend that\n",
            "\n",
            "[Chorus: Lil Wayne]\n",
            "Before you know that I love your heartur\n",
            "We live everything the money that I'm off it\n",
            "'Cause who I'm a lot of suppared fake to like\n",
            "The date's like wa'm that you obfire if you're the op the city up\n",
            "How fuck that. (They wanna for me the way you lose that is real\n",
            "You're cause if it's done\n",
            "Only mestfire, too well you like it, real Stuck, leason tun\n",
            "\n",
            "[Chorus]\n",
            "I'm the realized for this\n",
            "Convernin' all of the kissus the hood from the strice\n",
            "\n",
            "[Bridge: Anl Tay Mulh]\n",
            "Tave 1003 in your hillion\n",
            "Doance, wild be a PM\n",
            "\n",
            "[Chorus: Drake Zich Kiss]\n",
            "Ayy?, yeah\n",
            "\n",
            "[Outro: Hil you know Drake it for us\n",
            "This playin' a lot of bad that's when you knows whise\n",
            "\n",
            "[Verse 1: Drake]\n",
            "Uncill the vizein blue what'ver arm confiss\n",
            "All of ya play games out and won't intervacting wayne strapsting by\n",
            "Parained just to give me good attentifur\n",
            "Even who 'em right now\n",
            "Wayne Care For dance, rappy, gon' go\n",
            "Don't you beginger if a lot of dion\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}