{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_and_gptv1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donganzh/csc413/blob/main/PA3/bert_and_gptv1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KSUpX1-P1uv"
      },
      "source": [
        "#Part 4 Fine-tuning for arithmetic sentiment analysis\n",
        "\n",
        "Acknowledgement: We used most of the code from https://mccormickml.com/2019/07/22/BERT-fine-tuning/ \n",
        "\n",
        "Most Credit to: \n",
        "Chris McCormick and Nick Ryan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l47jS3ifQJz3"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy13-DMJckYt"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00GDvCJucnpv"
      },
      "source": [
        "## Install transformers repo that has Bert and GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AIe26NRcuYZ"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p18ZuvMUcyQE"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkGc79g2c3DL"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJynoDMfc6Tr"
      },
      "source": [
        "## Download & Extract\n",
        "Run the following cells to downlaod the dataset files from the CSC413 webpage.\n",
        "\n",
        "<!-- Download the two csv dataset files from CSC413 webpage, click the folder icon,  -->\n",
        "<!-- and click \"upload\" to upload them.  -->\n",
        "\n",
        "<!-- https://csc413-2020.github.io/assets/misc/PA03_data_20_train.csv \n",
        "\n",
        "https://csc413-2020.github.io/assets/misc/PA03_data_20_test.csv   -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFvJibK2wEdP"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN77phG-f3DY"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading verbal arithmetic dataset')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://csc413-uoft.github.io/2021/assets/misc/'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./PA03_data_20_train.csv'):\n",
        "  wget.download(url + 'PA03_data_20_train.csv', './PA03_data_20_train.csv')\n",
        "  print('Done downloading training data')\n",
        "else:\n",
        "  print('Already downloaded training data')\n",
        "\n",
        "if not os.path.exists('./PA03_data_20_test.csv'):\n",
        "  wget.download(url + 'PA03_data_20_test.csv', './PA03_data_20_test.csv')\n",
        "  print('Done downloading test data')\n",
        "else:\n",
        "  print('Already downloaded test data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iylR87jhf440"
      },
      "source": [
        "##  Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovSDN8kBf_zF"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./PA03_data_20_train.csv\", header=0, names=[\"index\", \"input\", \"label\"])\n",
        "\n",
        "print(\"Number of data points: \", df.shape[0])\n",
        "sampled = df.sample(10)\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyNFphizgCiM"
      },
      "source": [
        "## Bert Tokenizer\n",
        "\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JazZH2kzgHF_"
      },
      "source": [
        "# try bert tokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJi4x1sgYKX"
      },
      "source": [
        "inputs = df.input.values\n",
        "labels = df.label.values\n",
        "print(\"Train data size \", len(inputs))\n",
        "print(' Original: ', inputs[0])\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', bert_tokenizer.tokenize(inputs[0]))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(inputs[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iznyPgnlgZTI"
      },
      "source": [
        "We can actually use the `tokenizer.__call__` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RenfgCJg4Fi"
      },
      "source": [
        "## BERT Required Formatting\n",
        "\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWOVuJohI3q"
      },
      "source": [
        "### Special Tokens\n",
        "\n",
        "**`[SEP]`**\n",
        "\n",
        "At the end of every sentence, we need to append the special `[SEP]` token. \n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n",
        "\n",
        "**`[CLS]`**\n",
        "\n",
        "For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n",
        "\n",
        "On the output of the final transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n",
        "\n",
        ">  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\n",
        "corresponding to this token is used as the aggregate sequence representation for classification\n",
        "tasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Also, because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrvaiOR2hAEH"
      },
      "source": [
        "### Sentence Length & Attention Mask\n",
        "\n",
        "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
        "\n",
        "BERT has two constraints:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. \n",
        "\n",
        "The \"Attention Mask\" is simply an array of 0s and 1s indicating which tokens are padding and which aren't "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-jDt4i3hFea"
      },
      "source": [
        "## Sentences to IDs\n",
        "\n",
        "The `tokenizer.__call__` function combines multiple steps for us:\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special tokens including `[CLS]` and `[SEP]` .\n",
        "3. Map the tokens to their IDs.\n",
        "4. Handle padding and assign attention mask accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rzdc-TGqf7E"
      },
      "source": [
        "In our dataset, all sentences have three word tokens. However, we set the max length of sentence to 7 in this example to show what paddings will be in real world applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irjbFrNapXjn"
      },
      "source": [
        "# Set the maximum sequence length.\n",
        "MAX_LEN = 7\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(bert_tokenizer.pad_token, bert_tokenizer.pad_token_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3JJmMOdqeoO"
      },
      "source": [
        "# pytorch version\n",
        "\n",
        "# `tokenizer.__call__` will:\n",
        "#   (1) Tokenize the sentence.\n",
        "#   (2) Prepend the `[CLS]` token to the start.\n",
        "#   (3) Append the `[SEP]` token to the end.\n",
        "#   (4) Map tokens to their IDs.\n",
        "#   (5) Add paddings.\n",
        "#   (6) Provide attention masks.\n",
        "tokenized_inputs = bert_tokenizer(\n",
        "                    inputs.tolist(),           # Sentence to encode.\n",
        "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                    padding = 'max_length',    # pad to a length specified \n",
        "                                               # by the max_length.\n",
        "                    max_length = MAX_LEN,      # Truncate all sentences.\n",
        "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_masks = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', inputs[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Tokenized:', bert_tokenizer.decode(input_ids[0]))\n",
        "print('Attention_mask', attention_masks[0])\n",
        "\n",
        "bert_input_ids = input_ids\n",
        "bert_attention_masks = attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNu-FquIsSYM"
      },
      "source": [
        "## GPT Tokenizer\n",
        "\n",
        "Similar to the BERT Tokenizer, but we have to explicitly add the padding token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qq32MFPtKQp"
      },
      "source": [
        "from transformers import OpenAIGPTTokenizer\n",
        "\n",
        "print('Loading GPT tokenizer...')\n",
        "gpt_tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', do_lower_case=True)\n",
        "special_tokens_dict = {'pad_token': '<PAD>'}\n",
        "gpt_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "print('pad_token', gpt_tokenizer.pad_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfDTJ20uapV"
      },
      "source": [
        "## GPT Required Formatting\n",
        "\n",
        "There is no special token in the pretraining stage of GPT. In fine tuning, although the original paper suggested using \"start\" and \"extract\" tokens, here we follow the implementation of HuggingFace that we are going to simply use the representation of the last token in the sentence for later classification. \n",
        "\n",
        "Therefore, we only need to add padding tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DUEmEVWvt11"
      },
      "source": [
        "# pytorch version\n",
        "\n",
        "# `tokenizer.__call__` will:\n",
        "#   (1) Tokenize the sentence.\n",
        "#   (2) Prepend the `[CLS]` token to the start.\n",
        "#   (3) Append the `[SEP]` token to the end.\n",
        "#   (4) Map tokens to their IDs.\n",
        "#   (5) Add paddings.\n",
        "#   (6) Provide attention masks.\n",
        "tokenized_inputs = gpt_tokenizer(\n",
        "                    inputs.tolist(),           # Sentence to encode.\n",
        "                    padding = 'max_length',    # pad to a length specified \n",
        "                                               # by the max_length.\n",
        "                    max_length = MAX_LEN,      # Truncate all sentences.\n",
        "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_masks = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', inputs[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Tokenized:', gpt_tokenizer.decode(input_ids[0]))\n",
        "print('Attention_mask', attention_masks[0])\n",
        "\n",
        "gpt_input_ids = input_ids\n",
        "gpt_attention_masks = attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCMCg2B-xHZU"
      },
      "source": [
        "## Training & Validation Split\n",
        "\n",
        "Divide up our training set to use 80% for training and 20% for validation.\n",
        "\n",
        "We'also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4WUEsPnxege"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def train_valid_split(input_ids, attention_masks):\n",
        "  # Use 80% for training and 20% for validation.\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                              random_state=2021, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                              random_state=2021, test_size=0.2)\n",
        "\n",
        "  set(labels)\n",
        "  print('example train_input:', train_inputs[0])\n",
        "  print('example attention_mask', train_masks[0])\n",
        "\n",
        "  train_labels = torch.tensor(train_labels)\n",
        "  validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW8M1GiAx14E"
      },
      "source": [
        "bert_train_dataloader, bert_validation_dataloader = train_valid_split(bert_input_ids, bert_attention_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEX1omYS0SWB"
      },
      "source": [
        "gpt_train_dataloader, gpt_validation_dataloader = train_valid_split(gpt_input_ids, gpt_attention_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_NPA-o0m3v"
      },
      "source": [
        "# 4. Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkp-UJdO0xbF"
      },
      "source": [
        "## Question1 [1pts]\n",
        "\n",
        "The pre-trained neural network here is the normal BERT model from [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). The goal is to add a new classification layer to the pre-trained model. We have provided two example classes to do so.\n",
        "\n",
        "In this part, you need to make your own  `GPTCSC413` class `self.classifier` which will build on a pretrained [gpt](https://huggingface.co/openai-gpt) model and add a clsddifier at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbG9zbiz05OT"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "class BertCSC413_Linear(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertCSC413_Linear, self).__init__(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "class BertCSC413_MLP(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertCSC413_MLP, self).__init__(config)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6EPYj9d11cm"
      },
      "source": [
        "from transformers import OpenAIGPTForSequenceClassification\n",
        "class GPTCSC413(OpenAIGPTForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(GPTCSC413, self).__init__(config)\n",
        "        # Your own classifier goes here\n",
        "        # score is name for the classifier in OpenAIGPTForSequenceClassification, similar as the self.classifier in BertForSequenceClassification\n",
        "        self.score = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmeBvaXY2Mcm"
      },
      "source": [
        "## Question2 [0pts]\n",
        "\n",
        "We instantiated two different BERT models from `BertCSC413_MLP` class, which are called `model_freeze_bert` and `model_finetune_bert` in the notebook. \n",
        "\n",
        "**Run** the following code to train the models, and attach the training error curves of `model_freeze_bert` and `model_finetune_bert`.  \n",
        "    \n",
        "Comment on how these two models will differ during the training? Which one would lead to smaller training errors? Which one would generalize better? And briefly discuss why models are failing under certain target labels.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sPmVD0P3CyS"
      },
      "source": [
        "from transformers import AdamW, BertConfig\n",
        "\n",
        "model_freeze_bert = BertCSC413_MLP.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 3, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Y078xU3OGy"
      },
      "source": [
        "for name, param in model_freeze_bert.named_parameters():\n",
        "\tif 'classifier' not in name: # classifier layer\n",
        "\t\tparam.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeM-sMbq3Tem"
      },
      "source": [
        "model_finetune_bert = BertCSC413_Linear.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 3,    \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEuhZX6f3XiI"
      },
      "source": [
        "# Model parameters visualization\n",
        "params = list(model_finetune_bert.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-io5NMws3ckc"
      },
      "source": [
        "We use\n",
        "- Batch size: 32\n",
        "- Learning rate (Adam): 2e-5  \n",
        "- Number of epochs: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_gVnpy13iqC"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, validation_dataloader):      \n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "    loss_values = []\n",
        "    eval_accs = []\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input_ids = batch[0] #.to(device)\n",
        "            b_input_mask = batch[1] #.to(device)\n",
        "            b_labels = batch[2] #.to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # This will return the loss (rather than the model output) because we\n",
        "            # have provided the `labels`.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "            \n",
        "            # The call to `model` always returns a tuple, so we need to pull the \n",
        "            # loss value out of the tuple.\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value \n",
        "            # from the tensor.\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "            \n",
        "\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            # batch = tuple(t.to(device) for t in batch)\n",
        "            batch = tuple(t for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            \n",
        "            with torch.no_grad():        \n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask)\n",
        "            \n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        avg_eval_acc = eval_accuracy/nb_eval_steps\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_eval_acc))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "        eval_accs.append(avg_eval_acc)\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    return loss_values, eval_accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "__uaGXF13jXY"
      },
      "source": [
        "freeze_bert_loss_vals, freeze_bert_eval_accs = train_model(model_freeze_bert, 4, \n",
        "                                    bert_train_dataloader,\n",
        "                                    bert_validation_dataloader) # about 1 minute for 4 epochs using CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3k7F9xJD3pP4"
      },
      "source": [
        "finetune_bert_loss_vals, finetune_bert_eval_accs = train_model(model_finetune_bert, 4,\n",
        "                                                               bert_train_dataloader,\n",
        "                                                               bert_validation_dataloader) # about 5 minutes for 4 epochs using CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o3J-c8wQ5gDj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "def plot_loss_and_acc(loss_vals, eval_accs):\n",
        "    sns.set(style='darkgrid')\n",
        "    sns.set(font_scale=1.5)\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "    fig, ax1 = plt.subplots(1,1)\n",
        "    ax1.plot(loss_vals, 'b-o', label = 'training loss')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(eval_accs, 'y-o', label = 'validation accuracy')\n",
        "    ax2.set_title(\"Training loss and validation accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\", color='b')\n",
        "    ax2.set_ylabel(\"Accuracy\", color='y')\n",
        "    ax1.tick_params(axis='y', rotation=0, labelcolor='b' )\n",
        "    ax2.tick_params(axis='y', rotation=0, labelcolor='y' )\n",
        "    plt.show()\n",
        "plot_loss_and_acc(freeze_bert_loss_vals, freeze_bert_eval_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c1dYIrtT5kIu"
      },
      "source": [
        "plot_loss_and_acc(finetune_bert_loss_vals, finetune_bert_eval_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfraCnR5xP8"
      },
      "source": [
        "*Run* the evaluation functions.  Report the test performances of using trained model\\_freeze\\_bert and model\\_finetune\\_bert, and briefly discuss why models are failing under certain target labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x9pLcGpm52rU"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "#df = pd.read_csv(\"./output.txt_test.csv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df = pd.read_csv(\"./PA03_data_20_test.csv\", header=0, names=[\"index\", \"input\", \"label\"])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "inputs = df.input.values\n",
        "labels = df.label.values\n",
        "\n",
        "def make_test_data(tokenizer):\n",
        "  tokenized_inputs = tokenizer(\n",
        "                      inputs.tolist(),                     # Sentence to encode.\n",
        "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                      padding = 'max_length',    # pad to a length specified \n",
        "                                                  # by the max_length.\n",
        "                      max_length = 7,            # Truncate all sentences.\n",
        "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                  )\n",
        "\n",
        "  input_ids = tokenized_inputs['input_ids']\n",
        "  attention_masks = tokenized_inputs['attention_mask']\n",
        "\n",
        "\n",
        "  # Convert to tensors.\n",
        "  prediction_inputs = torch.tensor(input_ids)\n",
        "  prediction_masks = torch.tensor(attention_masks)\n",
        "  prediction_labels = torch.tensor(labels)\n",
        "\n",
        "  # Set the batch size.  \n",
        "  batch_size = 32  \n",
        "\n",
        "  # Create the DataLoader.\n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "  return prediction_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7IZPn1SR65yw"
      },
      "source": [
        "bert_test_dataloader = make_test_data(bert_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CfxsJnQU7I6e"
      },
      "source": [
        "def eval_testdata(model_test, prediction_dataloader, tokenizer, show_all_predictions=False):\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(prediction_dataloader.dataset)))\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model_test.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    predictions , true_labels, input_sents = [], [], []\n",
        "\n",
        "    # Predict \n",
        "    for batch in prediction_dataloader:\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        decoded_inputs = [tokenizer.decode(b_input_ids[i]).strip(\"[CLS] \").strip( \"[SEP] \").strip(\" [PAD] \").strip(\"[PAD]\").strip(\" [SE\") for i in range(len(b_input_ids))]\n",
        "        # Telling the model not to compute or store gradients, saving memory and \n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model_test(b_input_ids, token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        input_sents.extend(decoded_inputs)\n",
        "        \n",
        "        # Store predictions and true labels\n",
        "        predictions.extend(np.argmax(logits, axis=1))\n",
        "        true_labels.extend(label_ids)\n",
        "    correct = [0,0,0]\n",
        "    totals = [0, 0, 0]\n",
        "    for true_label, prediction in zip(true_labels, predictions):\n",
        "        if true_label == prediction:\n",
        "            correct[true_label] += 1\n",
        "        totals[true_label] += 1\n",
        "    print(\"Number of expressions with negative result\", true_labels.count(0), \"\\n\",  correct[0],  \" predicted correctly\",  \", accuracy \", correct[0]/totals[0] , \"\\n\")\n",
        "    print(\"Number of expressions with 0 result\", true_labels.count(1), \"\\n\",  correct[1], \" predicted correctly\", \", accuracy \", correct[1]/totals[1], \"\\n\")\n",
        "    print(\"Number of expressions with positive result\", true_labels.count(2),\"\\n\",  correct[2], \" predicted correctly\",\", accuracy \", correct[2]/totals[2], \"\\n\")\n",
        "    if show_all_predictions:\n",
        "        index_to_sentiment_map = {0:\"negative\", 1:\"zero\", 2:\"positive\"}\n",
        "        for sent in [sent + \"--> \"+index_to_sentiment_map[index]  for sent, index in zip(input_sents, predictions)]:\n",
        "            print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "deSFj2hs71g6"
      },
      "source": [
        "eval_testdata(model_freeze_bert, bert_test_dataloader, bert_tokenizer, show_all_predictions=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nhg9DIg28DCF"
      },
      "source": [
        "eval_testdata(model_finetune_bert, bert_test_dataloader, bert_tokenizer, show_all_predictions=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAFzVhML8mru"
      },
      "source": [
        "## Question3 [0pts]\n",
        "\n",
        "We instantiated a GPT model from `GPTCSC413` class, which is called `model_finetune_gpt`. **Run** the training and evaluation functions for the model.\n",
        "    \n",
        "Compare the performance of `model_finetune_bert` and `model_finetune_gpt`. Try a few unseen examples of arithmetic questions using both model, and find 10 interesting results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K1IzXoJ09O-f"
      },
      "source": [
        "model_finetune_gpt = GPTCSC413.from_pretrained(\n",
        "    \"openai-gpt\", \n",
        "    num_labels = 3,    \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")\n",
        "# resize model embedding to match new tokenizer\n",
        "model_finetune_gpt.resize_token_embeddings(len(gpt_tokenizer))\n",
        "\n",
        "# fix model padding token id\n",
        "model_finetune_gpt.config.pad_token_id = gpt_tokenizer.pad_token_id\n",
        "print('pad token set to: ', model_finetune_gpt.config.pad_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dc9cSULR91Wj"
      },
      "source": [
        "finttune_gpt_loss_values, finttune_gpt_eval_accs = train_model(model_finetune_gpt, 4, \n",
        "                                                               gpt_train_dataloader,\n",
        "                                                               gpt_validation_dataloader) # about 5 minutes for 4 epochs using CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tHo0UO9P98WG"
      },
      "source": [
        "plot_loss_and_acc(finttune_gpt_loss_values, finttune_gpt_eval_accs)\n",
        "gpt_test_dataloader = make_test_data(gpt_tokenizer)\n",
        "eval_testdata(model_finetune_gpt, gpt_test_dataloader, gpt_tokenizer, show_all_predictions=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfHnZz-p-rTd"
      },
      "source": [
        "index_to_sentiment_map = {0:\"negative\", 1:\"zero\", 2:\"positive\"}\n",
        "def what_is(arithmetic_input, model=model_finetune_gpt):\n",
        "    if isinstance(model, GPTCSC413):\n",
        "      tokenizer = gpt_tokenizer\n",
        "    elif isinstance(model, BertForSequenceClassification):\n",
        "      tokenizer = bert_tokenizer\n",
        "    else:\n",
        "      raise ValueError\n",
        "    tokenized_inputs = tokenizer(\n",
        "                        arithmetic_input,                     # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        padding = 'max_length',    # pad to a length specified \n",
        "                                                    # by the max_length.\n",
        "                        max_length = 7,            # Truncate all sentences.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "\n",
        "    input_ids = tokenized_inputs['input_ids']\n",
        "    attention_masks = tokenized_inputs['attention_mask']\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, \n",
        "                            attention_mask=attention_masks)\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        print(index_to_sentiment_map[np.argmax(logits, axis=1)[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbr2Rm9pAuP7"
      },
      "source": [
        "what_is(\"three minus two minus two\", model=model_finetune_gpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CzHCK4qAzo_"
      },
      "source": [
        "what_is(\"three minus two minus two\", model=model_finetune_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S-7m67wBXgD"
      },
      "source": [
        "what_is(\"three minus 1\", model=model_finetune_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2azMw2tniPg2"
      },
      "source": [
        "## Question4 [1pts]\n",
        "\n",
        "Come up with 1 scenario/application that GPT architecture is more preferred than BERT. The proposed scenario/appication is not limited to sentiment classification or Natual Language Processing (NLP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpw-i3rsid4v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCxlwwp1BhdF"
      },
      "source": [
        "## Question5 [0pts]\n",
        "\n",
        "This is an open question, and we will give marks as long as you show an attempt to try one of the following tasks.   \n",
        "1. Try data augmentation tricks to improve the performances for certain target labels that models were failing to predict.  \n",
        "2. Make a t-sne or PCA plot to visualize the embedding vectors of word tokens related to arithmetic expressions. \n",
        "3. Try different hyperparameter tunings. E.g. learning rates, optimizer, architecture of the classifier, training epochs, and batch size.  \n",
        "4. Evaluate the Multi-class Matthews correlation score for our imbalanced test dataset.  \n",
        "5. Run a baseline model using MLP without pre-trained BERT or GPT. You can assume the sequence length of all the data is 3 in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHSdx-ECBwk-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}