{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "ecEq4TP2lZ4Z",
        "RWwA6OGqlaTq",
        "AJSafHSAmu_w",
        "73_p8d5EmvOJ",
        "vYPae08Io1Fi",
        "9tcpUFKqo2Oi",
        "z1hDi020rT36",
        "MBnBXRG8mvcn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donganzh/csc413/blob/main/PA3/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "f76d27f1-dea5-45d2-cf97-fe51ebc330ac"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/80/eca7a2d1a3c2dafb960f32f844d570de988e609f5fd17de92e1cf6a01b0a/Pillow-4.0.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 22.7MB/s \n",
            "\u001b[?25hCollecting olefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 54.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow, olefile\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl size=1007168 sha256=28c6174ac02833098b3936554f9ecfdca3c4d34f87ed987a6e33fae4fde4d4cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=75ff6e06978e12b51951f17aa4a32022e964bf0a2b68672eb8be7634ea7c9545\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built Pillow olefile\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: olefile, Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0 olefile-0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dc4b0f-a706-4a8c-dc7c-6b01d94122d9"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(MyLSTMCell, self).__init__()\r\n",
        "\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        # ------------\r\n",
        "        # FILL THIS IN\r\n",
        "        # ------------\r\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\r\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, h_prev, c_prev):\r\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\r\n",
        "\r\n",
        "        Arguments\r\n",
        "            x: batch_size x input_size\r\n",
        "            h_prev: batch_size x hidden_size\r\n",
        "            c_prev: batch_size x hidden_size\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            h_new: batch_size x hidden_size\r\n",
        "            c_new: batch_size x hidden_size\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # ------------\r\n",
        "        # FILL THIS IN\r\n",
        "        # ------------\r\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\r\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\r\n",
        "        g = torch.tanh(self.Wig(x) + self.Whg(h_prev))\r\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\r\n",
        "        c_new = f * c_prev + i * g\r\n",
        "        h_new = o * torch.tanh(c_new)\r\n",
        "        return h_new, c_new"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47bc9b6-057a-4654-ff2a-7c9c70778e7a"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "rnn_args_s = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True,\r\n",
        "              'nepochs':50,\r\n",
        "              'checkpoint_dir':\"checkpoints\",\r\n",
        "              'learning_rate':0.005,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'early_stopping_patience':20,\r\n",
        "              'batch_size':64,\r\n",
        "              'hidden_size':32,\r\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\r\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\r\n",
        "              'attention_type': '',  # options: additive / scaled_dot\r\n",
        "}\r\n",
        "rnn_args_s.update(args_dict)\r\n",
        "\r\n",
        "print_opts(rnn_args_s)\r\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('nerves', 'ervesnay')\n",
            "('concerto', 'oncertocay')\n",
            "('went', 'entway')\n",
            "('hated', 'atedhay')\n",
            "('evenings', 'eveningsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.360 | Val loss: 2.004 | Gen: ay-ay ay-ay ingengway-ay ay-ay ingay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.874 | Val loss: 1.811 | Gen: ay-ingway ay-ay-ay ongway-ingway ingway ongway-ay-ay\n",
            "Epoch:   2 | Train loss: 1.691 | Val loss: 1.678 | Gen: ay-ongway ay-atingway ondingway-ongway ingway illway-ongway\n",
            "Epoch:   3 | Train loss: 1.556 | Val loss: 1.560 | Gen: eay-ationway aray ondenday-ationway insay oringway\n",
            "Epoch:   4 | Train loss: 1.443 | Val loss: 1.485 | Gen: eay-atway aray-ay ondingstay-onay incay orway\n",
            "Epoch:   5 | Train loss: 1.342 | Val loss: 1.395 | Gen: eay-ay arway oncingstay-onday inway orway\n",
            "Epoch:   6 | Train loss: 1.247 | Val loss: 1.322 | Gen: etay arway oncincingway inway orrinway\n",
            "Epoch:   7 | Train loss: 1.163 | Val loss: 1.320 | Gen: etay arway oncintinghay inway orinway\n",
            "Epoch:   8 | Train loss: 1.109 | Val loss: 1.255 | Gen: etay ariway incondiontway inway oringway\n",
            "Epoch:   9 | Train loss: 1.043 | Val loss: 1.200 | Gen: ehay ariway oncincingway inway oringway\n",
            "Epoch:  10 | Train loss: 0.976 | Val loss: 1.143 | Gen: ehay airway oncindiontway insway orrinway\n",
            "Epoch:  11 | Train loss: 0.920 | Val loss: 1.089 | Gen: ethay ariway oncindionthay inway ordinay\n",
            "Epoch:  12 | Train loss: 0.861 | Val loss: 1.069 | Gen: ehay airway oncinditytay inway oringway\n",
            "Epoch:  13 | Train loss: 0.834 | Val loss: 1.092 | Gen: ehay airway oncintinghay isway orhay\n",
            "Epoch:  14 | Train loss: 0.804 | Val loss: 1.021 | Gen: ehay aiway oncinintiodway isway ordinay\n",
            "Epoch:  15 | Train loss: 0.745 | Val loss: 0.975 | Gen: ehay airway onindintionway isway ordingway\n",
            "Epoch:  16 | Train loss: 0.709 | Val loss: 0.944 | Gen: ehay aiway onininingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.687 | Val loss: 0.952 | Gen: ehay airway oniningitionway isway oringway\n",
            "Epoch:  18 | Train loss: 0.664 | Val loss: 0.930 | Gen: ehay ariway onininingway isway orkonmay\n",
            "Epoch:  19 | Train loss: 0.663 | Val loss: 1.061 | Gen: ehay airway ondintingway isway orkyday\n",
            "Epoch:  20 | Train loss: 0.649 | Val loss: 0.951 | Gen: ehay airway ondinitiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.618 | Val loss: 0.912 | Gen: ehay ariway oniningintyway isway orkyshay\n",
            "Epoch:  22 | Train loss: 0.578 | Val loss: 0.868 | Gen: ehay airway onininidyway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.552 | Val loss: 0.867 | Gen: ehay ariway ondintioncay isway orkytray\n",
            "Epoch:  24 | Train loss: 0.528 | Val loss: 0.840 | Gen: ethay ariway oniningway-insway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.514 | Val loss: 0.853 | Gen: ehay-axtay ariway oniningityday isway orkingway\n",
            "Epoch:  26 | Train loss: 0.504 | Val loss: 0.856 | Gen: ehay airway oninintingway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.494 | Val loss: 0.848 | Gen: ehay-ay-ay ariway onininay-itedway isway orkyday\n",
            "Epoch:  28 | Train loss: 0.481 | Val loss: 0.813 | Gen: ehay ariway oniningingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.457 | Val loss: 0.825 | Gen: ethay ariway ondintionthay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.446 | Val loss: 0.786 | Gen: ehay airway oniningway-intway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.422 | Val loss: 0.774 | Gen: ethay ariway oniningingway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.409 | Val loss: 0.808 | Gen: ethay ariway onininieday isway orkingway\n",
            "Epoch:  33 | Train loss: 0.399 | Val loss: 0.767 | Gen: ehtay ariway oniningway-inteway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.387 | Val loss: 0.739 | Gen: ethay airway onininingway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.373 | Val loss: 0.764 | Gen: ethay ariway oniningingway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.368 | Val loss: 0.761 | Gen: ethay ariway oninidenay-ay-aycay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.354 | Val loss: 0.768 | Gen: ethay ariway oniningingway isway orkingway\n",
            "Epoch:  38 | Train loss: 0.354 | Val loss: 0.745 | Gen: ethay ariway oniningway-ingay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.346 | Val loss: 0.780 | Gen: ehay ariway oniningay-inbay-ay-a isway orkingway\n",
            "Epoch:  40 | Train loss: 0.339 | Val loss: 0.772 | Gen: ethay ariway oniningingway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.342 | Val loss: 0.740 | Gen: ethay ariway oniningway-intway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.334 | Val loss: 0.751 | Gen: ethay ariway oninidenay-ay-aclay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.317 | Val loss: 0.757 | Gen: ethay ariway oniningingway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.301 | Val loss: 0.757 | Gen: ethay ariway oniningway-intsway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.291 | Val loss: 0.759 | Gen: ethay ariway oniningray-inway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.284 | Val loss: 0.725 | Gen: ethay airway oniningingway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.274 | Val loss: 0.756 | Gen: ethay airway oniningingway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.268 | Val loss: 0.715 | Gen: ethay airway oniningingcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.264 | Val loss: 0.739 | Gen: ethay airway oniningbay-intway isway orkingway\n",
            "Obtained lowest validation loss of: 0.7154550799765648\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway oniningbay-intway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \r\n",
        "\r\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcc11e7-46ce-4b58-ce3a-2977c3810f55"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('almost', 'almostway')\n",
            "('evenings', 'eveningsway')\n",
            "('lethal', 'ethallay')\n",
            "('cuban', 'ubancay')\n",
            "('reasonable', 'easonableray')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.358 | Val loss: 2.014 | Gen: ay-ay ay-ay intentay-intay intay ontay-ay\n",
            "Epoch:   1 | Train loss: 1.841 | Val loss: 1.857 | Gen: eray ay-intay ontay-interay-intay ingay ingay-intay\n",
            "Epoch:   2 | Train loss: 1.641 | Val loss: 1.779 | Gen: eray atinway ontintay-intay-intay istay otintay-ingay\n",
            "Epoch:   3 | Train loss: 1.498 | Val loss: 1.643 | Gen: eray atinay ontintay-iotay-intay isway othingay-ay\n",
            "Epoch:   4 | Train loss: 1.386 | Val loss: 1.544 | Gen: etay atinway ountingray-ingsay isway ougherway-ay\n",
            "Epoch:   5 | Train loss: 1.281 | Val loss: 1.502 | Gen: ethay away ountingicontay-inway isway oughterway\n",
            "Epoch:   6 | Train loss: 1.175 | Val loss: 1.382 | Gen: etay away ontingicetay-ilway isway oway-ingheway\n",
            "Epoch:   7 | Train loss: 1.102 | Val loss: 1.342 | Gen: ethay away oninglichetay-ilway isway ougherway\n",
            "Epoch:   8 | Train loss: 1.040 | Val loss: 1.264 | Gen: ethay away ontingicallway isway oway-ingheway\n",
            "Epoch:   9 | Train loss: 0.968 | Val loss: 1.255 | Gen: ethay away oniningstay-itay isway oway-inghway\n",
            "Epoch:  10 | Train loss: 0.914 | Val loss: 1.232 | Gen: ethay ay oninicationgay isway ourigheway\n",
            "Epoch:  11 | Train loss: 0.866 | Val loss: 1.156 | Gen: ethay away ontiningstay isway outhinway\n",
            "Epoch:  12 | Train loss: 0.817 | Val loss: 1.106 | Gen: erhay away onintay-intampay isway ourfinay-ay\n",
            "Epoch:  13 | Train loss: 0.776 | Val loss: 1.135 | Gen: ethay away ollinchionay isway oinway-ingway\n",
            "Epoch:  14 | Train loss: 0.738 | Val loss: 1.071 | Gen: ethay away ontinitemaypay isway oitherway\n",
            "Epoch:  15 | Train loss: 0.695 | Val loss: 0.998 | Gen: ethay aidway onintay-intay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.660 | Val loss: 0.983 | Gen: ethay aidway onintiongray isway oyirefay\n",
            "Epoch:  17 | Train loss: 0.629 | Val loss: 0.980 | Gen: ethay aidway onintiongshay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.619 | Val loss: 0.993 | Gen: ethay aidway onintay-ontay isway orkintway\n",
            "Epoch:  19 | Train loss: 0.599 | Val loss: 0.948 | Gen: ethay airway onintiationcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.569 | Val loss: 0.926 | Gen: ethay airway onticinotercay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.554 | Val loss: 0.902 | Gen: ethay aidway onintay-othinamay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.546 | Val loss: 0.940 | Gen: ethay aidway oninticationray isway orkingway\n",
            "Epoch:  23 | Train loss: 0.551 | Val loss: 0.912 | Gen: ethay airway ontiniconay-aturay isway okertiblay\n",
            "Epoch:  24 | Train loss: 0.527 | Val loss: 0.900 | Gen: ethay airway ontinicationray isway orkingway\n",
            "Epoch:  25 | Train loss: 0.502 | Val loss: 0.871 | Gen: ethay airway onintiongray isway orkingway\n",
            "Epoch:  26 | Train loss: 0.480 | Val loss: 0.841 | Gen: ethay airway onintiationcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.464 | Val loss: 0.908 | Gen: ethay airway onintiongstay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.467 | Val loss: 0.843 | Gen: ethay airway onintay-onturalcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.447 | Val loss: 0.801 | Gen: ethay airway oninticay-onrxay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.426 | Val loss: 0.840 | Gen: ethay airway onintchionaray isway ourkengay\n",
            "Epoch:  31 | Train loss: 0.420 | Val loss: 0.850 | Gen: ethay airway onintcionatlay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.427 | Val loss: 0.856 | Gen: ethay airway onintionarchay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.416 | Val loss: 0.846 | Gen: ethay airway onticionray-oxay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.404 | Val loss: 0.827 | Gen: ethay airway onitinotay-ontay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.391 | Val loss: 0.791 | Gen: ethay airway onitiondicalay isway ouringway\n",
            "Epoch:  36 | Train loss: 0.377 | Val loss: 0.820 | Gen: erhay airway onintay-onicarlay isway ouringway\n",
            "Epoch:  37 | Train loss: 0.372 | Val loss: 0.823 | Gen: ethay airway onitingpronay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.379 | Val loss: 0.819 | Gen: ethay airway onindicaytorlay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.366 | Val loss: 0.806 | Gen: ethay airway onintpay-otinecay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.352 | Val loss: 0.777 | Gen: ethay airway onitingdingday isway ourkestay\n",
            "Epoch:  41 | Train loss: 0.340 | Val loss: 0.794 | Gen: ethay airway oncinatiorylay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.331 | Val loss: 0.760 | Gen: ethay airway onitinodingday isway ouringway\n",
            "Epoch:  43 | Train loss: 0.317 | Val loss: 0.762 | Gen: ethay airway onticuctedhay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.313 | Val loss: 0.798 | Gen: ethay airway oninticouray isway orkingway\n",
            "Epoch:  45 | Train loss: 0.316 | Val loss: 0.775 | Gen: ethay airway ontioncuriscay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.308 | Val loss: 0.767 | Gen: ethay airway onintidoricay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.307 | Val loss: 0.767 | Gen: ethay airway onciturinatcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.316 | Val loss: 0.836 | Gen: erhay airway onintedicatycay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.339 | Val loss: 0.920 | Gen: ethay airway ontianiondhay isway orkingway\n",
            "Obtained lowest validation loss of: 0.7600159848070874\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ontianiondhay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "97cd7231-4430-4c8a-a555-01edf1183edf"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e21eb71-ac6c-4527-db14-787cdcb4f699"
      },
      "source": [
        "best_encoder = rnn_encode_l # Replace with rnn_losses_s or rnn_losses l\n",
        "best_decoder = rnn_decoder_l # etc.\n",
        "best_args = rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ontianiondhay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
        "            \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c355240-972a-469e-8aba-02d87fe02c74"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "rnn_attn_args = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':50, \r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':0.005,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'early_stopping_patience':10,\r\n",
        "              'batch_size':64, \r\n",
        "              'hidden_size':64, \r\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\r\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\r\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\r\n",
        "}\r\n",
        "rnn_attn_args.update(args_dict)\r\n",
        "\r\n",
        "print_opts(rnn_attn_args)\r\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('nerves', 'ervesnay')\n",
            "('concerto', 'oncertocay')\n",
            "('went', 'entway')\n",
            "('hated', 'atedhay')\n",
            "('evenings', 'eveningsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.096 | Val loss: 1.801 | Gen: ay-ingay ay-ingay ingsay ingsay oray-oray-ongay\n",
            "Epoch:   1 | Train loss: 1.595 | Val loss: 1.557 | Gen: otedway aray-ingway ingsay-ondgay isay oureway-ongsay\n",
            "Epoch:   2 | Train loss: 1.294 | Val loss: 1.360 | Gen: ehay aritway ingstiongsray isay-ingway odgay-otingway\n",
            "Epoch:   3 | Train loss: 1.074 | Val loss: 1.257 | Gen: ehay ay-arway ondingway-ingway iway oudgay-ay\n",
            "Epoch:   4 | Train loss: 0.893 | Val loss: 1.069 | Gen: ehthay ay-ingray ondingsay-ingcay isay onghay-ybay\n",
            "Epoch:   5 | Train loss: 0.730 | Val loss: 0.989 | Gen: ehay ay-ationway ondingtay-iongay isway ourway-ingway\n",
            "Epoch:   6 | Train loss: 0.620 | Val loss: 0.879 | Gen: ehtay ay-ingway ondicitingway isway orkway-ingway\n",
            "Epoch:   7 | Train loss: 0.538 | Val loss: 0.782 | Gen: ehay ay-iray ondingicationway iway orkingway\n",
            "Epoch:   8 | Train loss: 0.423 | Val loss: 0.663 | Gen: ehay airway ondicitingway isway oringway\n",
            "Epoch:   9 | Train loss: 0.333 | Val loss: 0.649 | Gen: ehay airway onditingicay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.283 | Val loss: 0.694 | Gen: ethay ay-irway ondicitingway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.260 | Val loss: 0.634 | Gen: ehay airway onditingingcay isway oringway\n",
            "Epoch:  12 | Train loss: 0.216 | Val loss: 0.579 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.183 | Val loss: 0.467 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.150 | Val loss: 0.472 | Gen: ehtay airway onditingcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.114 | Val loss: 0.494 | Gen: ehay ariway onditingcay-awy isway orkingway\n",
            "Epoch:  16 | Train loss: 0.103 | Val loss: 0.356 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.074 | Val loss: 0.358 | Gen: ehtay airway onditingnay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.071 | Val loss: 0.462 | Gen: ehtay airway onditingingway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.084 | Val loss: 0.401 | Gen: etay airway onditiongscay isway orkikngway\n",
            "Epoch:  20 | Train loss: 0.073 | Val loss: 0.296 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.044 | Val loss: 0.291 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.030 | Val loss: 0.295 | Gen: ehtay airway onditingingcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.027 | Val loss: 0.244 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.023 | Val loss: 0.340 | Gen: ehthay airway onditiongnay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.035 | Val loss: 0.346 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.030 | Val loss: 0.260 | Gen: ethay airway onditionicancay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.016 | Val loss: 0.239 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.010 | Val loss: 0.239 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.007 | Val loss: 0.237 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.006 | Val loss: 0.227 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.005 | Val loss: 0.228 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.004 | Val loss: 0.225 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.004 | Val loss: 0.224 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.003 | Val loss: 0.223 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.003 | Val loss: 0.223 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.003 | Val loss: 0.222 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.003 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.002 | Val loss: 0.222 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.002 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.002 | Val loss: 0.222 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.002 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.002 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.002 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.002 | Val loss: 0.223 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.002 | Val loss: 0.224 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.001 | Val loss: 0.224 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.001 | Val loss: 0.224 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.001 | Val loss: 0.225 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.22218693545959795\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca5bed3-553b-4677-cdd8-d6607b59a17e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, _i, _j = queries.size()\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
        "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
        "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(1,2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(1,2),v)\n",
        "        return context, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, _i, _j = queries.size()\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
        "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
        "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(1,2)) * self.scaling_factor\n",
        "        mask = self.neg_inf * torch.tril(torch.ones_like(unnormalized_attention), diagonal=-1)\n",
        "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
        "        context = torch.bmm(attention_weights.transpose(1,2),v)\n",
        "        return context, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f182388c-24c5-4c4e-f3e3-8e76eb5faf77"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans32_args_s = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_small',\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':100, \r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 100,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 64,\r\n",
        "              'hidden_size': 32,\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans32_args_s.update(args_dict)\r\n",
        "print_opts(trans32_args_s)\r\n",
        "\r\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('nerves', 'ervesnay')\n",
            "('concerto', 'oncertocay')\n",
            "('went', 'entway')\n",
            "('hated', 'atedhay')\n",
            "('evenings', 'eveningsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.853 | Val loss: 2.297 | Gen: eay ay onay ay onay \n",
            "Epoch:   1 | Train loss: 2.141 | Val loss: 2.021 | Gen: eayhay ay inay-inay isaysay oonay-ay\n",
            "Epoch:   2 | Train loss: 1.870 | Val loss: 1.821 | Gen: ehayhay aiay iiiiiiingay isaysay ooonay-ay\n",
            "Epoch:   3 | Train loss: 1.692 | Val loss: 1.687 | Gen: ehayhay aiay iiiiitingay isaysay oooooooonay\n",
            "Epoch:   4 | Train loss: 1.566 | Val loss: 1.684 | Gen: ehehay aiway oiiiiiingay isaysay ooongay\n",
            "Epoch:   5 | Train loss: 1.503 | Val loss: 1.548 | Gen: ehay aiway ongiouioungay isay oray\n",
            "Epoch:   6 | Train loss: 1.421 | Val loss: 1.507 | Gen: ehay ay ongingay isay ongway\n",
            "Epoch:   7 | Train loss: 1.339 | Val loss: 1.494 | Gen: ehthay arirway ongingingay isay ongrway\n",
            "Epoch:   8 | Train loss: 1.268 | Val loss: 1.469 | Gen: ehay away ongingay isay ongway\n",
            "Epoch:   9 | Train loss: 1.200 | Val loss: 1.420 | Gen: ehay ay ongingay isay ongway\n",
            "Epoch:  10 | Train loss: 1.158 | Val loss: 1.339 | Gen: ehay arway ondingay isay ongway\n",
            "Epoch:  11 | Train loss: 1.110 | Val loss: 1.349 | Gen: ehay arway ondingay isay ongway\n",
            "Epoch:  12 | Train loss: 1.086 | Val loss: 1.322 | Gen: ehay away ondingay isay orway\n",
            "Epoch:  13 | Train loss: 1.035 | Val loss: 1.230 | Gen: ehay ay ondingay isay orway\n",
            "Epoch:  14 | Train loss: 0.989 | Val loss: 1.242 | Gen: ehay arway ondigay isay orway\n",
            "Epoch:  15 | Train loss: 0.955 | Val loss: 1.152 | Gen: ehay away oningongay isay orway\n",
            "Epoch:  16 | Train loss: 0.902 | Val loss: 1.100 | Gen: ehay arway oningongay isay orway\n",
            "Epoch:  17 | Train loss: 0.848 | Val loss: 1.124 | Gen: ehay arway oningongay isay orway\n",
            "Epoch:  18 | Train loss: 0.835 | Val loss: 1.140 | Gen: ehay arway onciongayongay isay orway\n",
            "Epoch:  19 | Train loss: 0.809 | Val loss: 1.035 | Gen: ehay arway oningongay isay orway\n",
            "Epoch:  20 | Train loss: 0.762 | Val loss: 1.128 | Gen: ehay away ondingay isay orway\n",
            "Epoch:  21 | Train loss: 0.760 | Val loss: 1.008 | Gen: ehay arway ondiongay isay orway\n",
            "Epoch:  22 | Train loss: 0.712 | Val loss: 1.065 | Gen: ehay ay onnioniongway isay orway\n",
            "Epoch:  23 | Train loss: 0.681 | Val loss: 1.025 | Gen: ehay away ondiongay isay orway\n",
            "Epoch:  24 | Train loss: 0.662 | Val loss: 0.986 | Gen: ehay ay onnioniongongay isay orway\n",
            "Epoch:  25 | Train loss: 0.639 | Val loss: 0.974 | Gen: ehay ay onndioniongway isay orway\n",
            "Epoch:  26 | Train loss: 0.616 | Val loss: 0.950 | Gen: ehay arway oningoniongway isay orwway\n",
            "Epoch:  27 | Train loss: 0.632 | Val loss: 1.039 | Gen: ehay ay onciontiongongay isay orway\n",
            "Epoch:  28 | Train loss: 0.614 | Val loss: 0.982 | Gen: ehay ayway onnioniongiongay isay orwway\n",
            "Epoch:  29 | Train loss: 0.577 | Val loss: 1.021 | Gen: ehay arway onlingay isay orwway\n",
            "Epoch:  30 | Train loss: 0.567 | Val loss: 0.977 | Gen: ehay arway onlingay isay orway\n",
            "Epoch:  31 | Train loss: 0.544 | Val loss: 0.876 | Gen: ehay arway onnionioniongwy isay orway\n",
            "Epoch:  32 | Train loss: 0.496 | Val loss: 0.862 | Gen: ehay arway onnionioniongwwy isay orwwway\n",
            "Epoch:  33 | Train loss: 0.469 | Val loss: 0.865 | Gen: ehay arway onnionioniongay isay orway\n",
            "Epoch:  34 | Train loss: 0.456 | Val loss: 0.865 | Gen: ehay arway onnionioniongway isay orwwway\n",
            "Epoch:  35 | Train loss: 0.448 | Val loss: 0.892 | Gen: ehay ay ondionioniongwy isay orway\n",
            "Epoch:  36 | Train loss: 0.442 | Val loss: 0.821 | Gen: ehay ayway onnionioniongwy isay orwwway\n",
            "Epoch:  37 | Train loss: 0.415 | Val loss: 0.862 | Gen: ehay arway onndioniongiongay isay orway\n",
            "Epoch:  38 | Train loss: 0.405 | Val loss: 0.864 | Gen: ehay ay ondigtiongway isay orwwway\n",
            "Epoch:  39 | Train loss: 0.410 | Val loss: 0.858 | Gen: ehay arway ondigtiongiongway isay orway\n",
            "Epoch:  40 | Train loss: 0.408 | Val loss: 0.890 | Gen: ethay arway ondintiongiongwy isay oringway\n",
            "Epoch:  41 | Train loss: 0.390 | Val loss: 0.803 | Gen: ehay away onnionioniongwy isay orway\n",
            "Epoch:  42 | Train loss: 0.362 | Val loss: 0.789 | Gen: ehay arway ondingioniongway isay orwingway\n",
            "Epoch:  43 | Train loss: 0.344 | Val loss: 0.783 | Gen: ehay arway ondigtioniongway isay orwway\n",
            "Epoch:  44 | Train loss: 0.334 | Val loss: 0.796 | Gen: ehay arway ondingioniongwy isay orwingway\n",
            "Epoch:  45 | Train loss: 0.326 | Val loss: 0.774 | Gen: ehay arway onditioningiongwwwwa isay oringway\n",
            "Epoch:  46 | Train loss: 0.319 | Val loss: 0.813 | Gen: ehay arway ondigtingiongway isay orway\n",
            "Epoch:  47 | Train loss: 0.316 | Val loss: 0.756 | Gen: ehay arway ondigtingiongwray isay oringway\n",
            "Epoch:  48 | Train loss: 0.298 | Val loss: 0.775 | Gen: ehay arway onditioningiongwy isay orwingway\n",
            "Epoch:  49 | Train loss: 0.301 | Val loss: 0.756 | Gen: ehay arway ondionioniongwwy isay oringway\n",
            "Epoch:  50 | Train loss: 0.285 | Val loss: 0.760 | Gen: ehay arway ondingtiongiongwwy isay orway\n",
            "Epoch:  51 | Train loss: 0.275 | Val loss: 0.749 | Gen: ehay arway onditioningingwwray isay oringway\n",
            "Epoch:  52 | Train loss: 0.265 | Val loss: 0.752 | Gen: ehay arway ondintioniongcay isay orkingway\n",
            "Epoch:  53 | Train loss: 0.262 | Val loss: 0.757 | Gen: ehay arway onditioningiongwy isay oringway\n",
            "Epoch:  54 | Train loss: 0.250 | Val loss: 0.746 | Gen: ehay arway onditioningiongwray isay orkingway\n",
            "Epoch:  55 | Train loss: 0.240 | Val loss: 0.766 | Gen: ehay arway onditioningiongwray isay orkingway\n",
            "Epoch:  56 | Train loss: 0.254 | Val loss: 0.798 | Gen: ehay away ondionioningway isay oorwway\n",
            "Epoch:  57 | Train loss: 0.284 | Val loss: 0.901 | Gen: ethay arway ondintioningcay isay oringwway\n",
            "Epoch:  58 | Train loss: 0.283 | Val loss: 0.753 | Gen: ethay araway onditioningcay isay oringway\n",
            "Epoch:  59 | Train loss: 0.264 | Val loss: 0.825 | Gen: ehay away onndioniongingwray isay orsway\n",
            "Epoch:  60 | Train loss: 0.264 | Val loss: 0.747 | Gen: ehay arway onditioningway isay orsongway\n",
            "Epoch:  61 | Train loss: 0.247 | Val loss: 0.779 | Gen: ethay araway onditioningcay isay orsongwway\n",
            "Epoch:  62 | Train loss: 0.232 | Val loss: 0.758 | Gen: ethay arway onditioningcay isay orkingway\n",
            "Epoch:  63 | Train loss: 0.222 | Val loss: 0.749 | Gen: ethay arway onditioningcay isay orkingway\n",
            "Epoch:  64 | Train loss: 0.208 | Val loss: 0.725 | Gen: ethay arway onditioningway isay orsway\n",
            "Epoch:  65 | Train loss: 0.195 | Val loss: 0.729 | Gen: ethay arway onditioningway isay orkingway\n",
            "Epoch:  66 | Train loss: 0.187 | Val loss: 0.729 | Gen: ethay arway onditioningway isay orkingway\n",
            "Epoch:  67 | Train loss: 0.180 | Val loss: 0.737 | Gen: ehay arway onditioningway isay orkingway\n",
            "Epoch:  68 | Train loss: 0.175 | Val loss: 0.739 | Gen: ehay away onditioningway isay orkingway\n",
            "Epoch:  69 | Train loss: 0.170 | Val loss: 0.751 | Gen: ehay away onditioningway isay orkingway\n",
            "Epoch:  70 | Train loss: 0.164 | Val loss: 0.744 | Gen: ehay away onditioningway isay orkingway\n",
            "Epoch:  71 | Train loss: 0.160 | Val loss: 0.765 | Gen: ehay away onditioningway isay orkingway\n",
            "Epoch:  72 | Train loss: 0.157 | Val loss: 0.748 | Gen: ethay away onditioningway isay orkingway\n",
            "Epoch:  73 | Train loss: 0.158 | Val loss: 0.853 | Gen: ehay arway onditioningcay isay orkingwawwwwwwwwwwww\n",
            "Epoch:  74 | Train loss: 0.184 | Val loss: 0.777 | Gen: ethay away onditioningwray isay orkingway\n",
            "Epoch:  75 | Train loss: 0.170 | Val loss: 0.752 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  76 | Train loss: 0.157 | Val loss: 0.746 | Gen: ethay away onditioningway isay orkingway\n",
            "Epoch:  77 | Train loss: 0.145 | Val loss: 0.748 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  78 | Train loss: 0.140 | Val loss: 0.770 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  79 | Train loss: 0.137 | Val loss: 0.819 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  80 | Train loss: 0.137 | Val loss: 0.813 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  81 | Train loss: 0.134 | Val loss: 0.753 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  82 | Train loss: 0.127 | Val loss: 0.751 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  83 | Train loss: 0.123 | Val loss: 0.812 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  84 | Train loss: 0.122 | Val loss: 0.793 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  85 | Train loss: 0.120 | Val loss: 0.806 | Gen: ehay away onditioningway isay orkingway\n",
            "Epoch:  86 | Train loss: 0.137 | Val loss: 0.915 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  87 | Train loss: 0.202 | Val loss: 0.848 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  88 | Train loss: 0.165 | Val loss: 0.734 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  89 | Train loss: 0.133 | Val loss: 0.824 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  90 | Train loss: 0.124 | Val loss: 0.782 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  91 | Train loss: 0.112 | Val loss: 0.753 | Gen: ehay away onditioncay iday orkingway\n",
            "Epoch:  92 | Train loss: 0.104 | Val loss: 0.745 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  93 | Train loss: 0.098 | Val loss: 0.728 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  94 | Train loss: 0.093 | Val loss: 0.730 | Gen: ehay away onditioncay isay orkingway\n",
            "Epoch:  95 | Train loss: 0.090 | Val loss: 0.721 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  96 | Train loss: 0.087 | Val loss: 0.731 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  97 | Train loss: 0.083 | Val loss: 0.731 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  98 | Train loss: 0.081 | Val loss: 0.736 | Gen: ehay away onditioningcay isay orkingway\n",
            "Epoch:  99 | Train loss: 0.078 | Val loss: 0.739 | Gen: ehay away onditioningcay isay orkingway\n",
            "Obtained lowest validation loss of: 0.7206274665032442\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay away onditioningcay isay orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d352917-0c61-4c23-dbaa-32c916976969"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay away onditioningcay isay orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbec709c-5b45-4ca5-f576-c58f2cc77f3f"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans32_args_l = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':100,\r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 10,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 512,\r\n",
        "              'hidden_size': 32,\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans32_args_l.update(args_dict)\r\n",
        "print_opts(trans32_args_l)\r\n",
        "\r\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('almost', 'almostway')\n",
            "('evenings', 'eveningsway')\n",
            "('lethal', 'ethallay')\n",
            "('cuban', 'ubancay')\n",
            "('reasonable', 'easonableray')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.790 | Val loss: 2.365 | Gen: eay-ey i-ay-ay oonnnway inay oonwnway\n",
            "Epoch:   1 | Train loss: 2.128 | Val loss: 2.118 | Gen: etay-ay-ay-ay i-ay-ay onnnrngday-intay inay oonrnway-ay\n",
            "Epoch:   2 | Train loss: 1.899 | Val loss: 1.981 | Gen: etay-ay aray-ay-ay-ay oninnay-ay-ay inay onway-ay\n",
            "Epoch:   3 | Train loss: 1.739 | Val loss: 1.897 | Gen: eay-eay aray-ay-ay-ay oninay-inay-ay isay-ay ongay-ay\n",
            "Epoch:   4 | Train loss: 1.623 | Val loss: 1.827 | Gen: eay-eday aray-ay oningay-ionay isssssay ogay-inay\n",
            "Epoch:   5 | Train loss: 1.532 | Val loss: 1.734 | Gen: eatay-eay aray-iay oingay-intionay isssssssay ogay-ingay\n",
            "Epoch:   6 | Train loss: 1.446 | Val loss: 1.660 | Gen: eatay aray-igay oingy-inday-icay isssssssay ogay-ingay\n",
            "Epoch:   7 | Train loss: 1.384 | Val loss: 1.662 | Gen: eatay aray ondindtingay issssssay ogay-ingay\n",
            "Epoch:   8 | Train loss: 1.308 | Val loss: 1.542 | Gen: eatay arway onindigay isway ogy-ingay\n",
            "Epoch:   9 | Train loss: 1.253 | Val loss: 1.633 | Gen: eatay aray oondingay isway oray-oray\n",
            "Epoch:  10 | Train loss: 1.190 | Val loss: 1.620 | Gen: ehehtay arway-idway ongintingngngngay isway ogray-ingay\n",
            "Epoch:  11 | Train loss: 1.140 | Val loss: 1.511 | Gen: ehtay aray ondindintinway isay oray-oray\n",
            "Epoch:  12 | Train loss: 1.093 | Val loss: 1.436 | Gen: ehtheay arway-i ongintingngngngay isway ogray-ingay\n",
            "Epoch:  13 | Train loss: 1.051 | Val loss: 1.423 | Gen: ehtay aray ondgintintinay isay oray-ingay\n",
            "Epoch:  14 | Train loss: 1.012 | Val loss: 1.380 | Gen: ehthenay ariray onginticay isway orgay-ingay\n",
            "Epoch:  15 | Train loss: 0.968 | Val loss: 1.332 | Gen: ehtay aray ondginticay isay orgygay\n",
            "Epoch:  16 | Train loss: 0.934 | Val loss: 1.295 | Gen: ehtay araway onginticay isay orgay-ingay\n",
            "Epoch:  17 | Train loss: 0.898 | Val loss: 1.277 | Gen: ehtay araway ondgicicay isay orgygay\n",
            "Epoch:  18 | Train loss: 0.861 | Val loss: 1.252 | Gen: ehtay iaray onginicicay isay orgay-ingay\n",
            "Epoch:  19 | Train loss: 0.832 | Val loss: 1.235 | Gen: ehtay araway ondgicicay isay orgy-ingay\n",
            "Epoch:  20 | Train loss: 0.805 | Val loss: 1.202 | Gen: ehtay iaray onginicicay isay orgay-ingay\n",
            "Epoch:  21 | Train loss: 0.776 | Val loss: 1.174 | Gen: ehtay aray ondgicicay isay orgay-ingay\n",
            "Epoch:  22 | Train loss: 0.752 | Val loss: 1.197 | Gen: ehtay iaray onginiiiontingay isay orway-igay\n",
            "Epoch:  23 | Train loss: 0.733 | Val loss: 1.163 | Gen: ehtay aray ondidicay isay orway-igay\n",
            "Epoch:  24 | Train loss: 0.711 | Val loss: 1.165 | Gen: ehtay ariway ondigiionintingay isay orway-igay\n",
            "Epoch:  25 | Train loss: 0.691 | Val loss: 1.178 | Gen: ehtay aray ondidioningay isay orway-igay\n",
            "Epoch:  26 | Train loss: 0.679 | Val loss: 1.149 | Gen: ehtay araway ondidicay isay owwwway\n",
            "Epoch:  27 | Train loss: 0.665 | Val loss: 1.157 | Gen: etay aray ondidicay isay orway-ingway\n",
            "Epoch:  28 | Train loss: 0.649 | Val loss: 1.137 | Gen: ehtay aray ondidicay isay orway-ingway\n",
            "Epoch:  29 | Train loss: 0.624 | Val loss: 1.118 | Gen: ehtay arway ondidiciongay isay orway-igway\n",
            "Epoch:  30 | Train loss: 0.600 | Val loss: 1.003 | Gen: ehthay ariway ondidiogay isay orlway-igway\n",
            "Epoch:  31 | Train loss: 0.582 | Val loss: 1.068 | Gen: ehtay aray ondicionicay isay orlway-igay\n",
            "Epoch:  32 | Train loss: 0.576 | Val loss: 0.993 | Gen: ehthay arway ondidicingay isay owringway\n",
            "Epoch:  33 | Train loss: 0.553 | Val loss: 1.066 | Gen: ehtay aray ondidioningay isay orlway-ingway\n",
            "Epoch:  34 | Train loss: 0.547 | Val loss: 0.918 | Gen: ehthay arway ondidicay isay owringway\n",
            "Epoch:  35 | Train loss: 0.519 | Val loss: 0.999 | Gen: ehtay arway ondidiongay isay owringway\n",
            "Epoch:  36 | Train loss: 0.529 | Val loss: 0.907 | Gen: ehthay arway ondidiongay isay orlway-ingway\n",
            "Epoch:  37 | Train loss: 0.510 | Val loss: 0.981 | Gen: ehtay arway ondicioninay isay owringway\n",
            "Epoch:  38 | Train loss: 0.493 | Val loss: 0.938 | Gen: ehtay arway ondidiongay isay owringway\n",
            "Epoch:  39 | Train loss: 0.476 | Val loss: 0.954 | Gen: ehtay arway ondicioningay isay owringway\n",
            "Epoch:  40 | Train loss: 0.460 | Val loss: 0.982 | Gen: ehtay arway ondicioningay isay owringway\n",
            "Epoch:  41 | Train loss: 0.448 | Val loss: 0.917 | Gen: ehtay arway ondinioningnay isay orlway-oway\n",
            "Epoch:  42 | Train loss: 0.430 | Val loss: 0.874 | Gen: ehtay arway ondicioningay isay orlway-oway\n",
            "Epoch:  43 | Train loss: 0.419 | Val loss: 0.850 | Gen: ehtay arway ondicioningay isay orlway-oway\n",
            "Epoch:  44 | Train loss: 0.408 | Val loss: 0.816 | Gen: ehtay arway ondicioningnay isway orlway-oway\n",
            "Epoch:  45 | Train loss: 0.399 | Val loss: 0.796 | Gen: ehtay arway ondicioningnay isway orlway-oway\n",
            "Epoch:  46 | Train loss: 0.390 | Val loss: 0.776 | Gen: ehtay arway ondicioningnay isway orlway-ingway\n",
            "Epoch:  47 | Train loss: 0.382 | Val loss: 0.772 | Gen: ehtay arway ondicioningnay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.374 | Val loss: 0.757 | Gen: ehtay arway ondicioningway isway orlway-ingway\n",
            "Epoch:  49 | Train loss: 0.379 | Val loss: 0.896 | Gen: ehtay arway ondiditingnay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.410 | Val loss: 0.911 | Gen: ehtay arway ondintiongnay isway orlway-oway\n",
            "Epoch:  51 | Train loss: 0.400 | Val loss: 0.830 | Gen: ehtay arway ondicioningay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.374 | Val loss: 0.778 | Gen: ehtay arway ondidingnay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.348 | Val loss: 0.730 | Gen: ehtay arway ondicioningnay isway orlway-ingway\n",
            "Epoch:  54 | Train loss: 0.331 | Val loss: 0.703 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.320 | Val loss: 0.694 | Gen: ehtay arway ondicioningnay isway orlway-ingway\n",
            "Epoch:  56 | Train loss: 0.313 | Val loss: 0.687 | Gen: ehtay arway ondiciongnay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.306 | Val loss: 0.685 | Gen: ehthay arway ondiciongnay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.300 | Val loss: 0.687 | Gen: ehthay arway ondiciongnay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.294 | Val loss: 0.684 | Gen: ehthay arway ondiciongnay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.288 | Val loss: 0.686 | Gen: ehthay arway ondiciongnay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.283 | Val loss: 0.683 | Gen: ehthay arway ondiciongnay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.277 | Val loss: 0.684 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.272 | Val loss: 0.679 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.267 | Val loss: 0.685 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.262 | Val loss: 0.685 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.257 | Val loss: 0.691 | Gen: ehthay arway ondicioningnay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.252 | Val loss: 0.686 | Gen: ehthay arway ondiciongnicay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.248 | Val loss: 0.682 | Gen: ehthay arway ondiciongnicay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.243 | Val loss: 0.677 | Gen: ehthay ariway ondicioningnay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.239 | Val loss: 0.667 | Gen: ehthay ariway ondicioningnay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.233 | Val loss: 0.675 | Gen: ehthay ariway ondiciongnay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.230 | Val loss: 0.687 | Gen: ethay ariway ondiciongnay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.228 | Val loss: 0.670 | Gen: ethay ariway ondiciongnay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.221 | Val loss: 0.685 | Gen: ethay ariway ondiciongnay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.220 | Val loss: 0.691 | Gen: ehthay airway ondicioningnay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.221 | Val loss: 0.701 | Gen: ethay airway ondicioningnay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.210 | Val loss: 0.675 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.204 | Val loss: 0.656 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.199 | Val loss: 0.673 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.196 | Val loss: 0.689 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.191 | Val loss: 0.674 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.187 | Val loss: 0.693 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.184 | Val loss: 0.687 | Gen: ethay airway ondicioningway isway orkingway\n",
            "Epoch:  84 | Train loss: 0.180 | Val loss: 0.684 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.176 | Val loss: 0.701 | Gen: ethay airway ondicioningway isway orkingway\n",
            "Epoch:  86 | Train loss: 0.173 | Val loss: 0.685 | Gen: ethay airway ondiciongnay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.169 | Val loss: 0.682 | Gen: ethay airway ondicioningway isway orkingway\n",
            "Epoch:  88 | Train loss: 0.166 | Val loss: 0.714 | Gen: ethay airway ondicioningway isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.6557903501148127\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondiciongnay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983b4f79-b1ae-46ea-e317-16de8eceae46"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('nerves', 'ervesnay')\n",
            "('concerto', 'oncertocay')\n",
            "('went', 'entway')\n",
            "('hated', 'atedhay')\n",
            "('evenings', 'eveningsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.606 | Val loss: 2.021 | Gen: edy iway itingtititititngtiti ay ooongway\n",
            "Epoch:   1 | Train loss: 1.729 | Val loss: 1.648 | Gen: ethethay iway ontintingay iway oray-ingay\n",
            "Epoch:   2 | Train loss: 1.431 | Val loss: 1.458 | Gen: ehay iway ingingingay iway oway-iway\n",
            "Epoch:   3 | Train loss: 1.225 | Val loss: 1.388 | Gen: ehhhhay irway ondingay-iondiongay isway orway-iorway\n",
            "Epoch:   4 | Train loss: 1.074 | Val loss: 1.355 | Gen: ehay iway ondinay-iondionay-io iway orway-iongway\n",
            "Epoch:   5 | Train loss: 0.981 | Val loss: 1.305 | Gen: ehay irway indidingdidindingway isisway oringway\n",
            "Epoch:   6 | Train loss: 0.903 | Val loss: 1.239 | Gen: ehay arway onday-inay isway oway-inay\n",
            "Epoch:   7 | Train loss: 0.799 | Val loss: 1.154 | Gen: ehay arway ondidingngay isway orway-ingway\n",
            "Epoch:   8 | Train loss: 0.705 | Val loss: 0.937 | Gen: ehay arway ondidigtingday isway orkway-inggway\n",
            "Epoch:   9 | Train loss: 0.594 | Val loss: 0.958 | Gen: ehay arway ondidicigtiongway isway orkway-agay\n",
            "Epoch:  10 | Train loss: 0.527 | Val loss: 0.812 | Gen: ehay arway ondiditingday issway orkwagway-agray\n",
            "Epoch:  11 | Train loss: 0.465 | Val loss: 0.848 | Gen: ethay arway onditingcay issway onkingray\n",
            "Epoch:  12 | Train loss: 0.423 | Val loss: 0.868 | Gen: ehtay arway oontingnay issway ookway-ay\n",
            "Epoch:  13 | Train loss: 0.361 | Val loss: 0.764 | Gen: ehtay ariway onditingnay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.304 | Val loss: 0.751 | Gen: ehtay arway onditonngncingcay issway orkingway-arway\n",
            "Epoch:  15 | Train loss: 0.276 | Val loss: 0.730 | Gen: ehtay ariway onditingnay issway okway-ingway\n",
            "Epoch:  16 | Train loss: 0.337 | Val loss: 1.046 | Gen: ethay arway ondiditingcay isway owingway-agway\n",
            "Epoch:  17 | Train loss: 0.421 | Val loss: 0.848 | Gen: ehay irrway ondintingcay issway orkingway\n",
            "Epoch:  18 | Train loss: 0.303 | Val loss: 0.672 | Gen: ehtethay aiway ondidintingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.235 | Val loss: 0.605 | Gen: ehtay ariway ondiditongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.179 | Val loss: 0.655 | Gen: ehtay ariway onditiongngcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.152 | Val loss: 0.595 | Gen: ehtay airway onditiongcionay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.133 | Val loss: 0.618 | Gen: ehtay airway onditiongcingcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.126 | Val loss: 0.626 | Gen: ehtay airway onditiongncay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.118 | Val loss: 0.742 | Gen: ehtay irway onditiongcingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.135 | Val loss: 0.653 | Gen: ehtay aiway onditiongnay isway okingway\n",
            "Epoch:  26 | Train loss: 0.102 | Val loss: 0.570 | Gen: ehtay airway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.090 | Val loss: 0.693 | Gen: ehtay airway onditiongway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.098 | Val loss: 0.698 | Gen: ehtay aiway ondintingncay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.108 | Val loss: 0.701 | Gen: ehtay aiway onditiongcingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.093 | Val loss: 0.693 | Gen: ehtay airway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.141 | Val loss: 1.018 | Gen: ehtay ariway onditioonijay-icionc isway orkingway-agrkikikik\n",
            "Epoch:  32 | Train loss: 0.232 | Val loss: 0.739 | Gen: thay iarway onditioniongway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.294 | Val loss: 1.527 | Gen: hethay arway ondiditingngscingnca ay orkingway\n",
            "Epoch:  34 | Train loss: 0.391 | Val loss: 1.171 | Gen: hehthay iarfay onndincingncay isfay orkingway-agway\n",
            "Epoch:  35 | Train loss: 0.210 | Val loss: 0.593 | Gen: ehtay arway ondintiongcay isway orkingway-agway\n",
            "Epoch:  36 | Train loss: 0.121 | Val loss: 0.580 | Gen: ehthay ariway ondidintingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.085 | Val loss: 0.539 | Gen: ehtay ariway onditingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.064 | Val loss: 0.506 | Gen: ehtay ariway ondintiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.051 | Val loss: 0.524 | Gen: ethay ariway ondintiongcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.043 | Val loss: 0.505 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.037 | Val loss: 0.523 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.033 | Val loss: 0.503 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.030 | Val loss: 0.521 | Gen: ehtay airway ondintiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.029 | Val loss: 0.508 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.024 | Val loss: 0.503 | Gen: ethay airway ondintingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.022 | Val loss: 0.520 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.024 | Val loss: 0.540 | Gen: ehtay airway ondintingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.021 | Val loss: 0.528 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.021 | Val loss: 0.521 | Gen: ehtay airway ondintiongcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.5028246365105494\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehtay airway ondintiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756ebc0e-258f-4474-c6ab-8f3c8aaf85b5"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\r\n",
        "\r\n",
        "trans64_args_l = AttrDict()\r\n",
        "args_dict = {\r\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\r\n",
        "              'cuda':True, \r\n",
        "              'nepochs':50,\r\n",
        "              'checkpoint_dir':\"checkpoints\", \r\n",
        "              'learning_rate':5e-4,\r\n",
        "              'early_stopping_patience': 20,\r\n",
        "              'lr_decay':0.99,\r\n",
        "              'batch_size': 512, \r\n",
        "              'hidden_size': 64, # Increased model size\r\n",
        "              'encoder_type': 'transformer',\r\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\r\n",
        "              'num_transformer_layers': 3,\r\n",
        "}\r\n",
        "trans64_args_l.update(args_dict)\r\n",
        "print_opts(trans64_args_l)\r\n",
        "\r\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\r\n",
        "\r\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('almost', 'almostway')\n",
            "('evenings', 'eveningsway')\n",
            "('lethal', 'ethallay')\n",
            "('cuban', 'ubancay')\n",
            "('reasonable', 'easonableray')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.442 | Val loss: 1.963 | Gen: erey iway innnnnnnnnnnnnnnnnnn issssssssway ingnnnnnnny\n",
            "Epoch:   1 | Train loss: 1.686 | Val loss: 1.674 | Gen: ehey iray ongay-ingay issay ogay\n",
            "Epoch:   2 | Train loss: 1.415 | Val loss: 1.477 | Gen: ehhhhhhhhay ariray ongay-ongay-ingay isay orgay-ingray\n",
            "Epoch:   3 | Train loss: 1.214 | Val loss: 1.639 | Gen: ethehhhhety ariway ontingongay-ongay isay orgay-ingray\n",
            "Epoch:   4 | Train loss: 1.073 | Val loss: 1.401 | Gen: ehhay ariway ongay-ingay-ingay isway orgay-ingray\n",
            "Epoch:   5 | Train loss: 0.944 | Val loss: 1.294 | Gen: ehthehay arrway ontingntingngay isway orgay-ingngray\n",
            "Epoch:   6 | Train loss: 0.917 | Val loss: 1.212 | Gen: eththay ariway onidgingdgngongay iway ogaginggggray\n",
            "Epoch:   7 | Train loss: 0.790 | Val loss: 1.168 | Gen: eththay arrway onidingtingay isway orkinggray\n",
            "Epoch:   8 | Train loss: 0.704 | Val loss: 1.034 | Gen: ethay ariway oniditingingay isway okigigngway\n",
            "Epoch:   9 | Train loss: 0.615 | Val loss: 1.169 | Gen: etay ariway onidingay isway orkingray\n",
            "Epoch:  10 | Train loss: 0.544 | Val loss: 1.025 | Gen: etay ariway ondintingnay isway okingnay\n",
            "Epoch:  11 | Train loss: 0.484 | Val loss: 0.852 | Gen: etay ariway onditingnay isway okingingway\n",
            "Epoch:  12 | Train loss: 0.425 | Val loss: 0.799 | Gen: etay ariway onditingningcay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.374 | Val loss: 0.743 | Gen: ethay airway onditictingcay isway okingingway\n",
            "Epoch:  14 | Train loss: 0.330 | Val loss: 0.714 | Gen: ethay airway onditiongingcay isway orkinigway\n",
            "Epoch:  15 | Train loss: 0.305 | Val loss: 0.872 | Gen: etay airway onditicay isway okigray\n",
            "Epoch:  16 | Train loss: 0.309 | Val loss: 0.749 | Gen: ethay ariwayway onditioningay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.272 | Val loss: 0.785 | Gen: ettay airway onditionicaycay isway okingray\n",
            "Epoch:  18 | Train loss: 0.254 | Val loss: 0.958 | Gen: ehthay airway ondititongingcay isway orkiggway\n",
            "Epoch:  19 | Train loss: 0.252 | Val loss: 0.567 | Gen: ethay ariway onditioningcay isway orkingwway\n",
            "Epoch:  20 | Train loss: 0.212 | Val loss: 0.571 | Gen: ethay airway onditicingcncay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.196 | Val loss: 0.674 | Gen: ethay airway onditincongingcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.178 | Val loss: 0.472 | Gen: ethay airway onditioningcay isway orkingwway\n",
            "Epoch:  23 | Train loss: 0.174 | Val loss: 0.637 | Gen: ethay ariway onditioningcay iswiway orkingwway\n",
            "Epoch:  24 | Train loss: 0.166 | Val loss: 0.768 | Gen: ethay away onnditingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.251 | Val loss: 0.681 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.207 | Val loss: 0.730 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.218 | Val loss: 0.481 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.152 | Val loss: 0.494 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.131 | Val loss: 0.416 | Gen: ethay airway onditioningcngway isway orkingwway\n",
            "Epoch:  30 | Train loss: 0.117 | Val loss: 0.377 | Gen: ethay arirway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.083 | Val loss: 0.336 | Gen: ethay airway onditioningcay isway orkingwway\n",
            "Epoch:  32 | Train loss: 0.066 | Val loss: 0.319 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.058 | Val loss: 0.299 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.053 | Val loss: 0.300 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.048 | Val loss: 0.294 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.044 | Val loss: 0.298 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.041 | Val loss: 0.307 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.038 | Val loss: 0.299 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.039 | Val loss: 0.353 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.036 | Val loss: 0.310 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.034 | Val loss: 0.317 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.033 | Val loss: 0.327 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.047 | Val loss: 0.417 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.038 | Val loss: 0.382 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.091 | Val loss: 1.373 | Gen: ehay airway onditiioningcay isway orkiingway-igway\n",
            "Epoch:  46 | Train loss: 0.309 | Val loss: 1.253 | Gen: ehay aiway onditioniogcay isway okingway\n",
            "Epoch:  47 | Train loss: 0.233 | Val loss: 0.615 | Gen: ethay ariway ooditioningcay isway orkigingway\n",
            "Epoch:  48 | Train loss: 0.131 | Val loss: 0.406 | Gen: ethay airway onditinoningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.081 | Val loss: 0.362 | Gen: ethay airway onditinoningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.2939709716153388\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditinoningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6aa7fc19-411f-47f2-b57e-e5ccef8df55b"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\r\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "cd136b39-1304-4551-b1ea-0df33cffb57e"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "outputId": "7afa2939-f37f-4b89-c414-173a0cfd3729"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l, )"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU9bnv8c8zwybbALIzRk3imhVxOR6N4k6WG7NoDGa55uQVbqIxcRkQlT2IsmQxiSbBxMMx0eM9xuV4jQnmqiTGFRBEJGqUaBxANgFBDMzynD+qRttxerq7fjV09fB9++qX3UX1U0939fTTv/pV/X7m7oiIiJSqqtwJiIhIZVIBERGRRFRAREQkERUQERFJRAVEREQSUQEREZFEVEBKYGb9zOz8cuch+Wkfiew5KiCl6Qfoy6kdFinn50r7SGQPyXwBMbMvm9kTZrbczH5hZtVlTOca4H1xLnPLmEem3hczO8DMnjOzm4CVwH7lyoWM7CMz62VmvzOzp8xspZmdU65c4nzuMrOlZvaMmY0rYx4zzOyinMdXmdl3y5WPhLEsX4luZocBc4DPuXuDmV0PPObuN5UpnwOAe9z9g+XYfk4eWXxfVgP/6u6PlSOHVrlkYR99Hhjj7t+IH9e4+7Yy5jPA3V8zs32AxcCJ7r65DHkcANzh7kfELdW/AUeXIxcJ16XcCRRwCjAKWGxmAPsAG8qaUTZk8X15udzFI2OeBr5vZrOJCtpDZc7nO2b22fj+fsBBwB7/0nb3l8xss5mNBIYAy1Q8KlfWC4gB/+Hul5c7kYzJ4vvyRrkTyBJ3f97MjgA+Acw0s/vdfUY5cjGz0cCpwLHuvtPMFgE9ypFL7JfAecBQ4MYy5iGBst4Hcj9wlpkNhqgZbmb7lzGf7UCfMm6/RdbelyzJxD4ys+HATnf/DTAXOKKM6dQAW+LicSjwL2XMBeBOYAxwFLCwzLlIgEy3QNx9lZlNAu6Lj5c2ABcAL5cpn81m9rCZrQR+7+7jy5RHpt6XLMnKPgI+BMw1s2ai/fOtMuUB8Afgm2b2V+A5oKyHGt19t5k9CGx196Zy5iJhMt2JLiKdT/yj50ngbHf/W7nzkeSyfghLRDoRMzsceAG4X8Wj8qkFIiIiiagFIiIiiaiAiIhIIhVTQMo5/EJryuXdspIHKJd8lEvbspRLpamYAgJkaScrl3fLSh6gXPJRLm3LUi4VpZIKiIiIZEiHX0jYvXtP79Wrb3CcAQP2ZcCAoUGnjDU1NgbnAdC//wBq+g4MyuX17a+lkktNTQ1mVUG5xONpBedRVVUdfEqfe3MquZhZJk4vVC5t66S5bHL3QSnEYcyYMb5p06ai11+6dOlCdx+TxrZL0eEFpFevvpx2xv8OjnPG6R9m4X0rgmJs3bwxOA+As84+kd/e9qegGA88eHMquUyZMoXLLgsbEqtr1+7BeUydOpUrr5waHGfXrp3BMSZPnsyECZcFx2luDr9IevLkydTV1QXHSUN6uYT/4IhySWOQgPDv/RT3UWojQWzatIklS5YUvb6ZDUxr26XI9FAmIiJ7q0q4Rk8FREQkg5pVQEREpFSOWiAiIpKI4yn073Q0FRARkaxxaM5+/VABERHJGgeamsNPae9oKiAiIhmkPhAREUlEBURERErm7jqNV0REklELREREEtFpvCIiUjJHp/GKiEhCOoQlIiKJVEInelETSpnZ2WbWJ74/yczuMLMjOjY1EZG9lDtewq1cip2RcLK7bzez44FTgV8BP+u4tERE9l4tgylmvYBYMRs3s2XuPtLMrgaedvdbWpblWX8c8TzDAwbsO2ruvGuDE63puw/bXn8zKEZ6MxL2ZsuWHUExtm/fnEouI0aMYM2aNUEx0piRMI08AJpTGL6htraW+vr64DhpUC5t64y51NXVLXX3I1NIiY+MHOl/ePDBotcf3r9/atsuRbF9IGvM7BfAacBsM+tOO60Xd58PzAcYMGCoh84kCJqRMJ/Zs6/OxIyEV101PTMzEs6ZMzszMxLOmzcvMzMSppdL+A+OefPmZmZGwizto1yV0Ile7CGsLwALgTPcfSswAEhj74uIyLt4Sf+VS1EtEHffCdyR83gdsK6jkhIR2Zu5hnMXEZGkKuEQlgqIiEgGqYCIiEjJoqFMVEBERCQBtUBERKR0mg9ERESSUgtERERK5kCTCoiIiCShFoiIiCSiAiIiIiVzdaKLiEhSaoGIiEgiKiAiIlIyXYke27lzO08tWxQc54TjDwyOs3nz2uA8AD7xyZE8ueyPQTF69uybSi5VVdXBsXbs2BKcR3NzcypzeaT1q6sSfr1VrrTeW+2j9pRzmPZiqQUiIpJBlTCce7ETSomIyJ5Swnzoxba2zWyMmT1nZi+Y2cQ2/v09ZvagmS0zsxVm9olCMVVAREQyxiHVAmJm1cB1wMeBw4GxZnZ4q9UmAf/l7iOBLwLXF4qrQ1giIhmUcif60cAL7r4awMxuBc4EVuWs40BLh2oNULDTWAVERCSDSjwRZKCZLcl5PN/d5+c8HgG8kvO4HjimVYxpwH1mdiHQCzi10EZVQEREMqjEArLJ3Y8M3ORYYIG7f9/MjgV+bWYfdPfmfE9QARERyZgOGMpkDbBfzuPaeFmurwNj4u0/amY9gIHAhnxB1YkuIpJBXsJ/RVgMHGRmB5pZN6JO8rtbrfMP4BQAMzsM6AFsbC+oWiAiIhmU5nUg7t5oZt8GFgLVwI3u/oyZzQCWuPvdwKXADWZ2MVGH+nle4DiaCoiISMa0nMabakz3e4F7Wy2bknN/FXBcKTFVQEREMqgShuNRARERySANpigiIqUrYYiSclIBERHJGAeamvNefpEZBU/jNbPZxSwTEZH0pHwab4co5jqQ09pY9vG0ExERkbe5F38rl7yHsMzsW8D5wHvNbEXOP/UBHu7oxERE9laVMiOh5euoMbMaoD9wNZA7dvx2d3+t3aBm44BxAP369R81c+as4EQHD96XDRs2B8VobGwIzgNg2LAhrFu3PihGO8PLlGT48GGsXbsuKEZTU2NwHrW1tdTX1wfHSYNyaZtyaVtaudTV1S1NYTwqAA46/HD/0S23FL3+p0aOTG3bpcjbAnH3bcA2ogG2ShKPAjkfoEePXv6Tn/x74gRbXHjh1wiNk9aUtpMmTWDmzDlBMXbv/mcquUyfPompU2cGxUhjSts5c2YzYcJlwXHSOPNk7tw5jB8/IYVcwov8vHnzqKurC46TBuXStizlkqsSWiA6C0tEJGM64kr0jqACIiKSQSogIiKSiA5hiYhIAuW9vqNYKiAiIhlT7us7iqUCIiKSQTqEJSIiiagTXURESlYpV6KrgIiIZJBaICIiUjrNByIiIompgIiISBLerAIiIiIJVEADRAVERCRrogsJs19BVEBERDJIBQRoaNjNunWrU4izKzjOeRdcEZwHwICBQzj3G5cExfj70y+mkkvv3gM48YRzgmKcft6Y4DyGVDdx7X/dFRxn5gUXBsfo2rUbgwe/JzjOxo2vBMcAqKqqDo7R3NyUQibp6Nq1e3AMM0slTkPD7uAYEUshRppf+DoLS0REEnCH5qZ0Zi3tSCogIiIZpBaIiIgkowIiIiJJVED9UAEREckcd11IKCIiyagPRERESuaogIiISEIqICIikogKiIiIlM4d1IkuIiJJqAUiIiKJVED9UAEREckanYUlIiLJdJb5QMzMgFp3T2dsaxERKagSrkSvKrSCR2Xw3j2Qi4iIAC3zgRR7K5eCBST2pJkd1aGZiIjIWyqhgFgxGzezZ4H3Ay8DbxBN3+Xu/uE8648DxgHU1NSMmjZtenCiw4cPY+3adUEx9h00LDgPgF77dOWNNxuCYux6c1cqufTr14utW98IilEzsG9wHl2AxuAosO4f/wiOMXToEF59dX1wnDRmu6utraW+vj44ThrSyiU6qh1mxIgRrFmzJjhOGl+eab0vdXV1S939yOBAwH7vfb9fOmte0etfPPazBbdtZmOAa4Fq4Jfufk0b63wBmEbUj/+Uu5/bXsxiO9HPKHI9ANx9PjAfoLq6q0+dOrOUp7dp+vRJhMZJa0rbIz8wgiXPhH3405rS9tNnHsPd//14UIy0prRd3xQ+deusWT8IjnHFFZekEieNKW3nzJnNhAmXBcdJY0rbefPmUVdXFxwnjalor7lmFhMnhv89plHk582bS13d+OA4qUuxZWFm1cB1wGlAPbDYzO5291U56xwEXA4c5+5bzGxwobhFFRB3fzlZ2iIikoSnO6Pt0cAL7r4awMxuBc4EVuWs8w3gOnffAuDuGwoFLbYPRERE9qAS+0AGmtmSnNu4VuFGALlN6vp4Wa6DgYPN7GEzeyw+5NUuXQciIpI17jQ3l9QE2ZRC/0sX4CBgNFAL/NnMPuTuW/M9QS0QEZGMabkSPcWzsNYA++U8ro2X5aoH7nb3Bnf/O/A8UUHJSwVERCRrPLqQsNhbERYDB5nZgWbWDfgicHerde4ian1gZgOJDmmtbi+oDmGJiGRRimdhuXujmX0bWEh0Gu+N7v6Mmc0Alrj73fG/nW5mq4AmYLy7b24vrgqIiEjmpH+BoLvfS6tRRdx9Ss59By6Jb0VRARERyaAKGEtRBUREJIs6xWi8IiKyZ7lXxmi8KiAiIhmkFoiIiCSiAiIiIgmUd5j2YqmAiIhkTWeZ0lZERMpAnejQu3c/jjvucynE6R8c57YF1wXnAfC+iRcFxzrljLGp5FLdpZqagf2DYsy5eEJwHuPHX8DcueHvb69eNcExqqqqU4nz5s7twTGqq7rQu1e/4Djbd2wJjgFgFj56URoTSqUVJ0u5pNliiMbCSi1ch1ELREQkg3QIS0RESlfmuc6LpQIiIpJBupBQREQSUQtERERK1jKhVNapgIiIZE2FnIalAiIikjnqRBcRkYSam1RARESkVBrKREREklAnuoiIJKYCIiIiCbguJBQRkQTUByIiIompgIiISBIVUD8oanIAi3zZzKbEj99jZkd3bGoiInunlrOwir2VS7Gzy1wPHAu0zIK0HUhndiYREXknj0bjLfZWLlZM9TKzJ939CDNb5u4j42VPuftH8qw/DhgH0L//gFFz5vwwONF+/XqxdesbQTF27nw9OA+AoUOH8Oqr64Ni9Ok7IJVc+vbpwevb/xkUY8f28NnuhgwZzPr1G4LjpDEz3JAhg1i/fmNwnMaGhuAYw0cMY+2adcFxmpobg2PU1tZSX18fHCeNfTRixAjWrFkTHCeNX99pvS91dXVL3f3I4EDAkGH7+dh/u7To9a+ddXFq2y5FsX0gDWZWTdSywswGAc35Vnb3+cB8gJqaQX7HHQ+H5snnPnccoXGWL78/OA+AiRMv4pprfhQUI60pbU8+8TAe+NNfg2L86f7bg/NIa0rbrl27Bce4+OJx/PCH84PjbNoY/qUyffpkpk79XnCcNKa0nTt3DuPHh09fnMY+uvrqq7j88iuD4zQ07A6Okdb7krbOdBbWj4E7gcFmdhVwFjCpw7ISEdnLdZoC4u43m9lS4BTAgM+4e9jPXhERya+zFBAAd38WeLYDcxEREaLaoSvRRUQkkQpogKiAiIhkjyaUEhGRhFRARESkdBpMUUREknDUiS4iIgmpBSIiIqVzx5vzDvaRGSogIiIZVAENEBUQEZEsUh+IiIiUrGU+kKxTARERyZoKOY232AmlRERkjyl+NsJiC42ZjTGz58zsBTOb2M56nzczN7OC84t0eAvkjTe28thjdwfHOf30DwTH2b79teA8ABobd7Nx4ytBMR77y+9TyeWYUcODY73xxtbgPJqbm1KJc8ghxwTH6NKlO0OGHBgcp0f3XsExunbrQW3tIcFxXnxxWXAMsyq6de0eHGefnn2DY1RVdaFnz5rgODtSmCfFzKiuDv8qbGwMn5skV5otkHg+p+uA04B6YLGZ3e3uq1qt1wf4LvB4MXHVAhERyaCUp7Q9GnjB3Ve7+27gVuDMNtb7HjAbKGqaUxUQEZGsiXrRi7/BQDNbknMb1yriCCD3sEl9vOwtZnYEsJ+7/67YNNWJLiKSMS31owSbQuZEN7Mq4AfAeaU8TwVERCSDUj4Law2wX87j2nhZiz7AB4FFZgYwFLjbzD7t7kvyBVUBERHJnNTnA1kMHGRmBxIVji8C5761NfdtwMCWx2a2CKhrr3iACoiISPakPKWtuzea2beBhUA1cKO7P2NmM4Al7p7oFFcVEBGRDEr7QkJ3vxe4t9WyKXnWHV1MTBUQEZGM0VAmIiKSmAqIiIgk4CWfx1sOKiAiIlnj4NmfT0oFREQki3QIS0REElEBERGRkuksLBERSaZCJpQqqoBYNDjKl4D3uvsMM3sPMNTdn+jQ7ERE9kqON2W/F92KqXJm9jOgGTjZ3Q8zs/7Afe5+VJ71xwHjAGpqakZNmzYjONHhw4eydu2rQTGamxuD8wAYMWIEa9asKbxiO7p12yeVXIYMGcj69ZuCYjQ1NQTnMWzYUNatC9s/AN1TmMRp331r2Lx5W3Ccpqbwz8vgwfuyYcPm4Di7du0MjpHG5xagqqo6OEYaf8+Qzt90Wu/LpZdeujRkRNxc/fsP8dGjxxa9/l13XZvatktR7CGsY9z9CDNbBuDuW8ysW76V3X0+MB+gS5euPmPG1cGJTplyOaFx0pqRcPbsq7nsssuDYhxwwIdSyeW73/061177q6AYr722NjiPKVMmMmPGNcFx0piR8Ktf/RQ33XRPcJxtWzcExzj/gq9y/XU3BcdJY0bCWVdfxRWXXxkcJ40ZCadOvYLp02cFx0ljRsI0/p7T5p3pEBbQEE+J6ABmNoioRSIiIqlzvAIuBCm2gPwYuBMYbGZXAWcBkzosKxGRvVynaYG4+81mthQ4BTDgM+7+1w7NTERkL9ZpCgiAuz8LPNuBuYiISKxTFRAREdkz3DtXH4iIiOxJaoGIiEgSjgqIiIgkoD4QERFJRAVEREQSUCe6iIgk0NmGMhERkT1IBURERBJRARERkQRc14GIiEgyXgEDnquAiIhkkA5hEc3qtmVL+MxjTU0NqcRJg7vT2Lg7KMbq1ctTyWXXrp3BsdL4oDY2NrBly/rgOGlMnLRr18mpxPHm8F+AjY272bjxleA4PXvVBMeoqqpOJU40w3VoDKiqqgqOk8aske6eSpw06SwsERFJyFVAREQkmebmpnKnUJAKiIhIBqkFIiIipXOdxisiIgk4Gs5dREQS0mCKIiKSgM7CEhGRhFRAREQkERUQEREpWXQSlvpARESkZOoDERGRpFRAREQkCV0HIiIiiVTCIayC4ymb2exilomISFoc9+aib+VSzID8p7Wx7ONpJyIiIpGW+UCKvRXDzMaY2XNm9oKZTWzj3y8xs1VmtsLM7jez/QvGzLdxM/sWcD7wXuDFnH/qAzzs7l9uJ9FxwDiAmpqaUZMnTy6UR0G1tbXU19cHx0mDcum4PLp06RYcY9iwIaxbFz65VRrSyiWNwxnDhw9l7drwSdlSmE+KYcOGsm5deC6NjQ3BMdL67NbV1S119yODAwE9e/b1Qw45uuj1ly+/v91tm1k18DxRg6AeWAyMdfdVOeucBDzu7jvj7//R7n5Oe9ttrw/kFuD3wNVAbrXa7u6vtRfU3ecD8+OkvK6urr3VizJv3jzSiJOGNHKpqqpOJZc5c2YzYcJlQTHS+HKaO3cO48dPCI4zaNB+wTGuuOJSZs36fnCcNGYkvHLSeK6aOTc4TmNT+BfllCmXM2PG1cFx0piRcMqUicyYcU1wnDRmwUzrs5u2lPtAjgZecPfVAGZ2K3Am8FYBcfcHc9Z/DMjbSGiRt4C4+zZgGzA2YcIiIpJQiQVkoJktyXk8P/4h32IEkDu3cj1wTDvxvk7UgGiXzsISEckch9I6xzeldfjMzL4MHAmcWGhdFRARkQxK+TqQNUDu8eHaeNk7mNmpwJXAie6+q1BQFRARkYxpOQsrRYuBg8zsQKLC8UXg3NwVzGwk8AtgjLtvKCaoCoiISOY4zc1N6UVzbzSzbwMLgWrgRnd/xsxmAEvc/W5gLtAbuC0+UeIf7v7p9uKqgIiIZFDaV6K7+73Ava2WTcm5f2qpMVVAREQyqBKGMlEBERHJmA7oA+kQKiAiIpnjGs5dRESScTQjoYiIJKBDWCIikogKiIiIJKA50UVEJIHoLCz1gYiISAJqgYiISCIqIFIR9t13eHCMLl26pRLnlNPPLbxSAX37DkglzglnnxAcY1/fzYxfzS+8YgGfOr742eny+euyZax44ZngOKs3FDXOXrtef+kl7nz4geA4Jxx6aHCMRYsWpTLuVBoTbb1N14GIiEhCKQ/n3iFUQEREMkid6CIiUjKNhSUiIgnpOhAREUlIBURERBJRARERkUTUiS4iIqVzXQciIiIJOLoOREREEkrj6viOpgIiIpI5Oo1XREQSUgEREZGS6Up0ERFJTAVEREQScNB1ICIikkQlnMZr7TWTzOwo4BV3fzV+/FXg88DLwDR3fy3P88YB4wBqampGTZ48OTjR2tpa6uvrg+OkobPl0qVLt+A8hg0bwrp164Pj9O07IIUY+/D6628Gx+k9oE9wjC44jYRPNNSvd6/gGP/cuZMePXsGx9nV0BAco2n3bqq7hX/uevfoERxjx44d9O7dOzjOSSedtNTdjwwOBHTp0tX79Cn+b2Hr1g2pbbsUhQrIk8Cp7v6amZ0A3ApcCHwUOMzdzyq4AbNUyui8efOoq6tLI1SwNHKpqqpOJZc5c2YzYcJlQTEGDBgWnMekSROYOXNOcJzTxnwlPMbJH+CPD4TPvJfWjISbLfyLMq0ZCQ8bOTI4TlozEvY94IDgOGnNSDh69OjgOGaWagHp3bt/0etv27axLAWk0CGs6pxWxjnAfHe/HbjdzJZ3bGoiInsnd6+IsbCqCvx7tZm1FJlTgNxJjNV/IiLSQaIiUtytXAoVgf8E/mRmm4A3gYcAzOz9wLYOzk1EZK9V8afxuvtVZnY/MAy4z99+RVVEfSEiItIBKr6AALj7Y2Z2EvA1MwN4xt0f7PDMRET2ZpVeQMxsBHAH8E9gabz4bDObDXzW3dd0cH4iInshx8l+J3qhFshPgZ+5+4LchfH1INcDZ3ZQXiIie61KGQur0FlYh7cuHgDufhMQfgK2iIi0qTOchdVmgTGzKiCdK+FERORdOkML5B4zu8HM3hpHIb7/c+DeDs1MRGSvVXzro5yFplABmUB0vcfLZrbUzJYCLwGvA9kYV0REpBNyby76Vi6FrgNpAOrMbDLw/njxi+6+s8MzExHZS3WKTnQzmwDg7m8Ch7r70y3Fw8xm7YH8RET2Ql4RLZBCh7C+mHP/8lb/NiblXEREJFYJBaTQWViW535bj0VEJCWVcAirUAHxPPfbeiwiIimphAJSaEKpJuANotbGPkBL57kBPdy9a8ENmG0kmsEw1EBgUwpx0qBc3i0reYByyUe5tC2tXPZ390EpxMHM/kCUV7E2ufse71Zot4BkiZktKceMW21RLtnNA5RLPsqlbVnKpdIU6kQXERFpkwqIiIgkUkkFZH65E8ihXN4tK3mAcslHubQtS7lUlIopIO5e8k42s8+YmZvZoTnLPmpmn8h5PNrM/jVpLmbWz8zOz3k83Mx+W2quSbX1vpjZN+Mh9/Mys/PM7Kd5/u2KUnIws/OAe0p9jpkNz3n8kpmV0mmYV5LPSkcJycXMDjCzc8uVi5k9UuL6C8zsrI7IJc/2DjWzR81sl5klHlopS5+XSlMxBSShscBf4v+3+CjwiZzHo4GSCkgr/YC3Coi7r3X3ov6IOoq7/zwecj+pkgoIcB4wvNBKKTxnjzGzgrN17gEHAKkVkFK5e8jfRera2CevAd8B5pUhHYHSxpyvpBvQG1gDHAw8Fy/rBvwD2AgsBy4DXo3XWw58DBgE3A4sjm/Hxc+dBtwILAJWA9+Jl98KvBk/fy7RH/3K+N96AP8OPA0sA06Kl59HNNPjH4C/AXPayP8o4I74/pnxNrrFMVfHy98Xx1gKPEQ03ExLrnU5cVbk5LeyvRyAa4CmeP2bgV7A74CngJXAOa3yPAvYATwXP2cf4JT49T4dv2fdi3jOS8B04Mn4eS2vpVcc44k45pltvFfDgD/HsVYCH4uXj41jrQRm56y/o1UuC+L7C4hGmn4c+AHR+G//P37tTwLvi9cbT/TZWAFMbyOf6jjWynj7FxfYXwuAHwOPEH22zoqXP0Y0mOly4OI47tycbf+feL3RRJ/L3wLPxvvNcvb/I/FreALoky9OG69jR6H4rdZfkJP7lDj+SqJDRBa//idz1j+o5TEwCvhT/N4sBIbFyxcBPwKWAJfmyXMa8eddtz38PVvuBDrshcGXgF/F9x8BRsX3zwN+mrPeOz58wC3A8fH99wB/zVnvEaA70fnZm4Gu5BSMeL23HgOXAjfG9w8lKl494hxWAzXx45eB/Vrl34W3C8W8+I/xOOBE4D/j5fcDB8X3jwEeaP2a4j/gY+P71/DOAtJmDrzzC/bzwA05j2vaeK8XAUfG93sArwAHx49vAi5q7znx45eAC+P75wO/jO/PAr4c3+8HPA/0ahXrUuDK+H410Zfk8Pj9HhS/lw8An2nj9bUuIPcA1fHjx4mmbm55XT2B03n7C7EqXv+EVvmMAv6Y87hfgf21ALgtjnc48EK8fDRwT06cccCk+H53oi/VA+P1tgG1cYxHgeOJfnCsBo6Kn9M3fi/ajNPGPsotIO+K38b6C3i7gAzIWf5r4H/F9x8EPpqzby8k+jt6BBgULz+Ht/9uFgHXF/hbn4YKSFluWWimd5SxwLXx/Vvjx0vzr/6WU4HDzd4aqaWvmfWO7//O3XcBu8xsAzCkQKzjgZ8AuPuzZvYyUYsI4H533wZgZquA/Ym+eInXbzSzF83sMOBool/EJxB9QT4U5/SvwG05uXbP3biZ9QP6uPuj8aJbgE/lrNJuDrGnge+b2WyiL7OHCrzmQ4C/u/vz8eP/AC4g+hVZyB3x/5cCn4vvnw58OucYdw/iwp7zvMXAjWbWFbjL3Zeb2cnAInffGL++m4nev7sK5HCbuzeZWR9ghLvfCeDu/4zjnB7ntCxevzfRL1hi9oEAAAQSSURBVOk/58RYDbzXzH5C1Hq7r4j9dZdHgxqtMrN8n6vTgQ/n9DPUxNveDTzh7vVxjsuJfshsA9a5++L4Nbye8xraivP3dt6XtuL/pZ31T4oHY+0JDACeAf4f8Evga2Z2CVGhOJroM/NB4I/xe1MNrMuJ9X/b2Y6UUacsIGY2ADgZ+JCZOdEH0s1sfBFPrwL+peULIycmwK6cRU2EvX/FxPoz8HGggehQygKi1zI+znOru3+0I3Nw9+fN7AiifqOZZna/u88I2GYx+eTmYsDn3f25fE9y9z+b2QnAJ4EFZvYDoi/PvE/Jud+j1b+9USBHA65291+0k88WM/sIcAbwTeALwEW0v79y90W+ceaMqJW28B0LzUZT2mezzTgFFB3fzHoA1xO1MF8xs2m8/T7fDkwlahEudffN8ckUz7j7sXlCFtonUiadtRP9LODX7r6/ux/g7vsR/br6GLCd6BBHi9aP7yNqVgPRWVsFttX6+bkeIjqUhpkdTPTLOe8XYZ7nXwQ8Gv+S3pfo19rK+Nfk383s7Di+xV9ab3H3rcB2MzsmXpQ7unJ7GuJf88R/3Dvd/TdEx82PaGP93PfgOeAAM2uZP+YrRMe223tOexYCF1pcwc1sZOsVzGx/YL2730D0C/cIouP9J5rZQDOrJmqBtuSx3swOi6dm/mxbG3X37UC9mX0m3kZ3M+sZ5/NvLa1SMxthZoNb5TMQqHL324FJwBHF7K82tH6PFgLfytk3B1vObKFteA4YZmZHxev3iTuiS41TqpZisSl+n946qST+YbYQ+BlR/2BLnoPM7Ng4n65m9oEU85EO0lkLyFjgzlbLbo+XP0h0iGq5mZ1D1Kz+bPz4Y0RndRxpZiviwzrfbG9D7r4ZeNjMVprZ3Fb/fD1QZWZPEzXDz4sPgRXrcaLDZC2HR1YAT7t7yy/oLwFfN7OniA4RnNlGjK8DN8SHHXrR/i/zFvOBFfFhnw8BT8TPnwrMbGP9BcDP43UM+BrRoZqngWaijum8zzGzfdrJ5XtEx8hXmNkz8ePWRgNPmdkyosMi17r7OmAi0f5+iujX7n/H608k6rt4hHceKmntK8B3zGxFvO5Qd7+P6FDgo/Hr+y3vLoQjgEXx+/Eb3p4KoZj9lWsF0GRmT5nZxUTFcRXwpJmtBH5BOy0Bd98dvx8/ibf5R6Iv95LilCr+4XIDUf/bQqJDjLluJvpc3JeT51nA7DjP5RRxZqSZDTWzeuASYJKZ1ZtZ37RehxRWMWNhSTJm1tvdd8T3JxKd3fLdMqcle7G4P6vG3SeXOxcJ0yn7QOQdPmlmlxPt65eJzr4SKQszu5PodN6Ty52LhFMLREREEumsfSAiItLBVEBERCQRFRAREUlEBURERBJRARERkUT+Bwxbjj0Jg5GSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'etertsay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}