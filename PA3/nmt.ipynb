{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "ecEq4TP2lZ4Z",
        "RWwA6OGqlaTq",
        "AJSafHSAmu_w",
        "73_p8d5EmvOJ",
        "vYPae08Io1Fi",
        "9tcpUFKqo2Oi",
        "z1hDi020rT36",
        "MBnBXRG8mvcn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donganzh/csc413/blob/main/PA3/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "3fa3f759-daa0-40f8-9a00-9eb5aa8708f2"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/80/eca7a2d1a3c2dafb960f32f844d570de988e609f5fd17de92e1cf6a01b0a/Pillow-4.0.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 14.4MB/s \n",
            "\u001b[?25hCollecting olefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 54.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow, olefile\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl size=1007346 sha256=1be1621dba51830fea8d2a4371f3c02a26996c9b12315e36e998fc645718dc55\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=0b29478483b92832d6247963c2eaedd52cabaca2e8f33814fa57c4f58ede066c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built Pillow olefile\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: olefile, Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0 olefile-0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79488230-6337-41bf-81fc-2e87cd656389"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
        "        g = torch.tanh(self.Wig(x) + self.Whg(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
        "        c_new = f * c_prev + i * g\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "        return h_new, c_new"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3e104d-2595-4c1f-e22a-b7fb56e2c04e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':64,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('involved', 'involvedway')\n",
            "('contemptuous', 'ontemptuouscay')\n",
            "('seclude', 'ecludesay')\n",
            "('declaration', 'eclarationday')\n",
            "('obtain', 'obtainway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.334 | Val loss: 2.084 | Gen: ay ay-ay insay-ay insay illay\n",
            "Epoch:   1 | Train loss: 1.846 | Val loss: 1.831 | Gen: ay-ay-ay-ay-ay atinsay ontinstinsay-ay-ay-a illay illlay-ay-ay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.661 | Val loss: 1.720 | Gen: ay-ay-ay-ay-ay ay-onday intionstionsay ilway oonstay\n",
            "Epoch:   3 | Train loss: 1.536 | Val loss: 1.701 | Gen: eday aleday ingingsay-onsay ilway onstay-ay-ay-ay-ay-a\n",
            "Epoch:   4 | Train loss: 1.424 | Val loss: 1.629 | Gen: eay allay ingionstay-onay iway ontingay\n",
            "Epoch:   5 | Train loss: 1.320 | Val loss: 1.504 | Gen: etay alay onlledingsay iway ontionsay\n",
            "Epoch:   6 | Train loss: 1.219 | Val loss: 1.381 | Gen: etay alay onsiontionstay isway orledway\n",
            "Epoch:   7 | Train loss: 1.134 | Val loss: 1.352 | Gen: etay alay incolledionay isway ortingway\n",
            "Epoch:   8 | Train loss: 1.072 | Val loss: 1.316 | Gen: etay alay onciongitionay isway ortionay\n",
            "Epoch:   9 | Train loss: 1.018 | Val loss: 1.251 | Gen: etay alway oncitiongingay isway ortionway\n",
            "Epoch:  10 | Train loss: 0.979 | Val loss: 1.320 | Gen: ehay arway ontiongiongay isway ortionway\n",
            "Epoch:  11 | Train loss: 0.937 | Val loss: 1.175 | Gen: ehay ariway ontingiondedway isway oringhay\n",
            "Epoch:  12 | Train loss: 0.881 | Val loss: 1.152 | Gen: ehay alway ontiongioncay isway otingway\n",
            "Epoch:  13 | Train loss: 0.841 | Val loss: 1.179 | Gen: ehay arway ingionsioncay isway ontionay\n",
            "Epoch:  14 | Train loss: 0.810 | Val loss: 1.113 | Gen: ehay ariway ondingicictay isway irtionay\n",
            "Epoch:  15 | Train loss: 0.768 | Val loss: 1.060 | Gen: ehay arway ongingioncay isway oringhay\n",
            "Epoch:  16 | Train loss: 0.723 | Val loss: 1.115 | Gen: ehay away-imway ondingionsicay isway oringhay\n",
            "Epoch:  17 | Train loss: 0.716 | Val loss: 1.072 | Gen: ehay arway ongingificatway isway oringhay\n",
            "Epoch:  18 | Train loss: 0.698 | Val loss: 1.036 | Gen: ehay arway ondingitioncay isway oringhay\n",
            "Epoch:  19 | Train loss: 0.669 | Val loss: 1.055 | Gen: ehay ariway ondingietioncay isway ioway-ireway\n",
            "Epoch:  20 | Train loss: 0.657 | Val loss: 1.017 | Gen: ehay arway ondingionstcay isway irrityway\n",
            "Epoch:  21 | Train loss: 0.623 | Val loss: 0.968 | Gen: ehay ariway ondingicioncay isway oringhay\n",
            "Epoch:  22 | Train loss: 0.583 | Val loss: 0.979 | Gen: ehay ariway onsingitioncay isway oringhay\n",
            "Epoch:  23 | Train loss: 0.558 | Val loss: 0.961 | Gen: ehay ariway ondingitiocay isway oringhay\n",
            "Epoch:  24 | Train loss: 0.558 | Val loss: 0.978 | Gen: ehay ariway ondingionsday isway oriway-itway\n",
            "Epoch:  25 | Train loss: 0.559 | Val loss: 0.982 | Gen: ehay ariway ondingiticatway isway orideway\n",
            "Epoch:  26 | Train loss: 0.542 | Val loss: 0.999 | Gen: ehay airway ondingificatway isway oriwrhay\n",
            "Epoch:  27 | Train loss: 0.515 | Val loss: 0.930 | Gen: ehay ariway ondingitionday isway oriway-iway\n",
            "Epoch:  28 | Train loss: 0.493 | Val loss: 0.963 | Gen: ehay arway ondingitioncay isway oringhay\n",
            "Epoch:  29 | Train loss: 0.484 | Val loss: 0.935 | Gen: ehay arway ondingionstcay isway oriway-imeway\n",
            "Epoch:  30 | Train loss: 0.481 | Val loss: 0.997 | Gen: ehay arway ondingitionsay isway oringhay\n",
            "Epoch:  31 | Train loss: 0.475 | Val loss: 0.934 | Gen: ehay airway ondingifictay isway oriway-ieway\n",
            "Epoch:  32 | Train loss: 0.459 | Val loss: 0.947 | Gen: ehay arwray onditionstay-onway isway oririway\n",
            "Epoch:  33 | Train loss: 0.439 | Val loss: 0.899 | Gen: ehay ariway onditionsticay isway oridgay\n",
            "Epoch:  34 | Train loss: 0.413 | Val loss: 0.905 | Gen: ehay arway onditionsicay isway oriway-iemay\n",
            "Epoch:  35 | Train loss: 0.397 | Val loss: 0.905 | Gen: ehay arwray onditiongicay isway oriwhpay\n",
            "Epoch:  36 | Train loss: 0.392 | Val loss: 0.903 | Gen: ehay arwhay onditionsticay isway oridgeway\n",
            "Epoch:  37 | Train loss: 0.384 | Val loss: 0.932 | Gen: ehay arwray onditiongstay isway oridgway\n",
            "Epoch:  38 | Train loss: 0.381 | Val loss: 0.937 | Gen: ehay arway onditiongscycay isway origitway\n",
            "Epoch:  39 | Train loss: 0.388 | Val loss: 0.961 | Gen: ehay ariway onditionday-iceway isway oridingay\n",
            "Epoch:  40 | Train loss: 0.370 | Val loss: 0.924 | Gen: ehay airway onditiongitay isway oridgiway\n",
            "Epoch:  41 | Train loss: 0.363 | Val loss: 0.935 | Gen: ehay airway onditionsieday isway oridingay\n",
            "Epoch:  42 | Train loss: 0.362 | Val loss: 0.967 | Gen: ehay airway onditionstay-ay isway oririway\n",
            "Epoch:  43 | Train loss: 0.348 | Val loss: 0.919 | Gen: ehay airway onditiongstcay isway originway\n",
            "Epoch:  44 | Train loss: 0.338 | Val loss: 0.915 | Gen: ehay airway onditionsiceway isway oridinway\n",
            "Epoch:  45 | Train loss: 0.324 | Val loss: 0.941 | Gen: ehay airway onditionstay isway oriwhay\n",
            "Epoch:  46 | Train loss: 0.317 | Val loss: 0.917 | Gen: ehay airway onditionsday-ay isway origionway\n",
            "Epoch:  47 | Train loss: 0.319 | Val loss: 0.888 | Gen: ehay airway onditionsioncay isway ordionway\n",
            "Epoch:  48 | Train loss: 0.305 | Val loss: 0.881 | Gen: ehay airway onditiongitday isway origizyway\n",
            "Epoch:  49 | Train loss: 0.290 | Val loss: 0.922 | Gen: ehay airway onditiongshay isway oridgway\n",
            "Obtained lowest validation loss of: 0.8813275418470841\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway onditiongshay isway oridgway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a76503-9678-414d-8018-f7a77e267aad"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('declaration', 'eclarationday')\n",
            "('desperately', 'esperatelyday')\n",
            "('aspire', 'aspireway')\n",
            "('timeshare', 'imesharetay')\n",
            "('aniston', 'anistonway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.318 | Val loss: 2.131 | Gen: ay ay-ay ontay-ay-ay-ay ay ontay-ay\n",
            "Epoch:   1 | Train loss: 1.865 | Val loss: 1.937 | Gen: eray away ontay-ay-ay-ay-ay inay ontay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.676 | Val loss: 1.830 | Gen: etay away-ay ontintay-otay-ay isay ontay-onay\n",
            "Epoch:   3 | Train loss: 1.532 | Val loss: 1.742 | Gen: eway away ontiontay-away isway ongay-inay\n",
            "Epoch:   4 | Train loss: 1.405 | Val loss: 1.620 | Gen: etay away ontiontistay-ay isay onghay-ay\n",
            "Epoch:   5 | Train loss: 1.278 | Val loss: 1.546 | Gen: etay away-ay ontistay-onday isway ongsay-onway\n",
            "Epoch:   6 | Train loss: 1.197 | Val loss: 1.508 | Gen: etay aray onticay-otay-anway isay oungsay\n",
            "Epoch:   7 | Train loss: 1.130 | Val loss: 1.441 | Gen: ethay aray onticationgsay isway ongray-inay\n",
            "Epoch:   8 | Train loss: 1.042 | Val loss: 1.405 | Gen: ethay artay onticationglay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.993 | Val loss: 1.389 | Gen: ethay artay onticationtlay isway okeshay\n",
            "Epoch:  10 | Train loss: 0.950 | Val loss: 1.355 | Gen: ethay artay onticomatingway isway ovingray\n",
            "Epoch:  11 | Train loss: 0.887 | Val loss: 1.314 | Gen: ethay artay onticticay-omay isway orvingway\n",
            "Epoch:  12 | Train loss: 0.838 | Val loss: 1.292 | Gen: ethay artay onticationmay-ay isway okingray\n",
            "Epoch:  13 | Train loss: 0.802 | Val loss: 1.273 | Gen: ethay artay onticomtanigway isway ovingray\n",
            "Epoch:  14 | Train loss: 0.771 | Val loss: 1.254 | Gen: ethay artay onticay-indicay-aywa isway ovingray\n",
            "Epoch:  15 | Train loss: 0.726 | Val loss: 1.203 | Gen: ethay ariway ondicticay-ontay isway okingray\n",
            "Epoch:  16 | Train loss: 0.697 | Val loss: 1.202 | Gen: ehtay artiay ondictinomtay isway orighay-yway\n",
            "Epoch:  17 | Train loss: 0.669 | Val loss: 1.220 | Gen: ethay artay omtingringtay-ayday isway orghingway\n",
            "Epoch:  18 | Train loss: 0.644 | Val loss: 1.149 | Gen: ethay artiay onticompntinay isway okingrway\n",
            "Epoch:  19 | Train loss: 0.620 | Val loss: 1.144 | Gen: ethay artiway ondictingdray-aysay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.619 | Val loss: 1.206 | Gen: ethay artiway onticomstmandway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.596 | Val loss: 1.145 | Gen: ethay artiway onticomstmay-oway isway okingway\n",
            "Epoch:  22 | Train loss: 0.556 | Val loss: 1.141 | Gen: ethay ariway ondictingtmay-ay isway okingrway\n",
            "Epoch:  23 | Train loss: 0.531 | Val loss: 1.122 | Gen: ehay ariway ondictingnaty isway okingrway\n",
            "Epoch:  24 | Train loss: 0.519 | Val loss: 1.077 | Gen: ethay iarway ondictinedtalway isway okingrway\n",
            "Epoch:  25 | Train loss: 0.499 | Val loss: 1.127 | Gen: ehay ariway ondictinomansday isway okingrway\n",
            "Epoch:  26 | Train loss: 0.485 | Val loss: 1.102 | Gen: ethay ariway ondictinousgay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.480 | Val loss: 1.069 | Gen: ethay iarway ondingtingriay isway okingrway\n",
            "Epoch:  28 | Train loss: 0.465 | Val loss: 1.114 | Gen: ethay iarway ondictintmansway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.477 | Val loss: 1.112 | Gen: ehtay ariway ondictinontmay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.459 | Val loss: 1.056 | Gen: ethay iarway ontigingtrinayday isway orkigway\n",
            "Epoch:  31 | Train loss: 0.443 | Val loss: 1.071 | Gen: ethay ariway ondictinongstay isway orgingway\n",
            "Epoch:  32 | Train loss: 0.420 | Val loss: 1.033 | Gen: ethay iarway ondictinonsiway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.400 | Val loss: 1.064 | Gen: ethay iarway ondictinonsgay isway orgingway\n",
            "Epoch:  34 | Train loss: 0.388 | Val loss: 1.011 | Gen: ehay iarway ontignicationway isway orgingway\n",
            "Epoch:  35 | Train loss: 0.375 | Val loss: 1.050 | Gen: ethay iarway ontignicationway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.376 | Val loss: 1.033 | Gen: ethay iarway ontignicationway isway orgingway\n",
            "Epoch:  37 | Train loss: 0.370 | Val loss: 0.997 | Gen: ethay iarway ontigingompray isway orgingway\n",
            "Epoch:  38 | Train loss: 0.355 | Val loss: 1.019 | Gen: ethay iarway ontigincationway isway orgingway\n",
            "Epoch:  39 | Train loss: 0.348 | Val loss: 1.010 | Gen: ethay iarway ontigingompray isway orkingway\n",
            "Epoch:  40 | Train loss: 0.352 | Val loss: 1.008 | Gen: ethay iarway ontingicay-otay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.343 | Val loss: 0.979 | Gen: ethay iarway ontigingompray isway orkingway\n",
            "Epoch:  42 | Train loss: 0.342 | Val loss: 1.048 | Gen: ethay ariway ontingicay-omstay isway orgingway\n",
            "Epoch:  43 | Train loss: 0.349 | Val loss: 1.045 | Gen: ethay iarway ondingiticay-away isway orkingway\n",
            "Epoch:  44 | Train loss: 0.338 | Val loss: 0.978 | Gen: ethay airway ontingicationway isway orgingway\n",
            "Epoch:  45 | Train loss: 0.317 | Val loss: 1.000 | Gen: ethay airway ontingicationday isway orkingway\n",
            "Epoch:  46 | Train loss: 0.314 | Val loss: 1.017 | Gen: ethay airway ontingdirationway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.304 | Val loss: 0.981 | Gen: ethay iarway ontingicatiovay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.288 | Val loss: 1.013 | Gen: ethay airway ontingicationway isway orgingway\n",
            "Epoch:  49 | Train loss: 0.287 | Val loss: 0.997 | Gen: ethay airway ontingicay-otay isway orkingway\n",
            "Obtained lowest validation loss of: 0.9783914646239213\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ontingicay-otay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "00fa4761-619e-4d50-9325-6040bbdf0bd4"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8530c812-2f12-4e99-e4f2-59cf626d85e2"
      },
      "source": [
        "best_encoder = rnn_encode_s # Replace with rnn_losses_s or rnn_losses l\n",
        "best_decoder = rnn_decoder_s # etc.\n",
        "best_args = rnn_args_s\n",
        "\n",
        "TEST_SENTENCE = 'the princess listened peacefully to what the frogs had to sing'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe princess listened peacefully to what the frogs had to sing \n",
            "translated:\tehay inscessray istenedlay anedchlayssay otay athay ehay osghay adhay otay ingsay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
        "            \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e98913-ac28-4653-8108-76b9c2e84f91"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('oneself', 'oneselfway')\n",
            "('heighten', 'eightenhay')\n",
            "('addressing', 'addressingway')\n",
            "('sort', 'ortsay')\n",
            "('cried', 'iedcray')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.176 | Val loss: 1.887 | Gen: ay ay-ay onstingay isssssssay onsssay\n",
            "Epoch:   1 | Train loss: 1.640 | Val loss: 1.544 | Gen: etay arsitiay ooncounsingsay issay oodgay-ay-ingay\n",
            "Epoch:   2 | Train loss: 1.300 | Val loss: 1.413 | Gen: etay atay ongingway-ontay isay ongray\n",
            "Epoch:   3 | Train loss: 1.086 | Val loss: 1.301 | Gen: etay arsay onsigingway issay ongigway\n",
            "Epoch:   4 | Train loss: 0.882 | Val loss: 1.123 | Gen: ehtay ariway ondintingway isway ongingway\n",
            "Epoch:   5 | Train loss: 0.747 | Val loss: 0.984 | Gen: etay aray ondingingiongay isay orkingway\n",
            "Epoch:   6 | Train loss: 0.591 | Val loss: 0.976 | Gen: ehtay airway onditionmintingway issay orkingway\n",
            "Epoch:   7 | Train loss: 0.539 | Val loss: 0.839 | Gen: ethay airway onditiondingngnay isway orkingway\n",
            "Epoch:   8 | Train loss: 0.490 | Val loss: 0.990 | Gen: etay airway onditionsay isay orkingway\n",
            "Epoch:   9 | Train loss: 0.429 | Val loss: 0.683 | Gen: ehtay airway onitingingway isway orkingway\n",
            "Epoch:  10 | Train loss: 0.311 | Val loss: 0.526 | Gen: ethay airway onditititingway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.218 | Val loss: 0.409 | Gen: ehtay airway onditionpingway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.167 | Val loss: 0.431 | Gen: ethay airway onditionicngway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.173 | Val loss: 0.411 | Gen: ehtay airway onditiongingway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.129 | Val loss: 0.311 | Gen: ehtay airway onditiongingway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.096 | Val loss: 0.425 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.106 | Val loss: 0.430 | Gen: ehay airway onditincingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.099 | Val loss: 0.290 | Gen: ehtay airway onditiongingway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.067 | Val loss: 0.393 | Gen: ehtay airway onditionipgay isway orkway-ingway\n",
            "Epoch:  19 | Train loss: 0.078 | Val loss: 0.474 | Gen: ewhay airway onitionipgray isway orkingway\n",
            "Epoch:  20 | Train loss: 0.076 | Val loss: 0.341 | Gen: ehay airway onditiongingway isway orkwngway\n",
            "Epoch:  21 | Train loss: 0.049 | Val loss: 0.240 | Gen: ehay airway onditionipgnay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.033 | Val loss: 0.207 | Gen: ehtay airway onditioniongway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.022 | Val loss: 0.220 | Gen: ehtay airway ondititiongway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.027 | Val loss: 0.288 | Gen: ehtay airway ondititiongway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.111 | Val loss: 0.520 | Gen: ehtay airway onditingingway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.217 | Val loss: 0.636 | Gen: ehay awray onditingingnay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.198 | Val loss: 0.563 | Gen: etay airway onditicingingway isway orkigway\n",
            "Epoch:  28 | Train loss: 0.151 | Val loss: 0.369 | Gen: ethay airway onditiningnay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.078 | Val loss: 0.269 | Gen: ehay airway onditioningway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.042 | Val loss: 0.234 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.025 | Val loss: 0.174 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.014 | Val loss: 0.161 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.010 | Val loss: 0.158 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.008 | Val loss: 0.156 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.006 | Val loss: 0.154 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.005 | Val loss: 0.154 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.005 | Val loss: 0.153 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.004 | Val loss: 0.154 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.004 | Val loss: 0.154 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.003 | Val loss: 0.155 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.003 | Val loss: 0.155 | Gen: ehay airway onditionicicay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.003 | Val loss: 0.156 | Gen: ehay airway onditionicicay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.003 | Val loss: 0.156 | Gen: ehay airway onditionicicnay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.002 | Val loss: 0.157 | Gen: ehay airway onditionicicnay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.002 | Val loss: 0.157 | Gen: ehay airway onditionicicnay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.002 | Val loss: 0.158 | Gen: ehay airway onditionicicnay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.002 | Val loss: 0.159 | Gen: ehay airway onditionicicnay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.1533516053284984\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway onditionicicnay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe308814-910c-4839-8f2e-358528912e3a"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway onditionicicnay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, _i, _j = queries.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(2,1)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention.transpose(2,1))\n",
        "        context = torch.bmm(attention_weights.transpose(2,1),v)\n",
        "        return context, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, _i, _j = queries.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(2,1)) * self.scaling_factor\n",
        "        mask = torch.triu(torch.full_like(unnormalized_attention, fill_value=self.neg_inf), diagonal =+1)\n",
        "        unnormalized_attention = unnormalized_attention + mask\n",
        "        attention_weights = self.softmax(unnormalized_attention.transpose(2,1))\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
        "        return context, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6d2e1a-93ff-41ac-c789-2222e2fab117"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 100,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('oneself', 'oneselfway')\n",
            "('heighten', 'eightenhay')\n",
            "('addressing', 'addressingway')\n",
            "('sort', 'ortsay')\n",
            "('cried', 'iedcray')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.812 | Val loss: 2.265 | Gen: ellllllll ay oooooooooooooiay issssss oooooooooooooay\n",
            "Epoch:   1 | Train loss: 2.106 | Val loss: 1.986 | Gen: ellllllllay ay oooooooooooooiay issssssssay ooooooooay\n",
            "Epoch:   2 | Train loss: 1.877 | Val loss: 1.818 | Gen: etllllay aray oooooooooay isssssway ooooay\n",
            "Epoch:   3 | Train loss: 1.696 | Val loss: 1.706 | Gen: ety aray oooncay isway oooay\n",
            "Epoch:   4 | Train loss: 1.559 | Val loss: 1.656 | Gen: ety aray ontintinay isay oay\n",
            "Epoch:   5 | Train loss: 1.450 | Val loss: 1.572 | Gen: ety aray ontintinay isway oorway\n",
            "Epoch:   6 | Train loss: 1.351 | Val loss: 1.494 | Gen: ehay aray ondantionay isay ooorway\n",
            "Epoch:   7 | Train loss: 1.272 | Val loss: 1.478 | Gen: ethay aray ontintinay isway oorway\n",
            "Epoch:   8 | Train loss: 1.228 | Val loss: 1.483 | Gen: ethay aray oncinnanay isway oooowayway\n",
            "Epoch:   9 | Train loss: 1.172 | Val loss: 1.425 | Gen: ethay aray ononatinay isway ooooiway\n",
            "Epoch:  10 | Train loss: 1.125 | Val loss: 1.426 | Gen: ehay arway onatinay away ooingayway\n",
            "Epoch:  11 | Train loss: 1.065 | Val loss: 1.316 | Gen: ehay arway onginacay isway orkway\n",
            "Epoch:  12 | Train loss: 0.999 | Val loss: 1.360 | Gen: ehay arway oncatinay asway oorkway\n",
            "Epoch:  13 | Train loss: 0.958 | Val loss: 1.294 | Gen: ehay aray oncacacinationay isway orkway\n",
            "Epoch:  14 | Train loss: 0.914 | Val loss: 1.222 | Gen: ehay arway oncatingay isway orkway\n",
            "Epoch:  15 | Train loss: 0.864 | Val loss: 1.215 | Gen: ehay arway oncingingay isway orkway\n",
            "Epoch:  16 | Train loss: 0.829 | Val loss: 1.131 | Gen: ehay arway oncingingatinay isway orkwaway\n",
            "Epoch:  17 | Train loss: 0.793 | Val loss: 1.163 | Gen: ehay arway oncingininay isway oorkwayway\n",
            "Epoch:  18 | Train loss: 0.783 | Val loss: 1.173 | Gen: ehay arway oncindcatingionay isway oorwaway\n",
            "Epoch:  19 | Train loss: 0.731 | Val loss: 1.109 | Gen: ehahay arway ondingingcationay isway orkwawway\n",
            "Epoch:  20 | Train loss: 0.690 | Val loss: 1.092 | Gen: ehay arway oncingininationay isway oorkwingway\n",
            "Epoch:  21 | Train loss: 0.659 | Val loss: 1.078 | Gen: ehay arway ondingingcationcay isway orkwingway\n",
            "Epoch:  22 | Train loss: 0.636 | Val loss: 1.092 | Gen: ehay arirway oncinginingincay isway oorkwingway\n",
            "Epoch:  23 | Train loss: 0.614 | Val loss: 1.008 | Gen: ehay ariway oncingioninatinay isway orkwingway\n",
            "Epoch:  24 | Train loss: 0.611 | Val loss: 1.075 | Gen: ehahay ariway ondingingcationay isway orkwingway\n",
            "Epoch:  25 | Train loss: 0.603 | Val loss: 1.055 | Gen: ehay arirway ondingioninay isway oorkwingway\n",
            "Epoch:  26 | Train loss: 0.560 | Val loss: 1.063 | Gen: ehay arirway ondingininatincay isway orkwingway\n",
            "Epoch:  27 | Train loss: 0.563 | Val loss: 1.056 | Gen: ehay arway ondingioningway isway orkwingway\n",
            "Epoch:  28 | Train loss: 0.536 | Val loss: 1.004 | Gen: ehay ariway ondingiongingcay isway orkwingway\n",
            "Epoch:  29 | Train loss: 0.492 | Val loss: 0.977 | Gen: ehay arirway ondingioningway isway orkwingway\n",
            "Epoch:  30 | Train loss: 0.469 | Val loss: 0.986 | Gen: ehay arirway ondingiongingcay isway orkwingway\n",
            "Epoch:  31 | Train loss: 0.452 | Val loss: 1.012 | Gen: ehay arirway ondingioningway isway orkwingway\n",
            "Epoch:  32 | Train loss: 0.437 | Val loss: 0.990 | Gen: ehay arirway ondingioningway isway orkwingway\n",
            "Epoch:  33 | Train loss: 0.423 | Val loss: 0.992 | Gen: ehay arirway onditiongngincay isway orkwingway\n",
            "Epoch:  34 | Train loss: 0.407 | Val loss: 0.974 | Gen: ehay arirway onditiongingway isway orkwingway\n",
            "Epoch:  35 | Train loss: 0.389 | Val loss: 0.936 | Gen: ehay arirway onditiongingway isway orkwingway\n",
            "Epoch:  36 | Train loss: 0.376 | Val loss: 1.005 | Gen: ehay arirway onditiongningway isway orkwingway\n",
            "Epoch:  37 | Train loss: 0.362 | Val loss: 0.965 | Gen: ehay arirway onditiongingway isway orkwingway\n",
            "Epoch:  38 | Train loss: 0.346 | Val loss: 0.966 | Gen: ehay ariway onditiongnincay isway orkwingway\n",
            "Epoch:  39 | Train loss: 0.336 | Val loss: 0.939 | Gen: ethay irway onditioningway isway orkwingway\n",
            "Epoch:  40 | Train loss: 0.324 | Val loss: 1.003 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  41 | Train loss: 0.341 | Val loss: 0.982 | Gen: ethay irway onditioningway isway orkwingway\n",
            "Epoch:  42 | Train loss: 0.372 | Val loss: 1.010 | Gen: ethay ariway onditiongninway isway orkwingway\n",
            "Epoch:  43 | Train loss: 0.366 | Val loss: 1.135 | Gen: ehay arirway ondindiongway isway orkwingway\n",
            "Epoch:  44 | Train loss: 0.445 | Val loss: 1.040 | Gen: ethay arway onditioningway isway orkway\n",
            "Epoch:  45 | Train loss: 0.357 | Val loss: 0.893 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  46 | Train loss: 0.300 | Val loss: 0.875 | Gen: ethay arirway onditioningway isway orkwingway\n",
            "Epoch:  47 | Train loss: 0.276 | Val loss: 0.868 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  48 | Train loss: 0.260 | Val loss: 0.877 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  49 | Train loss: 0.249 | Val loss: 0.889 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  50 | Train loss: 0.243 | Val loss: 0.899 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  51 | Train loss: 0.235 | Val loss: 0.915 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  52 | Train loss: 0.229 | Val loss: 0.923 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  53 | Train loss: 0.223 | Val loss: 0.929 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  54 | Train loss: 0.216 | Val loss: 0.953 | Gen: ethay ariway onditioningway isway orkwingway\n",
            "Epoch:  55 | Train loss: 0.211 | Val loss: 0.944 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  56 | Train loss: 0.203 | Val loss: 0.957 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  57 | Train loss: 0.197 | Val loss: 0.963 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  58 | Train loss: 0.190 | Val loss: 0.989 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  59 | Train loss: 0.185 | Val loss: 0.980 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  60 | Train loss: 0.180 | Val loss: 1.027 | Gen: ethay ariway onditioncay isway orkwingway\n",
            "Epoch:  61 | Train loss: 0.190 | Val loss: 0.943 | Gen: ethay ariway onditioninay isway orkwingway\n",
            "Epoch:  62 | Train loss: 0.189 | Val loss: 0.957 | Gen: ethay ariway onditioningcay isway orkwingway\n",
            "Epoch:  63 | Train loss: 0.193 | Val loss: 0.942 | Gen: ethay ariway ondidtionincray isway orwingway\n",
            "Epoch:  64 | Train loss: 0.221 | Val loss: 0.928 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  65 | Train loss: 0.227 | Val loss: 1.174 | Gen: ethay ariway onditioningcay isway orkwingway\n",
            "Epoch:  66 | Train loss: 0.237 | Val loss: 0.915 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  67 | Train loss: 0.217 | Val loss: 0.881 | Gen: ethay ariway onditioningcay isway orkwingway\n",
            "Epoch:  68 | Train loss: 0.197 | Val loss: 0.876 | Gen: ethay airway ondidciongcay isway orkwingway\n",
            "Epoch:  69 | Train loss: 0.167 | Val loss: 0.888 | Gen: ethay airway onditioninay isway orkwingway\n",
            "Epoch:  70 | Train loss: 0.154 | Val loss: 0.862 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  71 | Train loss: 0.144 | Val loss: 0.869 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  72 | Train loss: 0.139 | Val loss: 0.874 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  73 | Train loss: 0.134 | Val loss: 0.879 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  74 | Train loss: 0.130 | Val loss: 0.882 | Gen: ethay airway onditioningcay isway orkwingway\n",
            "Epoch:  75 | Train loss: 0.126 | Val loss: 0.886 | Gen: ethay airway ondinitonincay isway orkwingway\n",
            "Epoch:  76 | Train loss: 0.122 | Val loss: 0.891 | Gen: ethay airway ondinitonincay isway orkwingway\n",
            "Epoch:  77 | Train loss: 0.118 | Val loss: 0.895 | Gen: ethay airway ondinitonincay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.115 | Val loss: 0.900 | Gen: ethay airway ondinitonincay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.111 | Val loss: 0.904 | Gen: ethay airway ondinitoncay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.108 | Val loss: 0.908 | Gen: ethay airway ondinitoncay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.105 | Val loss: 0.914 | Gen: ethay airway ondinitoncay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.102 | Val loss: 0.920 | Gen: ethay airway ondinitoncay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.099 | Val loss: 0.921 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.096 | Val loss: 0.929 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.093 | Val loss: 0.934 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.090 | Val loss: 0.946 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.088 | Val loss: 0.939 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.086 | Val loss: 0.949 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.082 | Val loss: 0.958 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.080 | Val loss: 0.957 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.077 | Val loss: 0.970 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.075 | Val loss: 0.975 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.073 | Val loss: 0.985 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.071 | Val loss: 0.984 | Gen: ethay airway ondintioncay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.071 | Val loss: 1.025 | Gen: ethay airway ondintiongcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.149 | Val loss: 1.192 | Gen: ethay airway ondintiongcaincj isway orkwingway\n",
            "Epoch:  97 | Train loss: 0.287 | Val loss: 1.171 | Gen: ethay arirway ondditoncanctincray isway orkwingway\n",
            "Epoch:  98 | Train loss: 0.248 | Val loss: 1.093 | Gen: ethay airway ondidtioncay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.187 | Val loss: 0.779 | Gen: ethay airway onditiongcaincincjrw isway orkingway\n",
            "Obtained lowest validation loss of: 0.7788833344820887\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongcaincincjrw isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2378cb1-6719-4f44-f1a1-42b41aa75301"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongcaincincjrw isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abfdcd2-d790-4a8f-aa64-4dc9f416f01b"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':100,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 10,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('virus', 'irusvay')\n",
            "('advertisers', 'advertisersway')\n",
            "('broadly', 'oadlybray')\n",
            "('harris', 'arrishay')\n",
            "('mardi', 'ardimay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.887 | Val loss: 2.370 | Gen: eeeay ayay otaywayway elay otaywayayay\n",
            "Epoch:   1 | Train loss: 2.119 | Val loss: 2.084 | Gen: eeteteayay ayay ontioniintay isway otay-omy\n",
            "Epoch:   2 | Train loss: 1.895 | Val loss: 1.933 | Gen: etay ayway ontiniiiinay isway ontay-inday\n",
            "Epoch:   3 | Train loss: 1.759 | Val loss: 1.835 | Gen: etetay ayway onaninininananay isay intay\n",
            "Epoch:   4 | Train loss: 1.658 | Val loss: 1.754 | Gen: etay away-ay oncay-incay-inay isway ongay-ingay\n",
            "Epoch:   5 | Train loss: 1.593 | Val loss: 1.787 | Gen: etay away onatininatinanatanan isay onay-inay\n",
            "Epoch:   6 | Train loss: 1.495 | Val loss: 1.626 | Gen: etay away ongingncay isay ongay-ingay\n",
            "Epoch:   7 | Train loss: 1.421 | Val loss: 1.571 | Gen: etay aray ongationgnatatatatat isay ongay-ingay\n",
            "Epoch:   8 | Train loss: 1.382 | Val loss: 1.586 | Gen: etay away-iway ongingngngngatatatat isway ongay-ingay\n",
            "Epoch:   9 | Train loss: 1.357 | Val loss: 1.484 | Gen: etay away ontiongmationatay isway ongay\n",
            "Epoch:  10 | Train loss: 1.264 | Val loss: 1.431 | Gen: etay away ongiongmationatay isay ongay\n",
            "Epoch:  11 | Train loss: 1.205 | Val loss: 1.485 | Gen: etay away ongay-onay isay ongay\n",
            "Epoch:  12 | Train loss: 1.176 | Val loss: 1.414 | Gen: etay away oniongnay-onay isay ongay-ingay\n",
            "Epoch:  13 | Train loss: 1.146 | Val loss: 1.537 | Gen: etayay away ongay-ogay isay ongay\n",
            "Epoch:  14 | Train loss: 1.114 | Val loss: 1.419 | Gen: ethay away oningonay-onatigay isway onway-ingay\n",
            "Epoch:  15 | Train loss: 1.079 | Val loss: 1.462 | Gen: ethay away oniongaynay isay onway\n",
            "Epoch:  16 | Train loss: 1.054 | Val loss: 1.356 | Gen: ethay away oningnay-onay isay oningay\n",
            "Epoch:  17 | Train loss: 0.997 | Val loss: 1.480 | Gen: ethay away onionay isay onway\n",
            "Epoch:  18 | Train loss: 0.966 | Val loss: 1.465 | Gen: ethay away oningay-inay isay oningay\n",
            "Epoch:  19 | Train loss: 0.945 | Val loss: 1.410 | Gen: ethay away oningay isay onway\n",
            "Epoch:  20 | Train loss: 0.920 | Val loss: 1.425 | Gen: ethay aray oninay-inatinay isay oningay\n",
            "Epoch:  21 | Train loss: 0.893 | Val loss: 1.366 | Gen: ehay aray oninay-onay isay ontay\n",
            "Epoch:  22 | Train loss: 0.855 | Val loss: 1.257 | Gen: ethay arway oninay-ingnay isway oningay\n",
            "Epoch:  23 | Train loss: 0.835 | Val loss: 1.214 | Gen: ethay aray oninayngtinay isway oningay\n",
            "Epoch:  24 | Train loss: 0.798 | Val loss: 1.178 | Gen: ehay arway oninay-inatinatinay isway oningay\n",
            "Epoch:  25 | Train loss: 0.775 | Val loss: 1.131 | Gen: ethay arway oninayngtingnay isay oningay\n",
            "Epoch:  26 | Train loss: 0.762 | Val loss: 1.078 | Gen: ehay ariway ondingtinay isway oningway\n",
            "Epoch:  27 | Train loss: 0.732 | Val loss: 1.106 | Gen: ehay ariway ondayingnay isay oringay\n",
            "Epoch:  28 | Train loss: 0.708 | Val loss: 1.039 | Gen: ehay ariway ondingtiongnay isay oringay\n",
            "Epoch:  29 | Train loss: 0.690 | Val loss: 1.055 | Gen: ehay arway ondinay-ingnay isay oringay\n",
            "Epoch:  30 | Train loss: 0.671 | Val loss: 0.999 | Gen: ehay ariway ondingtiongnay isay oringway\n",
            "Epoch:  31 | Train loss: 0.648 | Val loss: 1.036 | Gen: ehay arway ondinayingnay isay oringay\n",
            "Epoch:  32 | Train loss: 0.637 | Val loss: 1.005 | Gen: ehay ariway ondingtingnay isway oringway\n",
            "Epoch:  33 | Train loss: 0.614 | Val loss: 0.972 | Gen: ehay ariway onditinay isay oringay\n",
            "Epoch:  34 | Train loss: 0.612 | Val loss: 0.965 | Gen: ehay ariway onditiongnay isway oringingay\n",
            "Epoch:  35 | Train loss: 0.595 | Val loss: 0.986 | Gen: ehay ariay onditinay isay oringay\n",
            "Epoch:  36 | Train loss: 0.583 | Val loss: 0.956 | Gen: ehay ariway onditingay isway oringway\n",
            "Epoch:  37 | Train loss: 0.552 | Val loss: 0.907 | Gen: ehay ariway onditingonay isway oringway\n",
            "Epoch:  38 | Train loss: 0.536 | Val loss: 0.929 | Gen: ehay ariway onditiongnay isway oringingay\n",
            "Epoch:  39 | Train loss: 0.519 | Val loss: 0.869 | Gen: ehay ariway onditinay isway oringway\n",
            "Epoch:  40 | Train loss: 0.503 | Val loss: 0.908 | Gen: ehay ariway onditionay isway oringingay\n",
            "Epoch:  41 | Train loss: 0.494 | Val loss: 0.842 | Gen: ehthay ariway onditinay-ingway isway oringway\n",
            "Epoch:  42 | Train loss: 0.479 | Val loss: 0.862 | Gen: ehay ariway onditionginay isway oringingay\n",
            "Epoch:  43 | Train loss: 0.470 | Val loss: 0.849 | Gen: ehthay ariway onditingonay isway oringway\n",
            "Epoch:  44 | Train loss: 0.459 | Val loss: 0.920 | Gen: ehthay ariway onditinayingngway isway oringingway\n",
            "Epoch:  45 | Train loss: 0.461 | Val loss: 0.849 | Gen: ehthay ariway onditiongmangmingway isway oringway\n",
            "Epoch:  46 | Train loss: 0.445 | Val loss: 0.828 | Gen: ehthay ariway onditionginay isway oringingway\n",
            "Epoch:  47 | Train loss: 0.433 | Val loss: 0.788 | Gen: ehthay ariway onditiongmangingway isway oringway\n",
            "Epoch:  48 | Train loss: 0.422 | Val loss: 0.836 | Gen: ehthay ariway onditionginay isway owkingway\n",
            "Epoch:  49 | Train loss: 0.410 | Val loss: 0.782 | Gen: ehthay ariway onditionginatinay isway oringway\n",
            "Epoch:  50 | Train loss: 0.396 | Val loss: 0.778 | Gen: ehthay ariway onditionginatinay isway owkingway\n",
            "Epoch:  51 | Train loss: 0.387 | Val loss: 0.763 | Gen: ehthay ariway onditiongingway isway oringway\n",
            "Epoch:  52 | Train loss: 0.377 | Val loss: 0.771 | Gen: ehthay ariway onditionginatinay isway oringway\n",
            "Epoch:  53 | Train loss: 0.370 | Val loss: 0.761 | Gen: ehthay ariway onditiongingway isway oringway\n",
            "Epoch:  54 | Train loss: 0.361 | Val loss: 0.782 | Gen: ehthay ariway onditionginay isway oringinay\n",
            "Epoch:  55 | Train loss: 0.354 | Val loss: 0.772 | Gen: ehthay ariway onditionginay isway oriningway\n",
            "Epoch:  56 | Train loss: 0.345 | Val loss: 0.802 | Gen: ehthay ariway onditionginay isway oriningway\n",
            "Epoch:  57 | Train loss: 0.340 | Val loss: 0.961 | Gen: ehtay ariway onditioningway isway oriningway\n",
            "Epoch:  58 | Train loss: 0.354 | Val loss: 0.804 | Gen: ehthay ariway onditinayingngay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.362 | Val loss: 0.753 | Gen: ehthay ariway ondidingongway isway orkingway\n",
            "Epoch:  60 | Train loss: 0.369 | Val loss: 0.830 | Gen: ehtay ariway ondicioningmay isway oringinay\n",
            "Epoch:  61 | Train loss: 0.332 | Val loss: 0.769 | Gen: ehtay ariway ondidicaywanay isway oringway\n",
            "Epoch:  62 | Train loss: 0.320 | Val loss: 0.766 | Gen: ehtay ariway ondicioningmaninay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.311 | Val loss: 0.751 | Gen: ehtay ariway ondiitiongngway isway orkingway\n",
            "Epoch:  64 | Train loss: 0.300 | Val loss: 0.761 | Gen: ehthay ariway onditioningway isway orkingway\n",
            "Epoch:  65 | Train loss: 0.295 | Val loss: 0.749 | Gen: ehtay ariway ondidicaingmingway isway orkingway\n",
            "Epoch:  66 | Train loss: 0.285 | Val loss: 0.765 | Gen: ehthay ariway ondicioningmay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.280 | Val loss: 0.722 | Gen: ehthay ariway ondidiciongmay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.271 | Val loss: 0.715 | Gen: ehthay ariway ondicioningmanging isway orkingway\n",
            "Epoch:  69 | Train loss: 0.266 | Val loss: 0.704 | Gen: ehthay ariway ondicioningway isway orkingway\n",
            "Epoch:  70 | Train loss: 0.260 | Val loss: 0.701 | Gen: ehthay ariway onditiconingway isway orkingway\n",
            "Epoch:  71 | Train loss: 0.255 | Val loss: 0.687 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  72 | Train loss: 0.249 | Val loss: 0.682 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  73 | Train loss: 0.244 | Val loss: 0.686 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  74 | Train loss: 0.240 | Val loss: 0.684 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  75 | Train loss: 0.234 | Val loss: 0.688 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  76 | Train loss: 0.230 | Val loss: 0.695 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  77 | Train loss: 0.226 | Val loss: 0.686 | Gen: ehthay airway onditiconingway isway orkingway\n",
            "Epoch:  78 | Train loss: 0.220 | Val loss: 0.703 | Gen: ehthay airway onditingongingway isway orkingway\n",
            "Epoch:  79 | Train loss: 0.221 | Val loss: 0.732 | Gen: ehthay airway onditingongmanging isway orkingway\n",
            "Epoch:  80 | Train loss: 0.350 | Val loss: 0.900 | Gen: etht airway onddiciongingway isway orkingway\n",
            "Epoch:  81 | Train loss: 0.449 | Val loss: 0.794 | Gen: ethay away onditicaningngway isway orkingway\n",
            "Epoch:  82 | Train loss: 0.356 | Val loss: 0.812 | Gen: ehthay ariway onditingongmway isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.6817965844716385\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehtay away onditiconingway isaway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2249191f-96c7-4a15-8fe7-f299a7b7e336"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('twelvemonth', 'elvemonthtway')\n",
            "('china', 'inachay')\n",
            "('ringing', 'ingingray')\n",
            "('privilege', 'ivilegepray')\n",
            "('believe', 'elievebay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.411 | Val loss: 1.872 | Gen: etetetetetetetetetet iay icioioioioininininin issisisay incay\n",
            "Epoch:   1 | Train loss: 1.672 | Val loss: 1.584 | Gen: ehay iway icccicinconcay isisway ingggggggay\n",
            "Epoch:   2 | Train loss: 1.407 | Val loss: 1.515 | Gen: ehay irway onininginginay isway oway\n",
            "Epoch:   3 | Train loss: 1.260 | Val loss: 1.600 | Gen: etay irway ondindway isway orway\n",
            "Epoch:   4 | Train loss: 1.163 | Val loss: 1.244 | Gen: ehay away ondingingindinay isay oway\n",
            "Epoch:   5 | Train loss: 0.995 | Val loss: 1.174 | Gen: ethay irway ondindingay isway owway\n",
            "Epoch:   6 | Train loss: 0.881 | Val loss: 1.115 | Gen: etay iray ondinay isay orway\n",
            "Epoch:   7 | Train loss: 0.777 | Val loss: 1.071 | Gen: etay irway ondingtiongdinay isway owway\n",
            "Epoch:   8 | Train loss: 0.701 | Val loss: 1.032 | Gen: ethay irway ondingcay isway orway\n",
            "Epoch:   9 | Train loss: 0.639 | Val loss: 1.017 | Gen: ethay irway ondincionginingway isway orway\n",
            "Epoch:  10 | Train loss: 0.626 | Val loss: 1.036 | Gen: ehthay irrway ondingtiongdnay isway orway\n",
            "Epoch:  11 | Train loss: 0.575 | Val loss: 0.966 | Gen: ethay irray ondincay issiay orway\n",
            "Epoch:  12 | Train loss: 0.518 | Val loss: 0.882 | Gen: ethay arway ondintiongway iay orway\n",
            "Epoch:  13 | Train loss: 0.438 | Val loss: 0.877 | Gen: ethay irway ondintiongway isway orway\n",
            "Epoch:  14 | Train loss: 0.386 | Val loss: 0.742 | Gen: ehthay ariway ondinationgway isay okway\n",
            "Epoch:  15 | Train loss: 0.318 | Val loss: 0.773 | Gen: ethay airway ondingcay isway orwaywway\n",
            "Epoch:  16 | Train loss: 0.299 | Val loss: 0.794 | Gen: ethay airway ondintiongway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.277 | Val loss: 0.718 | Gen: ethay arway ondintiongway isway orwingway\n",
            "Epoch:  18 | Train loss: 0.251 | Val loss: 0.680 | Gen: ethay arway ondingcatingway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.224 | Val loss: 0.719 | Gen: ethay arway ondintiongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.223 | Val loss: 0.771 | Gen: ethay arway onditingcay isway orkway\n",
            "Epoch:  21 | Train loss: 0.241 | Val loss: 0.827 | Gen: ethay irway onditingcay isway owingwray\n",
            "Epoch:  22 | Train loss: 0.257 | Val loss: 0.841 | Gen: ehtway ariway onditiongcay isway orkway\n",
            "Epoch:  23 | Train loss: 0.230 | Val loss: 0.794 | Gen: ethay arway onditingtionay isway orkway\n",
            "Epoch:  24 | Train loss: 0.201 | Val loss: 0.698 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.169 | Val loss: 0.643 | Gen: ehtay ariway onditiongcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.136 | Val loss: 0.596 | Gen: ehay airway ondingtiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.122 | Val loss: 0.583 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.111 | Val loss: 0.556 | Gen: ehtay airway onditiongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.091 | Val loss: 0.592 | Gen: ehtay airway onditiongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.082 | Val loss: 0.718 | Gen: ehttthay airway onditiongcinay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.092 | Val loss: 0.600 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.076 | Val loss: 0.744 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.096 | Val loss: 1.345 | Gen: ehtthay irirway onditinoncingcay isway orkingwgway\n",
            "Epoch:  34 | Train loss: 0.363 | Val loss: 0.854 | Gen: ehthay arrway ondinationgcay isway orrkngway\n",
            "Epoch:  35 | Train loss: 0.233 | Val loss: 0.588 | Gen: ehay ariway ondidionctionway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.135 | Val loss: 0.510 | Gen: ethay airway onditionday isway orkingway\n",
            "Epoch:  37 | Train loss: 0.100 | Val loss: 0.547 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.075 | Val loss: 0.522 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.065 | Val loss: 0.507 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.051 | Val loss: 0.491 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.047 | Val loss: 0.501 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.040 | Val loss: 0.504 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.036 | Val loss: 0.520 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.042 | Val loss: 0.606 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.042 | Val loss: 0.514 | Gen: ehay airway onditioncay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.027 | Val loss: 0.515 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.022 | Val loss: 0.514 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.018 | Val loss: 0.523 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.015 | Val loss: 0.521 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.4909873876052025\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioncingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70b2331-c3c5-4366-8352-434c92dff175"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('virus', 'irusvay')\n",
            "('advertisers', 'advertisersway')\n",
            "('broadly', 'oadlybray')\n",
            "('harris', 'arrishay')\n",
            "('mardi', 'ardimay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.407 | Val loss: 1.994 | Gen: erway iway intintintintintinway isway ay\n",
            "Epoch:   1 | Train loss: 1.708 | Val loss: 1.722 | Gen: eway-y-etay-ay-ay awayway-away ondndndndndindndndin isway-y-y ongway-y-y-y-y-y\n",
            "Epoch:   2 | Train loss: 1.445 | Val loss: 1.517 | Gen: eway awaway ondindindingay isway ongway\n",
            "Epoch:   3 | Train loss: 1.235 | Val loss: 1.463 | Gen: ehay-ay-ay away ondindingay isay-inay ongrway-way\n",
            "Epoch:   4 | Train loss: 1.134 | Val loss: 1.389 | Gen: eway away ontingay isay oway\n",
            "Epoch:   5 | Train loss: 0.987 | Val loss: 1.314 | Gen: ehay arway ondiongay-ongngngngn iway orgrgway\n",
            "Epoch:   6 | Train loss: 0.919 | Val loss: 1.218 | Gen: ehay iway ondindgtingay iway oringway\n",
            "Epoch:   7 | Train loss: 0.881 | Val loss: 1.203 | Gen: ehay ariaway ondiniongionininiay isayiay oingiay\n",
            "Epoch:   8 | Train loss: 0.747 | Val loss: 1.062 | Gen: ehay away onnintiongay iway oway\n",
            "Epoch:   9 | Train loss: 0.620 | Val loss: 0.967 | Gen: ehay arway ondingtiongay iway oway\n",
            "Epoch:  10 | Train loss: 0.598 | Val loss: 0.915 | Gen: ehay aiway ondintingay isay oway\n",
            "Epoch:  11 | Train loss: 0.529 | Val loss: 0.910 | Gen: ehay arway ondintingngay iway orwingway\n",
            "Epoch:  12 | Train loss: 0.488 | Val loss: 0.894 | Gen: ehay arway onditingngninay iway orway\n",
            "Epoch:  13 | Train loss: 0.441 | Val loss: 0.765 | Gen: ehay arway onditinongay iway orway\n",
            "Epoch:  14 | Train loss: 0.383 | Val loss: 0.747 | Gen: ethay away onditinongngnay isay oway-ingway\n",
            "Epoch:  15 | Train loss: 0.365 | Val loss: 0.689 | Gen: ehay arway onditiongnioninay isay owingway\n",
            "Epoch:  16 | Train loss: 0.332 | Val loss: 0.659 | Gen: ethay airway onditiongay iway orway\n",
            "Epoch:  17 | Train loss: 0.281 | Val loss: 0.594 | Gen: ehay airayway onditiongcay isay orkingway\n",
            "Epoch:  18 | Train loss: 0.274 | Val loss: 0.684 | Gen: ehay arway onditiongay isay orkingway\n",
            "Epoch:  19 | Train loss: 0.286 | Val loss: 0.754 | Gen: etay airay ondititiongngway isay orkingway\n",
            "Epoch:  20 | Train loss: 0.278 | Val loss: 0.593 | Gen: thay airway onditiongway isay orkingway\n",
            "Epoch:  21 | Train loss: 0.246 | Val loss: 0.941 | Gen: ehtay airway oooonioniongway isway ooowrinway\n",
            "Epoch:  22 | Train loss: 0.283 | Val loss: 0.539 | Gen: ehtay ariway onditiongway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.197 | Val loss: 0.539 | Gen: ehtay arway onditiongcay iway orkingwway\n",
            "Epoch:  24 | Train loss: 0.208 | Val loss: 0.805 | Gen: thay arway onditiongningway isway orkingwway\n",
            "Epoch:  25 | Train loss: 0.232 | Val loss: 0.630 | Gen: ehtay ariway onditiongcay isway orkingfay\n",
            "Epoch:  26 | Train loss: 0.184 | Val loss: 0.470 | Gen: ehtay ariway onditioniongway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.146 | Val loss: 0.478 | Gen: ehtay airway onditiongnioncway isway orkingwwwwray\n",
            "Epoch:  28 | Train loss: 0.143 | Val loss: 0.689 | Gen: ehtay aairaway onditiongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.185 | Val loss: 0.453 | Gen: ehtay airway onditioningway isway orkingwwway\n",
            "Epoch:  30 | Train loss: 0.115 | Val loss: 0.382 | Gen: ehtay airway onditiongcay isway orkingwwwway\n",
            "Epoch:  31 | Train loss: 0.093 | Val loss: 0.384 | Gen: ehtay airway onditiongcay isway orkingwwwway\n",
            "Epoch:  32 | Train loss: 0.086 | Val loss: 0.369 | Gen: ehtay airway onditiongnicway isway orkingwwwway\n",
            "Epoch:  33 | Train loss: 0.080 | Val loss: 0.347 | Gen: ehtay airway onditioningcay isway orkingwwwway\n",
            "Epoch:  34 | Train loss: 0.070 | Val loss: 0.346 | Gen: ehtay airway onditioningcay isway orkingwwwway\n",
            "Epoch:  35 | Train loss: 0.063 | Val loss: 0.341 | Gen: ehtay airway onditioningcay isway orkingwwway\n",
            "Epoch:  36 | Train loss: 0.057 | Val loss: 0.348 | Gen: ehtay airway onditioniongcay isway orkingwwway\n",
            "Epoch:  37 | Train loss: 0.056 | Val loss: 0.361 | Gen: ehtay airway onditioningcay isway orkingwwway\n",
            "Epoch:  38 | Train loss: 0.063 | Val loss: 0.361 | Gen: ethay airway onditiongnicgway isway orkingwwway\n",
            "Epoch:  39 | Train loss: 0.056 | Val loss: 0.361 | Gen: ehtay airway onditioningcay isway orkingwwway\n",
            "Epoch:  40 | Train loss: 0.053 | Val loss: 0.375 | Gen: ehtay airway onditioniogcay isway orkingwway\n",
            "Epoch:  41 | Train loss: 0.058 | Val loss: 0.417 | Gen: ethay airway onditioningcay isway orkingwwway\n",
            "Epoch:  42 | Train loss: 0.213 | Val loss: 0.947 | Gen: ethay airway onditiningninay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.219 | Val loss: 0.792 | Gen: ehtay airway onditiotiongcay isway orkinggway\n",
            "Epoch:  44 | Train loss: 0.192 | Val loss: 0.605 | Gen: ehtay aawiway onditiongcay iswiwiy-i orkkingwwway\n",
            "Epoch:  45 | Train loss: 0.194 | Val loss: 0.730 | Gen: ehtay away onditinongcay iway orkingway\n",
            "Epoch:  46 | Train loss: 0.155 | Val loss: 0.621 | Gen: ehay airwayway onditioningway iway orkingway\n",
            "Epoch:  47 | Train loss: 0.130 | Val loss: 0.396 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.074 | Val loss: 0.334 | Gen: ehetay airway onditioniongcay isway orkingwway\n",
            "Epoch:  49 | Train loss: 0.057 | Val loss: 0.285 | Gen: ehtay airway onditioningcay isway orkingwway\n",
            "Obtained lowest validation loss of: 0.2845677935500752\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehtay airway onditioningcay isway orkingwway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "95ffbe0a-1b8c-46ef-a97f-498929d84304"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "54bd5e89-f318-4ffc-8f6a-efdeb075b0c6"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "outputId": "561318bd-6aa4-4e6c-e876-fd525804665e"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l, )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'erettsay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}